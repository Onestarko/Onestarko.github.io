{"meta":{"title":"onestarko","subtitle":"","description":"","author":"kowhoy","url":"http://yoursite.com","root":"/"},"pages":[{"title":"categories","date":"2020-02-15T00:58:48.000Z","updated":"2020-02-15T00:59:32.763Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2020-02-15T00:59:39.000Z","updated":"2020-02-15T01:57:12.301Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"spark_core图谱","slug":"spark-core图谱","date":"2020-05-18T06:34:48.000Z","updated":"2020-05-18T06:34:48.266Z","comments":true,"path":"2020/05/18/spark-core图谱/","link":"","permalink":"http://yoursite.com/2020/05/18/spark-core%E5%9B%BE%E8%B0%B1/","excerpt":"","text":"","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark_transformation","slug":"spark-transformation","date":"2020-05-15T07:21:22.000Z","updated":"2020-05-15T07:21:22.568Z","comments":true,"path":"2020/05/15/spark-transformation/","link":"","permalink":"http://yoursite.com/2020/05/15/spark-transformation/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590package sk_demoimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object transformations &#123; val _sc_conf = new SparkConf() .setAppName(\"spark_transformation\") .setMaster(\"local\") val sc = new SparkContext(_sc_conf) def main(args: Array[String]): Unit = &#123; /** * Return a new distributed dataset formed by passing each element of the source through a function func. **/ map() /** * Similar to map, but runs separately on each partition (block) of the RDD, * so func must be of type Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt; when running on an RDD of type T. * */ mapPartitions() /** * Similar to mapPartitions, * but also provides func with an integer value representing the index of the partition, * so func must be of type (Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt; when running on an RDD of type T. * */ mapPartitionsWithIndex() /** * Similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item). * */ flatMap() /** * Return an RDD created by coalescing all elements within each partition into an array * */ glom() /** * Return an RDD of grouped items. Each group consists of a key and a sequence of elements * */ groupBy() /** * Return a new dataset formed by selecting those elements of the source on which func returns true. * */ filter() /** * Sample a fraction fraction of the data, with or without replacement, using a given random number generator seed. * */ sample() /** * Return a new dataset that contains the distinct elements of the source dataset. * */ distinct() /** * Decrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently after filtering down a large dataset. * */ coalesce() /** * Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them. This always shuffles all data over the network. * */ repartition() /** * Return this RDD sorted by the given key function. * */ sortBy() /** * Return an RDD created by piping elements to a forked external process * */ pipe() /** * Return the union of this RDD and another one. Any identical elements will appear multiple * */ union() /** * Return an RDD with the elements from `this` that are not in `other`. * */ subtract() /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * */ intersection() /** * Return the Cartesian product of this RDD and another one * */ cartesian() /** * Zips this RDD with another one, returning key-value pairs with the first element in each RDD * */ zip() /** * Return a copy of the RDD partitioned using the specified partitioner. * */ partitionBy() /** * Merge the values for each key using an associative and commutative reduce function. * */ reduceByKey() /** * Group the values for each key in the RDD into a single sequence. * */ groupByKey() /** * Aggregate the values of each key, using given combine functions and a neutral \"zero value\". * */ aggregateByKey() /** * Merge the values for each key using an associative function and a neutral \"zero value\" * */ foldByKey() /** * Sort the RDD by key, so that each partition contains a sorted range of the elements. * */ sortByKey() /** * Pass each value in the key-value pair RDD through a map function without changing the keys * */ mapValues() /** * Return an RDD containing all pairs of elements with matching keys in `this` and `other`. * */ join() /** * For each key k in `this` or `other`, return a resulting RDD that contains a tuple with the * list of values for that key in `this` as well as `other`. * */ cogroup() &#125; /** * @Desc RDD中的每个元素进行 func 操作, 返回一个新的RDD * @Date 10:57 上午 2020/5/15 * @Param [] * @Return void **/ def map(): Unit = &#123; val rdd = sc.parallelize(1 to 10) val res = rdd.map(_+2) res.foreach(println) &#125; /** * @Desc 将一个分区上的所有元素，使用一次函数，而不是map的每个元素都使用一次元素 * @append 效率要比map好，但是要注意数据量大而导致的oom * @Date 11:25 上午 2020/5/15 * @Param [] * @Return void **/ def mapPartitions(): Unit = &#123; val rdd = sc.parallelize(1 to 10) val rdd2 = rdd.mapPartitions(mapPartitionsFunc) rdd2.foreach(println) &#125; /*** * @Desc * @Date 11:31 上午 2020/5/15 * @Param [iter] * @Return scala.collection.Iterator&lt;java.lang.Object&gt; **/ def mapPartitionsFunc(iter:Iterator[Int]) :Iterator[Int]= &#123; var res = List[Int]() while (iter.hasNext) &#123; val iterVal = iter.next() res :+= (iterVal + 2) &#125; res.iterator &#125; /*** * @Desc 类似mapPartitions, 多了分区的index * @Date 11:39 上午 2020/5/15 * @Param [] * @Return void **/ def mapPartitionsWithIndex(): Unit = &#123; val rdd = sc.parallelize(1 to 10, 2) val res = rdd.mapPartitionsWithIndex(mapPartitionsWithIndexFunc) res.foreach(println) &#125; /*** * @Desc * @Date 11:39 上午 2020/5/15 * @Param [idx, iter] * @Return scala.collection.Iterator&lt;scala.Tuple2&lt;java.lang.Object,java.lang.Object&gt;&gt; **/ def mapPartitionsWithIndexFunc(idx:Int, iter:Iterator[Int]): Iterator[(Int, Int)] = &#123; var res = List[(Int, Int)]() while (iter.hasNext) &#123; val iterVal = iter.next() res :+= (idx, iterVal+2) &#125; res.iterator &#125; /*** * @Desc 扁平化的操作，所以输出的是一个序列，不是单个的值 * @Date 11:46 上午 2020/5/15 * @Param [] * @Return void **/ def flatMap(): Unit = &#123; val rdd = sc.parallelize(Array((\"A\", 1), (\"B\", 2), (\"C\", 2))) val mapRes = rdd.map(x =&gt; x._1 + \"-&gt;\" + x._2) val flatMapRes = rdd.flatMap(x =&gt; x._1 + \"-&gt;\" + x._2) mapRes.foreach(println) flatMapRes.foreach(println) &#125; /** * @Desc 按分区转化为数组类型的RDD * @Date 1:36 下午 2020/5/15 * @Param [] * @Return void **/ def glom(): Unit = &#123; val rdd = sc.parallelize(Array(1, 2, 3, 4, 1, 2, 3, 0), 2) val res = rdd.glom().map(_.max) res.foreach(println) &#125; /** * @Desc 根据每个元素的计算进行分组 * @Date 1:45 下午 2020/5/15 * @Param [] * @Return void **/ def groupBy(): Unit = &#123; val rdd = sc.parallelize(1 to 10) val res = rdd.groupBy(_%2) res.foreach(println) &#125; /** * @Desc 返回条件为true的 * @Date 1:49 下午 2020/5/15 * @Param [] * @Return void **/ def filter(): Unit = &#123; val rdd = sc.parallelize(1 to 10) val res = rdd.filter(_%2 == 0) res.foreach(println) &#125; /** * @Desc 取样 (是否重复取样) * @param fraction expected size of the sample as a fraction of this RDD's size * without replacement: probability that each element is chosen; fraction must be [0, 1] * with replacement: expected number of times each element is chosen; fraction must be greater * than or equal to 0 * @Date 1:51 下午 2020/5/15 * @Param [] * @Return void **/ def sample(): Unit = &#123; val rdd = sc.parallelize(1 to 10) val res = rdd.sample(false, 0.5) res.foreach(println) &#125; /** * @Desc 去重后的RDD 参数 numPartitions * @Date 2:02 下午 2020/5/15 * @Param [] * @Return void **/ def distinct(): Unit = &#123; val rdd = sc.parallelize(Array(1, 2, 3, 1, 2, 4, 5)) val res = rdd.distinct(2) res.foreach(println) &#125; /** * @Desc 降低分区数 * @Date 2:06 下午 2020/5/15 * @Param [] * @Return void **/ def coalesce(): Unit = &#123; val rdd = sc.parallelize(1 to 16, 4) val res = rdd.coalesce(3) println(res.partitions.size) &#125; /** * @Desc 重新分区,随机排列，分区之间保持平衡 * @Date 2:09 下午 2020/5/15 * @Param [] * @Return void **/ def repartition(): Unit = &#123; val rdd = sc.parallelize(1 to 16, 4) val res = rdd.repartition(2) println(res.partitions.size) &#125; /** * @Desc 根据给定key排序 * @Date 2:12 下午 2020/5/15 * @Param [] * @Return void **/ def sortBy(): Unit = &#123; val rdd = sc.parallelize(Array((\"A\", 1), (\"C\", 3), (\"B\", 4), (\"D\", 2))) val res = rdd.sortBy(_._1) res.foreach(println) &#125; /** * @Desc 在管道中执行shell/perl脚本 * @Date 2:15 下午 2020/5/15 * @Param [] * @Return void **/ def pipe(): Unit = &#123; val rdd = sc.parallelize(Array(\"A\", \"B\", \"C\")) val res = rdd.pipe(\"shell_path.sh\") res.foreach(println) &#125; /** * @Desc 连接RDD * @Date 2:18 下午 2020/5/15 * @Param [] * @Return void **/ def union(): Unit = &#123; val rdd1 = sc.parallelize(1 to 6) val rdd2 = sc.parallelize(5 to 10) val res = rdd1.union(rdd2) res.foreach(println) &#125; /** * @Desc 将rdd1中也在rdd2中的元素去掉 * @Date 2:22 下午 2020/5/15 * @Param [] * @Return void **/ def subtract(): Unit = &#123; val rdd1 = sc.parallelize(1 to 6) val rdd2 = sc.parallelize(5 to 10) val res = rdd1.subtract(rdd2) res.foreach(println) &#125; /** * @Desc 交集 * @Date 2:25 下午 2020/5/15 * @Param [] * @Return void **/ def intersection(): Unit = &#123; val rdd1 = sc.parallelize(1 to 6) val rdd2 = sc.parallelize(5 to 10) val res = rdd1.intersection(rdd2) res.foreach(println) &#125; /** * @Desc 笛卡尔积 * @Date 2:27 下午 2020/5/15 * @Param [] * @Return void **/ def cartesian(): Unit = &#123; val rdd1 = sc.parallelize(1 to 6) val rdd2 = sc.parallelize(5 to 10) val res = rdd1.cartesian(rdd2) res.foreach(println) &#125; /** * @Desc k v组合 * @Date 2:32 下午 2020/5/15 * @Param [] * @Return void **/ def zip(): Unit = &#123; val rdd1 = sc.parallelize(Array(\"A\", \"B\", \"C\")) val rdd2 = sc.parallelize(Array(1, 2, 3)) val res = rdd1.zip(rdd2) res.foreach(println) &#125; /** * @Desc 根据partitioner 进行分区 * @Date 2:38 下午 2020/5/15 * @Param [] * @Return void **/ def partitionBy(): Unit = &#123; val rdd = sc.parallelize(Array((\"A\", 1), (\"B\", 1), (\"C\", 2), (\"D\",3))) val res = rdd.partitionBy(new org.apache.spark.HashPartitioner(2)) res.foreach(println) &#125; /** * @Desc * @Date 2:41 下午 2020/5/15 * @Param [] * @Return void **/ def reduceByKey(): Unit = &#123; val rdd = sc.parallelize(Array((\"A\", 1), (\"B\", 1), (\"A\", 2), (\"D\", 3))) val res = rdd.reduceByKey((x, y) =&gt; x + y) res.foreach(println) &#125; /** * @Desc * @Date 2:45 下午 2020/5/15 * @Param [] * @Return void **/ def groupByKey(): Unit = &#123; val rdd = sc.parallelize(Array((\"A\", 1), (\"B\", 1), (\"A\", 2), (\"D\", 3))) val res = rdd.groupByKey() res.foreach(println) &#125; /** * @Desc * @Date 2:56 下午 2020/5/15 * @Param [] * @Return void **/ def aggregateByKey(): Unit = &#123; val rdd = sc.parallelize(Array(1, 2, 3, 4, 5, 6, 7, 8), 2) val res1 = rdd.aggregate(0)(math.max(_, _), _+_) val res2 = rdd.aggregate(5)(math.max(_, _), _+_) println(res1, res2) &#125; /** * @Desc * @Date 3:01 下午 2020/5/15 * @Param [] * @Return void **/ def foldByKey(): Unit = &#123; val rdd = sc.parallelize(List((1, 2), (2, 3), (1, 3), (2, 5)), 2) val res = rdd.foldByKey(0)(_+_) res.foreach(println) &#125; def combineByKey(): Unit = &#123; &#125; /** * @Desc * @Date 3:09 下午 2020/5/15 * @Param [] * @Return void **/ def sortByKey(): Unit = &#123; val rdd = sc.parallelize(Array((\"A\", 1), (\"C\", 2), (\"B\", 3))) val res = rdd.sortByKey(true) res.foreach(println) &#125; /** * @Desc v map * @Date 3:11 下午 2020/5/15 * @Param [] * @Return void **/ def mapValues():Unit = &#123; val rdd = sc.parallelize(Array((\"A\", 1), (\"C\", 2), (\"B\", 3))) val res = rdd.mapValues(_+2) res.foreach(println) &#125; /** * @Desc join * @Date 3:13 下午 2020/5/15 * @Param [] * @Return void **/ def join(): Unit = &#123; val rdd1 = sc.parallelize(Array((1,\"a\"),(2,\"b\"),(3,\"c\"))) val rdd2 = sc.parallelize(Array((1,4),(2,5),(3,6))) val res = rdd1.join(rdd2) res.foreach(println) &#125; /** * @Desc * @Date 3:16 下午 2020/5/15 * @Param [] * @Return void **/ def cogroup(): Unit = &#123; val rdd = sc.parallelize(Array((1,\"a\"),(2,\"b\"),(3,\"c\"))) val rdd1 = sc.parallelize(Array((1,4),(2,5),(3,6))) val res = rdd.cogroup(rdd1) res.foreach(println) &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"},{"name":"transformation","slug":"transformation","permalink":"http://yoursite.com/tags/transformation/"}]},{"title":"datax的基本使用","slug":"datax的基本使用","date":"2020-05-12T07:39:02.000Z","updated":"2020-05-12T07:39:02.922Z","comments":true,"path":"2020/05/12/datax的基本使用/","link":"","permalink":"http://yoursite.com/2020/05/12/datax%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","excerpt":"","text":"datax的基本使用 datax是一种中心化处理形式，分为reader和writer连接到datax，然后进行转储操作 mysql2mysql123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&#123; \"job\": &#123; \"content\": [ &#123; \"reader\": &#123; \"name\":\"mysqlreader\", \"parameter\": &#123; \"column\":[ \"id\", \"name\" ], \"connection\":[ &#123; \"jdbcUrl\":[ \"jdbc:mysql://127.0.0.1:3306/test\" ], \"table\":[ \"d1\" ] &#125; ], \"password\":\"root\", \"username\":\"root\" &#125; &#125;, \"writer\": &#123; \"name\":\"mysqlwriter\", \"parameter\":&#123; \"column\":[ \"id\", \"name\" ], \"connection\":[ &#123; \"jdbcUrl\":\"jdbc:mysql://127.0.0.1:3306/test\", \"table\":[\"d2\"] &#125; ], \"password\":\"root\", \"username\":\"root\" &#125; &#125; &#125; ], \"setting\":&#123; \"speed\":&#123; \"channel\":\"1\" &#125; &#125; &#125;&#125; file2hive1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&#123; \"job\":&#123; \"content\": [ &#123; \"reader\": &#123; \"name\": \"txtfilereader\", \"parameter\": &#123; \"path\": [\"/home/kowhoy/tmp/10_days_orders_2.csv\"], \"encoding\": \"UTF-8\", \"column\": [ &#123; \"index\":0, \"type\": \"long\", &#125;, &#123; \"index\": 1, \"type\": \"long\" &#125;, &#123; \"index\": 2, \"type\": \"string\" &#125;, &#123; \"index\": 3, \"type\": \"long\" &#125;, &#123;\"index\": 4, \"type\": \"long\"&#125;, &#123;\"index\": 5, \"type\": \"long\"&#125;, &#123;\"index\": 6, \"type\": \"long\"&#125;, &#123;\"index\": 7, \"type\": \"long\"&#125;, &#123;\"index\": 8, \"type\": \"long\"&#125;, &#123;\"index\": 9, \"type\": \"long\"&#125;, &#123;\"index\": 10, \"type\": \"long\"&#125;, &#123;\"index\": 11, \"type\": \"long\"&#125;, &#123;\"index\": 12, \"type\": \"long\"&#125;, &#123;\"index\": 13, \"type\": \"long\"&#125;, &#123;\"index\": 14, \"type\": \"long\"&#125;, &#123;\"index\": 15, \"type\": \"long\"&#125;, &#123;\"index\": 16, \"type\": \"long\"&#125;, &#123;\"index\": 17, \"type\": \"long\"&#125;, &#123;\"index\": 18, \"type\": \"long\"&#125;, &#123;\"index\": 19, \"type\": \"string\"&#125;, &#123;\"index\": 20, \"type\": \"double\"&#125;, &#123;\"index\": 21, \"type\": \"double\"&#125;, &#123;\"index\": 22, \"type\": \"double\"&#125;, &#123;\"index\": 23, \"type\": \"double\"&#125;, &#123;\"index\": 24, \"type\": \"long\"&#125;, &#123;\"index\": 25, \"type\": \"long\"&#125;, &#123;\"index\": 26, \"type\": \"long\"&#125;, &#123;\"index\": 27, \"type\": \"double\"&#125;, &#123;\"index\": 28, \"type\": \"double\"&#125;, &#123;\"index\": 29, \"type\": \"long\"&#125;, &#123;\"index\": 30, \"type\": \"long\"&#125;, &#123;\"index\": 31, \"type\": \"long\"&#125;, &#123;\"index\": 32, \"type\": \"long\"&#125;, &#123;\"index\": 33, \"type\": \"long\"&#125;, &#123;\"index\": 34, \"type\": \"long\"&#125;, &#123;\"index\": 35, \"type\": \"long\"&#125;, &#123;\"index\": 36, \"type\": \"long\"&#125;, &#123;\"index\": 37, \"type\": \"double\"&#125;, &#123;\"index\": 38, \"type\": \"long\"&#125;, &#123;\"index\": 39, \"type\": \"long\"&#125;, &#123;\"index\": 40, \"type\": \"long\"&#125;, &#123;\"index\": 41, \"type\": \"string\"&#125;, &#123;\"index\": 42, \"type\": \"string\"&#125;, &#123;\"index\": 43, \"type\": \"long\"&#125;, &#123;\"index\": 44, \"type\": \"long\"&#125;, &#123;\"index\": 45, \"type\": \"string\"&#125; ], \"fieldDelimiter\": \",\" &#125; &#125;, \"writer\": &#123; \"name\": \"hdfswriter\", \"parameter\": &#123; \"defaultFS\": \"hdfs://ecs:9000\", \"fileType\": \"text\", \"path\": \"/user/hive/warehouse/orders.db/datax_file_tab\", \"fileName\": \"datax_file_tab\", \"column\": [ &#123;\"name\": \"order_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"channel_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"order_sn\", \"type\": \"STRING\"&#125;, &#123;\"name\": \"is_serv_order\", \"type\": \"INT\"&#125;, &#123;\"name\": \"is_personal\", \"type\": \"INT\"&#125;, &#123;\"name\": \"order_type\", \"type\": \"INT\"&#125;, &#123;\"name\": \"mode_type\", \"type\": \"INT\"&#125;, &#123;\"name\": \"order_src\", \"type\": \"INT\"&#125;, &#123;\"name\": \"ao_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"member_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"employee_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"branch_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"company_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"city_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"osp_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"oss_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"oss_ao_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"osg_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"staff_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"sku_code\", \"type\": \"STRING\"&#125;, &#123;\"name\": \"sku_base_price\", \"type\": \"FLOAT\"&#125;, &#123;\"name\": \"sku_litre_price\", \"type\": \"FLOAT\"&#125;, &#123;\"name\": \"sku_price\", \"type\": \"FLOAT\"&#125;, &#123;\"name\": \"sku_litre\", \"type\": \"FLOAT\"&#125;, &#123;\"name\": \"oil_amount\", \"type\": \"INT\"&#125;, &#123;\"name\": \"amount\", \"type\": \"INT\"&#125;, &#123;\"name\": \"status\", \"type\": \"INT\"&#125;, &#123;\"name\": \"longitude\", \"type\": \"FLOAT\"&#125;, &#123;\"name\": \"latitude\", \"type\": \"FLOAT\"&#125;, &#123;\"name\": \"up_order_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"payment_type\", \"type\": \"INT\"&#125;, &#123;\"name\": \"payment_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"payment_status\", \"type\": \"INT\"&#125;, &#123;\"name\": \"payment_amout\", \"type\": \"INT\"&#125;, &#123;\"name\": \"refund_type\", \"type\": \"INT\"&#125;, &#123;\"name\": \"refund_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"refund_status\", \"type\": \"INT\"&#125;, &#123;\"name\": \"refund_amount\", \"type\": \"FLOAT\"&#125;, &#123;\"name\": \"settlement_rate\", \"type\": \"INT\"&#125;, &#123;\"name\": \"settlement_status\", \"type\": \"INT\"&#125;, &#123;\"name\": \"truck_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"create_time\", \"type\": \"STRING\"&#125;, &#123;\"name\": \"update_time\", \"type\": \"STRING\"&#125;, &#123;\"name\": \"delete_flag\", \"type\": \"INT\"&#125;, &#123;\"name\": \"index_num\", \"type\": \"INT\"&#125;, &#123;\"name\": \"order_date\", \"type\": \"STRING\"&#125; ], \"writeMode\": \"append\", \"fieldDelimiter\": \",\" &#125; &#125; &#125; ], \"setting\": &#123; \"speed\": &#123; \"channel\": 1 &#125; &#125; &#125;&#125; sql2hive1234567891011121314151617181920212223242526272829303132333435363738394041424344&#123; \"job\":&#123; \"content\": [ &#123; \"reader\": &#123; \"name\": \"mysqlreader\", \"parameter\": &#123; \"username\": \"hive\", \"password\": \"wojiushiwo\", \"connection\": [ &#123; \"querySql\": [ \"select order_id, channel_id, order_sn, is_serv_order, is_personal, order_type from datax_sql_tab;\" ], \"jdbcUrl\": [ \"jdbc:mysql://127.0.0.1:3306/test\" ] &#125; ] &#125; &#125;, \"writer\": &#123; \"name\": \"hdfswriter\", \"parameter\": &#123; \"defaultFS\": \"hdfs://39.99.221.146:9000\", \"fileType\": \"text\", \"path\": \"/user/hive/warehouse/orders.db/datax_sql_tab\", \"fileName\": \"datax_sql_tab\", \"column\": [ &#123;\"name\": \"order_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"channel_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"order_sn\", \"type\": \"STRING\"&#125;, &#123;\"name\": \"is_serv_order\", \"type\": \"INT\"&#125;, &#123;\"name\": \"is_personal\", \"type\": \"INT\"&#125;, &#123;\"name\": \"order_type\", \"type\": \"INT\"&#125; ], \"writeMode\": \"append\", \"fieldDelimiter\": \",\" &#125; &#125; &#125; ], \"setting\": &#123; \"speed\": &#123; \"channel\": 1 &#125; &#125; &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"datax","slug":"datax","permalink":"http://yoursite.com/tags/datax/"}]},{"title":"mysql_存储过程/触发器示例","slug":"mysql-存储过程-触发器示例","date":"2020-05-09T10:15:44.000Z","updated":"2020-05-09T10:15:44.250Z","comments":true,"path":"2020/05/09/mysql-存储过程-触发器示例/","link":"","permalink":"http://yoursite.com/2020/05/09/mysql-%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B-%E8%A7%A6%E5%8F%91%E5%99%A8%E7%A4%BA%E4%BE%8B/","excerpt":"","text":"存储过程1234567891011121314151617create procedure three2(in d double(5, 2),out v1 double(5,2),out v2 double(5,2),out v3 double(5,2),out v4 double(5,2))BEGINselect max(price_val) into v1 from price_log;select min(price_val) into v2 from price_log;select avg(price_val) into v3 from price_log;select d into v4;end;call three2(12, @v1, @v2, @v3, @v4);select @v1, @v2, @v3, @v4; 触发器123create trigger demo before update on sort_test for each rowset new.node &#x3D; 100;","categories":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"HQL执行流程","slug":"HQL执行流程","date":"2020-05-09T09:45:14.000Z","updated":"2020-05-09T09:45:14.944Z","comments":true,"path":"2020/05/09/HQL执行流程/","link":"","permalink":"http://yoursite.com/2020/05/09/HQL%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B/","excerpt":"","text":"","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"hive存储模型","slug":"hive存储模型","date":"2020-05-09T09:40:47.000Z","updated":"2020-05-09T09:40:47.342Z","comments":true,"path":"2020/05/09/hive存储模型/","link":"","permalink":"http://yoursite.com/2020/05/09/hive%E5%AD%98%E5%82%A8%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"hive存储模型一、内部表与外部表 内部表的一切都在hive上管理，删除了内部表，将丢失数据外部表hive上只是提供操作以及结构，数据分开存储，删除外部表，数据不会丢失，重新建表即可正常使用 1. 建立内部表123456789create table atm (id int,name string,address string,money float)row format delimitedfileds terminated by ','stored as textfile; 1load data local inpath &#39;file_path&#39; into table atm; 2. 建立外部表123456789create external table ex_atm(id int,name string,address string,money float)row format delimitedfields terminated by ','stored as textfile 1load data local inpath &#39;file_path&#39; into table ex_atm; 二、分区与分桶 分区表创建表时执行一个分区标识，传入数据的时候指定分区标识，没有高级到使用数据中的列作为分区标识，分区就是将数据按标识放到hdfs上不同文件夹下，在查询的时候可以减少数据量，而不是全表扫描分桶表可以方便抽样 1. 分区表的创建123456789create table atm (id int,address string,money float)partitoned by (name string)row format delimitedfields terminated by &#39;,&#39;stored as textfile; 1load data local inpath &#39;file_path&#39; into table atm partiton (name&#x3D;&quot;zh&quot;); 1select * from atm where name &#x3D; &#39;zh&#39;; 1show partitions atm; 2. 分桶表的创建1234567create table bucket_atm (id int, name string, address string, money float)clustered by (name) into 3 bucketsrow format delimitedfields terminated by &#39;,&#39;stored as textfile; 1insert overwrite table bucket_atm select * from atm; 1234567select id, name from table tablesample(bucket x out of y on column);--- x: 从第几个分桶开始抽取--- y: 每隔几个分桶抽取4分桶 x &#x3D; 1, y &#x3D; 2就抽取 1,3分桶","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"数据库的优化器","slug":"数据库的优化器","date":"2020-05-09T03:39:13.000Z","updated":"2020-05-09T03:39:13.957Z","comments":true,"path":"2020/05/09/数据库的优化器/","link":"","permalink":"http://yoursite.com/2020/05/09/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8/","excerpt":"","text":"数据库主要由三部分组成，分别是解析器、优化器和执行引擎。 RBORBO(Rule-Based Optimizer) 基于规则的优化器。是根据已经制定好的一些优化规则对关系表达式进行转换，最终生成一个最优的执行计划。它是一种经验式的优化方法，优化规则都是预先定义好的，只需要将SQL按照优化规则的顺序往上套就行，一旦满足某个规则则进行优化。这样的结果就是同样一条SQL，无论读取的表中的数据是怎样的，最后生成的执行计划都是一样的（优化规则都一样）。而且SQL的写法不同也很有可能影响最终的执行计划，从而影响SQL的性能（基于优化规则顺序执行）。所以说，虽然RBO是一个老司机，知道常见的套路，但是当路况不同时，也无法针对性的达到最佳的效果。 CBOCBO（Cost-Based Optimizer）基于代价的优化器。根据优化规则对关系表达式进行转换，生成多个执行计划，最后根据统计信息和代价模型计算每个执行计划的Cost。从中挑选Cost最小的执行计划作为最终的执行计划。从描述来看，CBO是优于RBO的，RBO只认规则，对数据不敏感，而在实际的过程中，数据的量级会严重影响同样SQL的性能。所以仅仅通过RBO生成的执行计划很有可能不是最优的。而CBO依赖于统计信息和代价模型，统计信息的准确与否、代价模型是否合理都会影响CBO选择最优计划。目前各大数据库和大数据计算引擎都已经在使用CBO了，比如Oracle、Hive、Spark、Flink等等。 动态CBO动态CBO，就是在执行计划生成的过程中动态优化的方式。随着大数据技术的飞速发展，静态的CBO已经无法满足我们SQL优化的需要了，静态的统计信息无法提供准确的参考，在执行计划的生成过程中动态统计才会得到最优的执行计划。","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"优化器","slug":"优化器","permalink":"http://yoursite.com/tags/%E4%BC%98%E5%8C%96%E5%99%A8/"}]},{"title":"python操作zookeeper","slug":"python操作zookeeper","date":"2020-05-07T06:45:17.000Z","updated":"2020-05-07T06:45:17.729Z","comments":true,"path":"2020/05/07/python操作zookeeper/","link":"","permalink":"http://yoursite.com/2020/05/07/python%E6%93%8D%E4%BD%9Czookeeper/","excerpt":"","text":"一、安装1pip install kazoo 二、连接Zookeeper123from kazoo.client import KazooClientzk = KazooClient(hosts='39.99.221.106') 默认端口为2181，注意防火墙/安全组 三、创建节点 path： 节点路径 value： 节点对应的值，注意值的类型是 bytes ephemeral： 若为 True 则创建一个临时节点，session 中断后自动删除该节点。默认 False sequence: 若为 True 则在你创建节点名后面增加10位数字（例如：你创建一个 testplatform/test 节点，实际创建的是 testplatform/test0000000003，这串数&gt;字是顺序递增的）。默认 False makepath： 若为 False 父节点不存在时抛 NoNodeError。若为 True 父节点不存在则创建父节点。默认 False 12345678from kazoo.client import KazooClientzk = KazooClient(hosts=\"39.99.221.106\")zk.start()zk.create(\"/testplatform/test\", b\"this is test\", makepath=True)zk.stop() 四、查看节点 get_children() 查看子节点, get() 查看值 12345678910from kazoo.client import KazooClientzk = KazooClient(hosts=\"39.99.221.106\")zk.start()node = zk.get_children(\"/testplatform\")value = zk.get(\"/testplatform/test\")zk.stop() 五、更改节点12345678910from kazoo.client import KazooClientzk = KazooClient(hosts=\"39.99.221.106\")zk.start()zk.set(\"/testplatform/test\", b\"testabc\")value = zk.get(\"/testplatform/test\")zk.stop() 六、删除节点1234567from kazoo.client import KazooClientzk = KazooClient(hosts=\"39.99.221.106\")zk.start()zk.delete(\"/testplatform/test\", recursive=False)zk.stop() 参数 recursive：若为 False，当需要删除的节点存在子节点，会抛异常 NotEmptyError 。若为True，则删除 此节点 以及 删除该节点的所有子节点 七、watches 实践12345678910from kazoo.client import KazooClientzk = KazooClient(hosts=\"39.99.221.106\")zk.start()def test(event): print(\"触发事件\")if __name__ == \"__main__\": zk.get(\"/testplatform/test\", watch=test)","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"},{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}]},{"title":"ZOOKEEPER基础","slug":"ZOOKEEPER基础","date":"2020-05-07T06:06:44.000Z","updated":"2020-05-07T06:22:21.418Z","comments":true,"path":"2020/05/07/ZOOKEEPER基础/","link":"","permalink":"http://yoursite.com/2020/05/07/ZOOKEEPER%E5%9F%BA%E7%A1%80/","excerpt":"","text":"ZOOKEEPER 监听器原理:（1）在Zookeeper的API操作中，创建main()主方法即主线程； （2）在main线程中创建Zookeeper客户端（zkClient），这时会创建两个线程： 线程connet负责网络通信连接，连接服务器； 线程Listener负责监听；（3）客户端通过connet线程连接服务器； 图中getChildren(&quot;/&quot; , true) ，&quot; / &quot;表示监听的是根目录，true表示监听，不监听用false（4）在Zookeeper的注册监听列表中将注册的监听事件添加到列表中，表示这个服务器中的/path，即根目录这个路径被客户端监听了； （5）一旦被监听的服务器根目录下，数据或路径发生改变，Zookeeper就会将这个消息发送给Listener线程； （6）Listener线程内部调用process方法，采取相应的措施，例如更新服务器列表等。 选举机制：1)半数机制:集群中半数以上机器存活，集群可用。所以 Zookeeper 适合安装奇数台 服务器。2)Zookeeper 虽然在配置文件中并没有指定 Master 和 Slave。但是，Zookeeper 工作时， 是有一个节点为 Leader，其他则为 Follower，Leader 是通过内部的选举机制临时产生的。 (1)服务器 1 启动，发起一次选举。服务器 1 投自己一票。此时服务器 1 票数一票，不够半数以上(3 票)，选举无法完成，服务器 1 状态保持为 LOOKING;(2)服务器 2 启动，再发起一次选举。服务器 1 和 2 分别投自己一票并交换选票信息: 此时服务器 1 发现服务器 2 的 ID 比自己目前投票推举的(服务器 1)大，更改选票为推举 服务器 2。此时服务器 1 票数 0 票，服务器 2 票数 2 票，没有半数以上结果，选举无法完成，服务器 1，2 状态保持 LOOKING(3)服务器 3 启动，发起一次选举。此时服务器 1 和 2 都会更改选票为服务器 3。此次投票结果:服务器 1 为 0 票，服务器 2 为 0 票，服务器 3 为 3 票。此时服务器 3 的票数已 经超过半数，服务器 3 当选 Leader。服务器 1，2 更改状态为 FOLLOWING，服务器 3 更改 状态为 LEADING;(4)服务器 4 启动，发起一次选举。此时服务器 1，2，3 已经不是 LOOKING 状态， 不会更改选票信息。交换选票信息结果:服务器 3 为 3 票，服务器 4 为 1 票。此时服务器 4 服从多数，更改选票信息为服务器 3，并更改状态为 FOLLOWING;(5)服务器 5 启动，同 4 一样当小弟。 集群最少需要机器数:3 ###写数据流程","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}]},{"title":"zookeeper安装部署","slug":"zookeeper安装部署","date":"2020-05-07T03:53:34.000Z","updated":"2020-05-07T03:53:34.490Z","comments":true,"path":"2020/05/07/zookeeper安装部署/","link":"","permalink":"http://yoursite.com/2020/05/07/zookeeper%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/","excerpt":"","text":"zookeeper安装部署一、本地模式安装部署1. 下载安装1wget https://mirrors.huaweicloud.com/apache/zookeeper/zookeeper-3.4.13/zookeeper-3.4.13.tar.gz 1tar -zxvf zookeeper-3.4.13.tar.gz -C ../ 12cd zookeeper-3.4.13mkdir zkData 12cd conf/cp zoo_sample.cfg zoo.cfg 12vim zoo.cfgdataDir=/home/kowhoy/software/zookeeper-3.4.13/zkData 2. 使用12345bin&#x2F;zkServer.sh start #启动serverbin&#x2F;zkServer.sh status #查看状态bin&#x2F;zkServer.sh stop #停止stopbin&#x2F;zkCli.sh #启动客户端 二、分布式安装部署1. 下载解压2. myid 创建zkData1234cd zkDatavim myid2 ##依次 3， 4 3. zoo.cfg1234567dataDir&#x3D;&#x2F;home&#x2F;kowhoy&#x2F;software&#x2F;zookeeper-3.4.13&#x2F;zkDataquorumListenOnAllIPs&#x3D;trueserver.2&#x3D;ecs01:2888:3888server.3&#x3D;ecs00:2888:3888server.4&#x3D;bcc00:2888:3888 1234567891011server.A&#x3D;B:C:DA 是一个数字，表示这个是第几号服务器;集群模式下配置一个文件 myid，这个文件在 dataDir 目录下，这个文件里面有一个数据 就是 A 的值，Zookeeper 启动时读取此文件，拿到里面的数据与 zoo.cfg 里面的配置信息比 较从而判断到底是哪个 server。B 是这个服务器的地址; C 是这个服务器 Follower 与集群中的 Leader 服务器交换信息的端口; D 是万一集群中的 Leader 服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。 4.同步到其他机器上5. 分别启动1./zkServer.sh start 6. 查看状态1./zkServer.sh status","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}]},{"title":"HIVE安装配置","slug":"HIVE安装配置","date":"2020-05-07T02:10:43.000Z","updated":"2020-05-07T02:10:43.794Z","comments":true,"path":"2020/05/07/HIVE安装配置/","link":"","permalink":"http://yoursite.com/2020/05/07/HIVE%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/","excerpt":"","text":"HIVE安装配置一、前置环境 JAVA HADOOP MYSQL 二、安装 下载解压 123wget https://mirrors.huaweicloud.com/apache/hive/hive-2.3.6/apache-hive-2.3.6-bin.tar.gztar -zxvf apache-hive-2.3.6-bin.tar.gz -C ../ 配置环境变量 vim ~/.bash_profile 12HIVE_HOME&#x3D;&#x2F;home&#x2F;kowhoy&#x2F;software&#x2F;apache-hive-2.3.6-binexport PATH&#x3D;$HIVE_HOME&#x2F;bin:$PATH 三、配置 拷贝配置文件 1234567cd $&#123;HIVE_HOME&#125;/confcp hive-env.sh.template hive-env.shcp hive-exec-log4j2.properties.template hive-exec-log4j2.propertiescp hive-log4j2.properties.template hive-log4j2.propertiescp hive-default.xml.template hive-site.xmlcp beeline-log4j2.properties.template beeline-log4j2.properties vim hive-env.sh 12export JAVA_HOME&#x3D;&#x2F;home&#x2F;kowhoy&#x2F;software&#x2F;jdk1.8.0_191export HADOOP_HOME&#x3D;&#x2F;home&#x2F;kowhoy&#x2F;software&#x2F;hadoop-2.8.5 vim hive-site.xml 添加 123456789&lt;property&gt;&lt;name&gt;system:java.io.tmpdir&lt;&#x2F;name&gt;&lt;value&gt;&#x2F;tmp&#x2F;hive&#x2F;java&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt;&lt;name&gt;system:user.name&lt;&#x2F;name&gt;&lt;value&gt;$&#123;user.name&#125;&lt;&#x2F;value&gt;&lt;&#x2F;property&gt; 修改12345678910111213141516171819&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionURL&lt;&#x2F;name&gt;&lt;value&gt;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;metastore?createDatabaseIfNotExist&#x3D;true&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;&#x2F;name&gt;&lt;value&gt;com.mysql.jdbc.Driver&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;&#x2F;name&gt;&lt;value&gt;hive&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;&#x2F;name&gt;&lt;value&gt;passwd&lt;&#x2F;value&gt;&lt;&#x2F;property&gt; 四、添加jar包1cp mysql-connector-java-5.1.48-bin.jar ../apache-hive-2.3.6-bin/lib/ 五、hdfs创建文件夹并设置权限123456hdfs dfs -mkdir &#x2F;hivehdfs dfs -mkdir &#x2F;hive&#x2F;tmphdfs dfs -mkdir &#x2F;hive&#x2F;loghdfs dfs -mkdir &#x2F;hive&#x2F;warehousehdfs dfs -chmod 777 &#x2F;hive&#x2F;tmp 六、异常处理 出现123Failed to open new session: java.lang.RuntimeException:org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User:xxx not allowed to impersonate anonymous vim ${HADOOP_HOME}/etc/hadoop/core-site.xml 123456789&lt;property&gt;&lt;name&gt;hadoop.proxyuser.xxx.hosts&lt;&#x2F;name&gt;&lt;value&gt;*&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt;&lt;name&gt;hadoop.proxyuser.xxx.groups&lt;&#x2F;name&gt;&lt;value&gt;*&lt;&#x2F;value&gt;&lt;&#x2F;property&gt; 报错信息中的xxx 与配置中的xxx一致 七、初始化数据库1$&#123;HIVE_HOME&#125;&#x2F;bin&#x2F;schematool --dbType mysql --initSchema 八、使用 hive-client 1hive beenline 开启 1$&#123;HIVE_HOME&#125;/bin/hiveserver2 &amp; 连接 1$&#123;HIVE_HOME&#125;/bin/beeline -u jdbc:hive2://localhost:10000 username passwd","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HIVE","slug":"HIVE","permalink":"http://yoursite.com/tags/HIVE/"}]},{"title":"Pandas脑图","slug":"Pandas脑图","date":"2020-04-26T10:02:45.000Z","updated":"2020-04-26T10:02:45.508Z","comments":true,"path":"2020/04/26/Pandas脑图/","link":"","permalink":"http://yoursite.com/2020/04/26/Pandas%E8%84%91%E5%9B%BE/","excerpt":"","text":"","categories":[{"name":"Pandas","slug":"Pandas","permalink":"http://yoursite.com/categories/Pandas/"},{"name":"Python","slug":"Pandas/Python","permalink":"http://yoursite.com/categories/Pandas/Python/"}],"tags":[{"name":"Pandas","slug":"Pandas","permalink":"http://yoursite.com/tags/Pandas/"}]},{"title":"Numpy脑图","slug":"Numpy脑图","date":"2020-04-26T10:01:45.000Z","updated":"2020-04-26T10:01:45.596Z","comments":true,"path":"2020/04/26/Numpy脑图/","link":"","permalink":"http://yoursite.com/2020/04/26/Numpy%E8%84%91%E5%9B%BE/","excerpt":"","text":"","categories":[{"name":"Numpy","slug":"Numpy","permalink":"http://yoursite.com/categories/Numpy/"},{"name":"Python","slug":"Numpy/Python","permalink":"http://yoursite.com/categories/Numpy/Python/"}],"tags":[{"name":"Numpy","slug":"Numpy","permalink":"http://yoursite.com/tags/Numpy/"}]},{"title":"matplotlib基础图表","slug":"matplotlib基础图表","date":"2020-04-26T09:53:07.000Z","updated":"2020-04-26T10:13:45.715Z","comments":true,"path":"2020/04/26/matplotlib基础图表/","link":"","permalink":"http://yoursite.com/2020/04/26/matplotlib%E5%9F%BA%E7%A1%80%E5%9B%BE%E8%A1%A8/","excerpt":"","text":"matplotlib基础图表一、根据x,y绘制12345678910111213import numpy as npfrom matplotlib import pyplot as pltx = np.arange(1, 11)y = x * 2 + 1plt.title(\"demo1\")plt.xlabel(\"x\")plt.ylabel(\"y\")plt.plot(x, y)plt.show() 12345678910111213import numpy as npfrom matplotlib import pyplot as pltx = np.arange(0, 3 * np.pi, 0.1)y = np.sin(x)plt.title(\"demo\")plt.xlabel(\"x\")plt.ylabel(\"y\")plt.plot(x, y, \"om\") #o指以圆点展示, m表示玫红色plt.show() 123456789101112131415161718192021222324252627282930import numpy as npfrom matplotlib import pyplot as pltp1_x = np.arange(1, 11)p1_y = p1_x * 5 + 8p2_x = np.arange(1, 11)p2_y = p2_x ** 3 + 10p3_x = np.arange(0, 4 * np.pi, 0.1)p3_y = np.sin(p3_x)p4_x = np.arange(0, 4 * np.pi, 0.1)p4_y = np.cos(p4_x)plt.subplot(2, 2, 1) # 两行两列第一个plt.title(\"p1\")plt.plot(p1_x, p1_y, \"om\")plt.subplot(2, 2, 2) # 两行两列第二个plt.title(\"p2\")plt.plot(p2_x, p2_y, \"pm\")plt.subplot(2, 2, 3)plt.title(\"p3\")plt.plot(p3_x, p3_y)plt.subplot(2, 2, 4)plt.title(\"p4\")plt.plot(p4_x, p4_y) 二、柱状图1234567891011121314import numpy as npfrom matplotlib import pyplot as pltb1_x = np.arange(1, 10, 2)b1_y = b1_x * 2 + 10b2_x = np.arange(2, 11, 2)b2_y = b2_x * 3 + 2plt.bar(b1_x, b1_y, color=\"m\", align='center')plt.bar(b2_x, b2_y, color=\"b\", align=\"center\")plt.title(\"bar_demo\")plt.show() 三、 直方图12345678910import numpy as npfrom matplotlib import pyplot as plta = np.random.randint(50, size=100)plt.hist(a, bins=[0, 10, 20, 30, 40, 50])plt.title(\"histogram\")plt.show() 四、散点图12345678import numpy as npfrom matplotlib import pyplot as pltx = np.random.randn(1000)y = np.random.randn(1000)plt.plot(x, y, \"om\")plt.show() 12345678import numpy as npfrom matplotlib import pyplot as pltx = np.random.randn(1000)y = np.random.randn(1000)plt.scatter(x, y)plt.show()","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"},{"name":"matplotlib","slug":"matplotlib","permalink":"http://yoursite.com/tags/matplotlib/"}]},{"title":"hadoop云服务器分布式安装配置","slug":"hadoop云服务器分布式安装配置","date":"2020-04-22T06:10:42.000Z","updated":"2020-04-22T06:10:42.489Z","comments":true,"path":"2020/04/22/hadoop云服务器分布式安装配置/","link":"","permalink":"http://yoursite.com/2020/04/22/hadoop%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/","excerpt":"","text":"hadoop云服务器分布式安装配置 在三台云服务器上搭建hadoop集群, 1master + 2slave使用版本: hadoop-3.2.1、jdk1.8.0_191、 scala-2.13.1⚠️ jdk11版本会出现Caused by: java.lang.NoClassDefFoundError: javax/activation/DataSource错误，还是选择了低版本 一、服务器创建用户设置密码、配置互相之间免密登录** ⚠️ 服务器的hostname 以及 /etc/hosts中配置的hostname 均不可带_下划线 ** 一、下载安装包(使用国内镜像)script12345wget https://mirrors.huaweicloud.com/java/jdk/8u191-b12/jdk-8u191-linux-x64.tar.gzwget https://downloads.lightbend.com/scala/2.13.1/scala-2.13.1.tgzwget https://mirrors.huaweicloud.com/apache/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz 二、解压配置环境变量script12345678910export SCALA_HOME=/home/kowhoy/software/scala-2.13.1export PATH=$SCALA_HOME/bin:$PATHexport HADOOP_HOME=/home/kowhoy/software/hadoop-3.2.1export PATH=$HADOOP_HOME/bin:$PATHJAVA_HOME=/home/kowhoy/software/jdk1.8.0_191JRE_HOME=/home/kowhoy/software/jdk1.8.0_191/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=$JAVA_HOME/bin:$PATH 三台服务器均进行以上操作 三、配置master,同步给slave 配置文件的根目录 $HADOOP_HOME/etc/hadoop配置文件中涉及到的文件夹，需要自行创建 hadoop-env.sh script1export JAVA_HOME=/home/kowhoy/software/jdk1.8.0_191 core-site.xml 12345678910111213&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;description&gt;hdfsurl&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/kowhoy/software/hadoop-3.2.1/tmp&lt;/value&gt; &lt;description&gt;临时文件路径&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml 1234567891011121314151617181920212223242526&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;master:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/kowhoy/software/hadoop-3.2.1/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/kowhoy/software/hadoop-3.2.1/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt; &lt;value&gt;/home/kowhoy/software/hadoop-3.2.1/dfs/namesecondary&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;decription&gt;master + slave个数&lt;/decription&gt; &lt;/property&gt;&lt;/configuration&gt; mapred-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml 12345678910111213&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;master&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; workers写配置的slave 12ecs00bcc00 masters文件是没有的，默认master的hosts配置应该是 master，如果不是，可能是在masters文件中配置，未测试 以上配置可scp到slave上，注意如果hostname不一致，需要修改 四、启动集群 格式化 script1hadoop namenode -format 启动 script1$&#123;HADOOP_HOME&#125;/sbin/start_all.sh jps查看 script12345678910#master25189 NameNode25638 ResourceManager25417 SecondaryNameNode26638 Jps#slave21406 NodeManager23646 Jps21278 DataNode 五、补充如有错误，查看${HADOOP_HOME}/logs下的日志，datanode相关日志是在slave上","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"}]},{"title":"Scala模式匹配","slug":"Scala模式匹配","date":"2020-04-21T07:01:32.000Z","updated":"2020-04-21T07:01:32.883Z","comments":true,"path":"2020/04/21/Scala模式匹配/","link":"","permalink":"http://yoursite.com/2020/04/21/Scala%E6%A8%A1%E5%BC%8F%E5%8C%B9%E9%85%8D/","excerpt":"","text":"模式匹配12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879package com.kowhoy.scalaimport scala.util.Random// 模式匹配object MatchDemo &#123; def main(args: Array[String]): Unit = &#123; // 基础匹配 basic_match() // 条件匹配 condition_match() // 类型匹配 type_match(List(1,2,4)) type_match(12) type_match(1.2) // array匹配 array_match(Array()) // 异常处理 error_exception_match() &#125; def basic_match(): Unit = &#123; val names = Array(\"A\", \"B\", \"C\") val name = names(Random.nextInt(names.length)) name match &#123; case \"A\" =&gt; println(\"AAAA\") case \"B\" =&gt; println(\"BBBB\") case _ =&gt; println(\"CCCCC\") &#125; &#125; def condition_match(): Unit = &#123; val names = Array(\"A\", \"B\", \"C\") val name = names(Random.nextInt(names.length))// name = \"C\" val buff = \"red\" name match &#123; case \"A\" =&gt; println(\"A\") case \"B\" =&gt; println(\"B\") case _ if(buff == \"red\") =&gt; println(\"D\") case _ =&gt; println(\"C\") &#125; &#125; def type_match(data:Any): Unit = &#123; data match &#123; case x:Int =&gt; println(\"INT\") case x:String =&gt; println(\"String\") case x:List[Any] =&gt; println(\"List\") case _ =&gt; println(\"other\") &#125; &#125; def array_match(array:Array[String]): Unit = &#123; array match &#123; case Array(\"abc\") =&gt; println(\"type1_abc\") case Array(x, y) =&gt; println(\"type2:\", x, y) case Array(x, _*) =&gt; println(\"type3\", x) case _ =&gt; println(\"other\") &#125; &#125; def error_exception_match(): Unit = &#123; try &#123; val d = 10 / 0 &#125; catch &#123; case e: ArithmeticException =&gt; println(\"error0\") case e: Exception =&gt; println(e.getStackTrace) &#125; finally &#123; println(\"Aaaaaaa\") &#125; &#125;&#125;","categories":[{"name":"Scala","slug":"Scala","permalink":"http://yoursite.com/categories/Scala/"}],"tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"}]},{"title":"柯里化","slug":"柯里化","date":"2020-04-15T10:29:12.000Z","updated":"2020-04-15T10:29:12.110Z","comments":true,"path":"2020/04/15/柯里化/","link":"","permalink":"http://yoursite.com/2020/04/15/%E6%9F%AF%E9%87%8C%E5%8C%96/","excerpt":"","text":"柯里化定义 将原来多个参数的函数，分成多个单个参数的函数 好处 复杂逻辑简单化 示例12345678910111213141516171819package com.kowhoy.scala//柯里化object keli &#123; def main(args: Array[String]): Unit = &#123; var inc = (x:Int, y:Int) =&gt; x + y println(inc(1, 2)) println(inc_kl(1)(2)) var firstNum = inc_kl((1))_ var res = firstNum(2) println(res) &#125; def inc_kl(x:Int)(y:Int) = &#123; x + y &#125;&#125; 注意 当只传递部分参数生产函数的时候，需要用占位符_补充后面的参数","categories":[{"name":"Scala","slug":"Scala","permalink":"http://yoursite.com/categories/Scala/"}],"tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"}]},{"title":"匿名函数","slug":"匿名函数","date":"2020-04-15T10:28:45.000Z","updated":"2020-04-15T10:28:45.241Z","comments":true,"path":"2020/04/15/匿名函数/","link":"","permalink":"http://yoursite.com/2020/04/15/%E5%8C%BF%E5%90%8D%E5%87%BD%E6%95%B0/","excerpt":"","text":"匿名函数 简写函数定义, 匿名函数不可以写成传名调用 12345678910111213141516171819202122232425262728293031package com.kowhoy.scala//匿名函数object niming &#123; def main(args: Array[String]): Unit = &#123; var initNum = 0 var counter = () =&gt; &#123; initNum += 1 initNum &#125; var sendValue = (x: Int) =&gt; &#123; for (i &lt;- 0 to 5) &#123; println(x) &#125; &#125; sendValue(counter()) sendName(counter()) &#125; def sendName(x: =&gt; Int) = &#123; for (i &lt;- 0 to 5) &#123; println(x) &#125; &#125;&#125;","categories":[{"name":"Scala","slug":"Scala","permalink":"http://yoursite.com/categories/Scala/"}],"tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"}]},{"title":"传值传名调用","slug":"传值传名调用","date":"2020-04-15T10:28:02.000Z","updated":"2020-04-15T10:28:02.982Z","comments":true,"path":"2020/04/15/传值传名调用/","link":"","permalink":"http://yoursite.com/2020/04/15/%E4%BC%A0%E5%80%BC%E4%BC%A0%E5%90%8D%E8%B0%83%E7%94%A8/","excerpt":"","text":"传值传名调用 传值 name:type 计算好值进行调用 传名 name: =&gt; type在调用处再进行计算 12345678910111213141516171819202122232425262728293031323334package com.kowhoy.scala// 传值调用和传名调用object sendWay &#123; var initNum = 0 def main(args: Array[String]):Unit = &#123; sendName(counter) println(\"--\" * 20) initNum = 0 sendValue(counter) &#125; def counter:Int = &#123; initNum += 1 initNum &#125; def sendName(x: =&gt; Int) :Unit = &#123; for (i &lt;- 0 to 5) &#123; println(x) &#125; &#125; def sendValue(x: Int) :Unit = &#123; for (i &lt;- 0 to 5) &#123; println(x) &#125; &#125;&#125; 12345678910111213123456----------------------------------------111111","categories":[{"name":"Scala","slug":"Scala","permalink":"http://yoursite.com/categories/Scala/"}],"tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"}]},{"title":"scala变长参数","slug":"scala变长参数","date":"2020-04-15T10:27:19.000Z","updated":"2020-04-15T10:27:19.716Z","comments":true,"path":"2020/04/15/scala变长参数/","link":"","permalink":"http://yoursite.com/2020/04/15/scala%E5%8F%98%E9%95%BF%E5%8F%82%E6%95%B0/","excerpt":"","text":"变长参数 函数的参数可以设置为多个同种类型或不同类型的参数 1234567891011121314151617181920212223242526272829303132333435363738package com.kowhoy.scala//变长参数, 可变参数object varargs &#123; def main(args: Array[String]): Unit = &#123; var numsSum = sumargs(1, 1, 3, 4, 5, 6) println(numsSum) var minusRight = minusargs(\"right\", 5, 4, 3, 2, 1) // 5 4 3 (2 -1 ) =&gt; 5 - 4 - (3-1) =&gt; 5 - 4 - 2 =&gt; 5 - 2 =&gt; 3 var minusLeft = minusargs(\"left\", 5, 4, 3, 2, 1) println(minusRight, minusLeft) anyArgs(\"abc\", 123) &#125; def sumargs(args: Int*):Int = &#123; var res = args.reduceLeft(_+_) res &#125; def minusargs(minuDirction:String, args: Int*):Int = &#123; var res = 0 if (minuDirction == \"right\") &#123; res = args.reduceRight(_-_) &#125; else if (minuDirction == \"left\") &#123; res = args.reduceLeft(_-_) &#125; res &#125; def anyArgs(args: Any*):Unit = &#123; args.foreach(println) &#125;&#125; reduceLeft / reduceRight","categories":[{"name":"Scala","slug":"Scala","permalink":"http://yoursite.com/categories/Scala/"}],"tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"}]},{"title":"Scala中的lazy使用","slug":"Scala中的lazy使用","date":"2020-04-15T10:26:21.000Z","updated":"2020-04-15T10:26:21.267Z","comments":true,"path":"2020/04/15/Scala中的lazy使用/","link":"","permalink":"http://yoursite.com/2020/04/15/Scala%E4%B8%AD%E7%9A%84lazy%E4%BD%BF%E7%94%A8/","excerpt":"","text":"Scala中的lazy使用什么是lazy lazy用在赋值的时候，目的是为了在赋值变量真正被使用的时候，才会进行变量赋值处理 示例12345678910111213141516171819202122232425262728293031323334353637383940414243//lazy赋值package com.kowhoy.scalaobject Lazy &#123; def main(args: Array[String]): Unit = &#123; withNoLazy.run() println(\"--\"*20) withLazy.run() &#125;&#125;object withNoLazy &#123; def init(): Int = &#123; println(\"非惰性赋值\") 0 &#125; def run(): Unit = &#123; val initNum = init() println(\"Start...\") println(initNum) println(\"End...\") &#125;&#125;object withLazy &#123; def init(): Int = &#123; println(\"惰性赋值\") 0 &#125; def run(): Unit = &#123; lazy val initNum = init() println(\"Start...\") println(initNum) println(\"End...\") &#125;&#125; 上文的输出如下123456789非惰性赋值Start...0End...----------------------------------------Start...惰性赋值0End... 注意 使用lazy赋值的变量只能是val声明的lazy modifier allowed only with value definitions","categories":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/categories/scala/"}],"tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"}]},{"title":"mysql分组top优化","slug":"mysql分组top优化","date":"2020-04-14T08:26:35.000Z","updated":"2020-04-14T08:26:35.915Z","comments":true,"path":"2020/04/14/mysql分组top优化/","link":"","permalink":"http://yoursite.com/2020/04/14/mysql%E5%88%86%E7%BB%84top%E4%BC%98%E5%8C%96/","excerpt":"","text":"mysql分组top优化 mysql分组取top1 通常是用max() 然后内连 可以使用变量的方式避免join操作 问题场景同一组oss_id, osp_id, pro_sku_code, start_time 会有多种 price_val,先需要依据update_time字段取每一组的最后一条更新内容 原始写法123456789101112select a.osp_id, a.oss_id, a.pro_sku_code, a.price_val, a.start_time from pricing.price_log a,( select max(update_time) as max_up, oss_id, osp_id, pro_sku_code, start_time from pricing.price_log where price_type = 10 AND STATUS = 1 AND delete_flag = 0 group by oss_id, osp_id, pro_sku_code, start_time ) b where a.oss_id = b.oss_id and a.osp_id = b.osp_id and a.pro_sku_code = b.pro_sku_code and a.start_time = b.start_time and a.update_time = b.max_up and a.price_type = 10 and a.status = 1 and a.delete_flag = 0 150w数据10min执行时间 * 使用变量写法1234567891011121314151617181920set @sameno := 1;set @oss_id := -1;set @osp_id := -1;set @pro_sku_code := \"\";set @start_time := \"1970-01-01\";select oss_id, osp_id, pro_sku_code, start_time, price_val from (select @sameno := case when @oss_id = oss_id and @osp_id = osp_id and @pro_sku_code = pro_sku_code and @start_time = start_time then @sameno + 1 else 1 end as sameno, @oss_id := oss_id oss_id, @osp_id := osp_id osp_id, @pro_sku_code := pro_sku_code pro_sku_code, @start_time := start_time start_time, price_valfrom(selectoss_id, osp_id, pro_sku_code, start_time, price_valfrom price_log where price_type = 10 and status = 1 and delete_flag = 0 order byoss_id, osp_id, pro_sku_code, start_time, update_time desc ) a) b where sameno = 1 30s左右 * ⚠️ 需要先设定头部初始变量，否则首次执行结果非预期，多次执行效果不一致，很奇怪 使用python操作 不可使用pd.read_sql_query() 有异常 1234567891011121314151617181920212223242526272829sql = ''' select oss_id, osp_id, pro_sku_code, start_time, price_val from ( select @sameno := case when @oss_id = oss_id and @osp_id = osp_id and @pro_sku_code = pro_sku_code and @start_time = start_time then @sameno + 1 else 1 end as sameno, @oss_id := oss_id oss_id, @osp_id := osp_id osp_id, @pro_sku_code := pro_sku_code pro_sku_code, @start_time := start_time start_time, price_val from (select oss_id, osp_id, pro_sku_code, start_time, price_val from price_log where price_type = 10 and status = 1 and delete_flag = 0 order by oss_id, osp_id, pro_sku_code, start_time, update_time desc ) a) b where sameno = 1 ''' db_conf = conf.db_conf oms_conf = db_conf['oms_db'] db = pymysql.connect(oms_conf['host'], oms_conf['user'], oms_conf['passwd'], 'pricing') heads = [\"set @sameno := 1;\", \"set @oss_id := -1;\", \"set @osp_id := -1;\", \"set @pro_sku_code := '';\", \"set @start_time := '1970-01-01'\"] cursor = db.cursor() for col in heads: cursor.execute(col) cursor.execute(sql) res = cursor.fetchall() 还可以使用程序内进行筛选，不过效率并不高，但是比max要好","categories":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"配置数据管理查询","slug":"配置数据管理查询","date":"2020-04-10T10:05:02.000Z","updated":"2020-04-10T10:16:55.898Z","comments":true,"path":"2020/04/10/配置数据管理查询/","link":"","permalink":"http://yoursite.com/2020/04/10/%E9%85%8D%E7%BD%AE%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86%E6%9F%A5%E8%AF%A2/","excerpt":"","text":"配置数据管理查询 背景: 数据库以及服务器的配置信息比较多，临时使用频率也高,找起来不方便 pylsy configparser catconfig.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import sysfrom pylsy import pylsytableimport configparsercp &#x3D; configparser.ConfigParser()cp.read(&quot;&#x2F;Users&#x2F;zhouke&#x2F;code&#x2F;catconfig&#x2F;catconfig.cfg&quot;)cp_sections &#x3D; cp.sections()config_dict &#x3D; &#123;&#125;for section in cp_sections: name &#x3D; section.split(&quot;:&quot;)[0] if name not in config_dict: config_dict[name] &#x3D; [] config_data &#x3D; cp._sections[section] config_dict[name].append(config_data)match_dict &#x3D; &#123; &quot;db_51&quot;:[&quot;51&quot;, &quot;生产&quot;], &quot;db_69&quot;:[&quot;db_69&quot;], &quot;db_oms&quot;:[&quot;oms&quot;], &quot;db_oil&quot;:[&quot;oil&quot;], &quot;db_oil_test&quot;:[&quot;oil_test&quot;], &quot;db_ygh&quot;:[&quot;ygh&quot;, &quot;petrol&quot;], &quot;db_cd&quot;:[&quot;db_cd&quot;, &quot;db_禅道&quot;, &quot;cd&quot;, &quot;禅道&quot;], &quot;db_yzg&quot;:[&quot;yzg&quot;], &quot;db_yzg_test&quot;:[&quot;yzg_test&quot;], &quot;mail_data&quot;:[&quot;mail&quot;], &quot;web_datav&quot;:[&quot;datav&quot;], &quot;web_ali&quot;:[&quot;zy_ali&quot;], &quot;tm&quot;:[&quot;teamview&quot;, &quot;teamviewer&quot;, &quot;tm&quot;], &quot;s_69&quot;:[&quot;s_69&quot;, &quot;69&quot;], &quot;s_jump&quot;:[&quot;jump&quot;, &quot;jumper&quot;, &quot;2222&quot;], &quot;s_cd&quot;:[&quot;s_cd&quot;, &quot;s_禅道&quot;], &quot;s_54&quot;:[&quot;54&quot;, &quot;s_54&quot;], &quot;s_56&quot;:[&quot;56&quot;, &quot;s_56&quot;, &quot;win&quot;, &quot;windows&quot;], &quot;s_ali&quot;:[&quot;ali&quot;], &quot;s_bcc&quot;:[&quot;bcc&quot;, &quot;baidu&quot;, &quot;s_bcc&quot;], &quot;db_bcc&quot;:[&quot;db_bcc&quot;], &quot;wx&quot;:[&quot;wx&quot;, &quot;wx_mail&quot;], &quot;bk&quot;:[&quot;bucket&quot;, &quot;bos&quot;], &quot;tab&quot;:[&quot;tableau&quot;, &quot;license&quot;, &quot;tab&quot;]&#125;find_dict &#x3D; &#123;&#125;for name, alias_list in match_dict.items(): for idx, alias in enumerate(alias_list): find_dict[alias] &#x3D; name find_dict[name.split(&quot;_&quot;)[0] + alias] &#x3D; namesearch_str &#x3D; sys.argv[1]if search_str not in find_dict: print(&quot;🈚️&quot;)else: config_res &#x3D; config_dict[find_dict[search_str]] table_dict &#x3D; &#123;&#125; for idx, row in enumerate(config_res): for k, v in row.items(): if k not in table_dict: table_dict[k] &#x3D; [] table_dict[k].append(v) table_head &#x3D; list(table_dict.keys()) table &#x3D; pylsytable(table_head) for k, v in table_dict.items(): table.add_data(k, v) print(table) catconfig.cfg 123456789[db_a:1]host &#x3D; 111user &#x3D; 111passwd &#x3D; 111[db_a:2]host &#x3D; 111user &#x3D; 222passwd &#x3D; 222 catconfig.sh 12#!&#x2F;bin&#x2F;bashpython &#x2F;Users&#x2F;zhouke&#x2F;code&#x2F;catconfig&#x2F;catconfig.py $1 添加全局命令 1sudo ln -s &#x2F;Users&#x2F;zhouke&#x2F;code&#x2F;catconfig&#x2F;catconfig.sh &#x2F;usr&#x2F;local&#x2F;bin&#x2F;catconfig 使用 1catconfig a ## a: 配置的映射字段 效果","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/tags/%E5%B7%A5%E5%85%B7/"}]},{"title":"uwsgi+nginx部署flask","slug":"uwsgi-nginx部署flask","date":"2020-04-10T09:54:21.000Z","updated":"2020-04-10T10:14:43.268Z","comments":true,"path":"2020/04/10/uwsgi-nginx部署flask/","link":"","permalink":"http://yoursite.com/2020/04/10/uwsgi-nginx%E9%83%A8%E7%BD%B2flask/","excerpt":"","text":"uwsgi + nginx部署flask一、安装uwsgi 使用python版本对应的pip进行安装uwsgi 1pip install uwsgi 在项目下创建uwsgi.ini文件 12345678910111213[uwsgi]http &#x3D; 0.0.0.0:5000pythonpath &#x3D; &#x2F;home&#x2F;anaconda3&#x2F;bin&#x2F;pythonchdir &#x3D; &#x2F;home&#x2F;code&#x2F;apiwsgi-file &#x3D; app.pycallable &#x3D; appprocesses &#x3D; 4threads &#x3D; 2stats &#x3D; 127.0.0.1:9191pidfile &#x3D; uwsgi.piddeamonize &#x3D; .&#x2F;log&#x2F;uwsgi.loglazy-apps &#x3D; truetouch-chain-reload &#x3D; true 后台运行uwsgi 1uwsgi -d --ini uwsgi.ini 二、安装配置nginx 下载安装包 1wget http:&#x2F;&#x2F;nginx.org&#x2F;download&#x2F;nginx-1.17.9.tar.gz 解压，不要放到/usr/local下，会安装到这个目录 1tar -zxvf nginx-1.17.9.tar.gz 查看环境是否满足 123456789101112131415cd nginx-1.17.9&#x2F;.&#x2F;configure##不出现error则满足##否则检查依赖库#依赖库安装sudo yum install gcc-c++sudo yum install pcresudo yum install pcre-develsudo yum install zlibsudo yum install zlib-develsudo yum install opensslsudo yum install openssl-devel 无错误进行编译 123makesudo make install 三、配置nginx 自定义配置文件 /usr/local/nginx/conf/conf.d/api.conf 12345678910111213141516171819202122232425262728upstream project&#123; server localhost:5000;&#125;server &#123; listen 8080; server_name IP&#x2F;localhost; access_log &#x2F;home&#x2F;api&#x2F;access.log; error_log &#x2F;home&#x2F;api&#x2F;error.log; location &#x2F; &#123; proxy_pass http:&#x2F;&#x2F;project; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504; proxy_max_temp_file_size 0; proxy_connect_timeout 90; proxy_send_timeout 90; proxy_read_timeout 90; proxy_buffer_size 4k; proxy_buffers 4 32k; proxy_busy_buffers_size 64k; proxy_temp_file_write_size 64k; &#125;&#125; 配置/usr/local/nginx/conf/nginx.conf 12##在http下面加上include &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;conf&#x2F;conf.d&#x2F;*.conf; 配置开机启动 1234567891011121314151617vim &#x2F;lib&#x2F;systemd&#x2F;system&#x2F;nginx.service[Unit]Description&#x3D;nginxAfter&#x3D;network.target[Service]Type&#x3D;forkingExecStart&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;nginxExecReload&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;nginx reloadExecStop&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;nginx quitPrivateTmp&#x3D;true[Install]WantedBy&#x3D;multi-user.targetsystemctl enable nginx.service 启动nginx 1systemctl start nginx","categories":[{"name":"nginx","slug":"nginx","permalink":"http://yoursite.com/categories/nginx/"},{"name":"uwsgi","slug":"nginx/uwsgi","permalink":"http://yoursite.com/categories/nginx/uwsgi/"}],"tags":[{"name":"uwsgi","slug":"uwsgi","permalink":"http://yoursite.com/tags/uwsgi/"},{"name":"nginx","slug":"nginx","permalink":"http://yoursite.com/tags/nginx/"}]},{"title":"Elasticsearch基础使用","slug":"Elasticsearch基础使用","date":"2020-04-02T08:23:17.000Z","updated":"2020-04-02T10:22:04.764Z","comments":true,"path":"2020/04/02/Elasticsearch基础使用/","link":"","permalink":"http://yoursite.com/2020/04/02/Elasticsearch%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/","excerpt":"","text":"Elasticsearch基础使用一、安装 需要java环境 下载安装包 1wget https:&#x2F;&#x2F;mirrors.huaweicloud.com&#x2F;elasticsearch&#x2F;7.6.1&#x2F;elasticsearch-7.6.1-linux-x86_64.tar.gz 解压，配置环境变量 12345tar -zxvf elasticsearch-7.6.1-linux-x86_64.tar.gz#~&#x2F;.bash_profileexport ELASTICSEARCH_PATH&#x3D;&#x2F;home&#x2F;kowhoy&#x2F;software&#x2F;elasticsearch-7.6.1export PATH&#x3D;$&#123;ELASTICSEARCH_PATH&#125;&#x2F;bin:$PATH 启动 1elasticsearch 检查 12#另启终端curl &#39;http:&#x2F;&#x2F;localhost:9200&#x2F;?pretty&#39; 二、基础概念 elasticsearch中的概念与关系型数据库的概念类比: 关系型数据库 elasticsearch databases indices tables types rows documents cloumns fields 三、基础使用 本篇使用的是insomnia工具目的是创建一个类似以下关系型数据库结构的Elasticsearch 类型 名称 database-name history-demo table-name shell column-1-name command column-2-name create-by 创建index 12345678910111213141516171819202122232425262728@method: PUT@url: http://localhost:9200/history_demo@return: &#123; \"acknowledged\": true, \"shards_acknowledged\": true, \"index\": \"history_demo\"&#125;or##写入数据的同时也创建index@method: PUT@url: http://localhost:9200/history_demo/shell/1@data: &#123;\"command\":\"df -h\"&#125;@return: &#123; \"_index\": \"history_demo\", \"_type\": \"shell\", \"_id\": \"1\", \"_version\": 1, \"result\": \"created\", \"_shards\": &#123; \"total\": 2, \"successful\": 1, \"failed\": 0 &#125;, \"_seq_no\": 0, \"_primary_term\": 1&#125; 查看indices 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748##查看指定的index@method: GET@url: http://localhost:9200/history_demo@return: &#123; \"history_demo\": &#123; \"aliases\": &#123;&#125;, \"mappings\": &#123; \"shell\": &#123; \"properties\": &#123; \"command\": &#123; \"type\": \"text\", \"fields\": &#123; \"keyword\": &#123; \"type\": \"keyword\", \"ignore_above\": 256 &#125; &#125; &#125; &#125; &#125; &#125;, \"settings\": &#123; \"index\": &#123; \"creation_date\": \"1585796252661\", \"number_of_shards\": \"5\", \"number_of_replicas\": \"1\", \"uuid\": \"FrQU3tV4Smu5XioGH6qUjw\", \"version\": &#123; \"created\": \"6060199\" &#125;, \"provided_name\": \"history_demo\" &#125; &#125; &#125;&#125;## 查看所有的indices@method: GET@url: http://localhost:9200/_cat/indices@remark: 使用`_cat/indices`@return:yellow open oss JYYZYHlxQsCYpUfJGjNnfw 5 1 0 0 1.2kb 1.2kbyellow open doc 0JUEuXarQ0aBaB1YHD2KcA 5 1 1 0 4.7kb 4.7kbyellow open history_demo FrQU3tV4Smu5XioGH6qUjw 5 1 1 0 4.4kb 4.4kbyellow open posts c18QVZ00Snyp1OOomxjJuA 5 1 1 0 5.5kb 5.5kbgreen open .kibana_1 jLWNf4ysRDSUvR3qfmv5Ow 1 0 2 0 9.3kb 9.3kbyellow open blog x_vNxVVHT6qgQY3_RKiaHg 5 1 6 0 28.3kb 28.3kbyellow open person 1ctpvuoqTV-vTO4q9G9cnQ 5 1 2 0 8.1kb 8.1kb 插入documents 123456789101112131415161718192021222324252627282930313233343536373839## 插入单条数据@method: PUT/POST@url: http://localhost:9200/history_demo/shell/3@data: &#123;\"command\":\"df -h\"&#125;@return: &#123; \"_index\": \"history_demo\", \"_type\": \"shell\", \"_id\": \"3\", \"_version\": 1, \"result\": \"created\", \"_shards\": &#123; \"total\": 2, \"successful\": 1, \"failed\": 0 &#125;, \"_seq_no\": 0, \"_primary_term\": 1&#125;## 插入指定ID的多条数据@method: PUT/POST@url: http://localhost:9200/history_demo/shell/_bulk@data: &#123;\"index\":&#123;\"_id\":6&#125;&#125;&#123;\"command\":\"mysql\"&#125;&#123;\"index\":&#123;\"_id\":7&#125;&#125;&#123;\"command\":\"logatsh\"&#125;#!!注意 这个json_data末尾要多留一行@return: ...## 插入不指定ID的多条数据@method: PUT/POST@url: http://localhost:9200/history_demo/shell/_bulk@data:&#123;\"index\":&#123;&#125;&#125;&#123;\"command\":\"mysql\"&#125;&#123;\"index\":&#123;&#125;&#125;&#123;\"command\":\"logatsh\"&#125; 查询 1234567891011## 查询所有的documents@method: GET@url: http://localhost:9200/history_demo/shell/_search@data: &#123;\"query\":&#123;\"match_all\":&#123;&#125;&#125;&#125;@return: ...## 按条件查询@method: GET@url: http://localhost:9200/history_demo/shell/_search@data: &#123;\"query\":&#123;\"match\":&#123;\"command\":\"mysql\"&#125;&#125;&#125;@return: ... 删除 12345678910## 指定ID进行delete@method: DELETE@url: http://localhost:9200/history_demo/shell/1@return: ...## 根据条件删除@method:POST@url: http://localhost:9200/history_demo/shell/_delete_by_query@data: &#123;\"query\":&#123;\"match\":&#123;\"command\":\"mysql\"&#125;&#125;&#125;@return: ... 修改 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051## 指定ID进行修改@method:POST@url: http://localhost:9200/history_demo/shell/2/_update@data: &#123; \"script\": &#123; \"source\":\"ctx._source.command=params.command\", \"lang\": \"painless\", \"params\": &#123; \"command\":\"history\" &#125; &#125;&#125;@return: ...@remark: ctx指当前事务, _source指当前的document, lang指使用painless脚本语言来编写script## 仅增加字段进行修改@method: POST@url: http://localhost:9200/history_demo/shell/2/_update@data: &#123; \"script\": &#123; \"source\": \"ctx._source.bash_time='2020-02-02'\" &#125;&#125;@return: ...## 仅删除字段进行修改@method: POST@url: http://localhost:9200/history_demo/shell/2/_update@data: &#123; \"script\": &#123; \"source\": \"ctx._source.remove('bash_time')\" &#125;&#125;@return: ...## 根据条件进行修改@method: POST@url: http://localhost:9200/history_demo/shell/_update_by_query@data: &#123; \"query\": &#123; \"match\":&#123;\"command\":\"df -h\"&#125; &#125;, \"script\": &#123; \"source\": \"ctx._source.command=params.command\", \"lang\": \"painless\", \"params\": &#123; \"command\": \"tail data.log\" &#125; &#125;&#125;@return: ..","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://yoursite.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://yoursite.com/tags/Elasticsearch/"}]},{"title":"flask思路整理","slug":"flask思路整理","date":"2020-03-27T10:13:06.000Z","updated":"2020-03-27T10:35:34.834Z","comments":true,"path":"2020/03/27/flask思路整理/","link":"","permalink":"http://yoursite.com/2020/03/27/flask%E6%80%9D%E8%B7%AF%E6%95%B4%E7%90%86/","excerpt":"","text":"flask思路整理一. 创建虚拟环境python3 -m venv venv. venv/bin/activate 二. init.py主要工作： 创建应用 添加配置 初始化应用 注册蓝图 创建应用: 123456import osfrom flask import Flaskdef create_app(test_config=None): ## instance_relative_config 使用True的话就会从instance文件夹作为配置文件的路径 app = Flask(__name__, instance_relative_config=True) 添加配置 12345678910111213##方式一:app.config.from_mapping( SECRET_KEY='dev' )##方式二:#silent 文件不存在是否静默报错app.config.from_pyfile('config.py', silent=True)#随机key的生成import osimport binasciibinascii.hexlify(os.urandom(16)) 初始化应用 123app.teardown_appcontext(db.close_db) ##在返回响应后进行清理调用的程序app.cli.add_command(db.init_db_command) ##添加命令行指令 注册蓝图 12from . import authapp.register_buleprint(auth.bp) 三、db.py 使用click创建命令 12345678import clickfrom flask.cli import with_appcontext@click.command('init-db')@with_appcontextdef init_db_command(): **** click.echo(\"输出内容\") 数据库连接使用g,不会每次请求都重新创建连接 1234567from flasj import current_app, gdef get_db(): if 'db' not in g: g.db = 创建连接 return g.db 关闭数据库 12345def close_db(e=None): db = g.pop('db', None) if db is not None: db.close() 四、具体逻辑 蓝图 123from flask import Blueprintbp &#x3D; Blueprint(&#39;auth&#39;, __name__, url_prefix&#x3D;&#39;&#x2F;auth&#39;) 密码相关 1from werkzeug.security import check_password_hash, generate_password_hash 保证请求之前知道当时的用户 12345678@bp.before app_requestdef load_logged_in_user(): user_id = session.get('user_id') if user_id is None: g.user = None else: g.user = get_db()..... 要求是在登录状态(使用装饰符) 123456789import functoolsdef login_required(view): @functools.wraps(view) def wrapped_view(**kwargs): if g.user is None: return redirect(url_for(\"auth.login\")) return view(**kwargs) return wrapped_view 五、项目可安装 setup.py 123456789101112from setuptools import find_packages, setupsetup( name='flaskr', version='1.0.0', packages=find_packages(), include_package_data=True, zip_safe=False, install_requires=[ 'flask', ],) MANIIFEST.in 1234include flaskr&#x2F;schema.sqlgraft flaskr&#x2F;staticgraft flaskr&#x2F;templatesglobal-exclude *.pyc 可以使用pip install -e .安装项目 六、部署产品 构建 123pip install wheelpython setup.py bdist_wheel 安装将生成的.whl文件发送到服务求 1pip install ***.whl 更改配置是在venv/var/***-instance/下 七、运行产品服务器12pip iinstall waitresswaitress-serve --call &#39;appname:create_app&#39; 启动应用 12345export FLASK_APP&#x3D;APP_NAMEexport FLASK_ENV&#x3D;developmentflask runflask init-db","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"flask","slug":"flask","permalink":"http://yoursite.com/tags/flask/"},{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"tmux——基本使用示例","slug":"tmux——基本使用示例","date":"2020-03-27T04:00:05.000Z","updated":"2020-03-27T04:09:32.564Z","comments":true,"path":"2020/03/27/tmux——基本使用示例/","link":"","permalink":"http://yoursite.com/2020/03/27/tmux%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B/","excerpt":"","text":"tmux——基本使用示例 使用tmux后台运行bash_history_notify项目 一、安装tmux12345#mac:brew install tmux#centos:yum install tmux 二、配置tmux 使用了github上的配置，然后做了修改tmux的默认快捷键前缀为 ctrl + b, 现在改为ctrl+a，下文中使用Prefix代替ctrl+a⚠️ :使用快捷键是先摁下快捷键前缀，然后抬手，在进行具体的快捷按键 1234cdgit clone https:&#x2F;&#x2F;github.com&#x2F;gpakosz&#x2F;.tmux.gitln -s -f .tmux&#x2F;.tmux.confcp .tmux&#x2F;.tmux.conf.local . 修改快捷键前缀 vim .tmux/.tmux.conf.local去掉290行附近的注释符 1234unbind C-aunbind C-bset -g prefix C-abind C-a send-prefix 三、使用tmux开启任务 bcc_00创建kafka_server的sessionn1tmux new -s kafka_server 使用Prefix+%分屏，使用Prefix+方向键切换鼠标所在窗格 在窗格1中启动zookeeper 12cd app&#x2F;kafka_2.12-2.4.0&#x2F;.&#x2F;bin&#x2F;zookeeper-server-start.sh .&#x2F;config&#x2F;zookeeper.properties 在窗格2中启动kafka 12cd app&#x2F;kafka_2.12-2.4.0&#x2F;.&#x2F;bin&#x2F;kafka-server-start.sh .&#x2F;config&#x2F;server.properties ) 使用Prefix+c 创建新窗口,并创建3个窗格,启动bash_history的消费端、生产端、实验窗口 在窗格1中启动消费端 12cd ~&#x2F;code&#x2F;python&#x2F;bash_history&#x2F;python bash_history_consumer.py 在窗格2中启动生产端 12cd ~&#x2F;code&#x2F;python&#x2F;bash_history&#x2F;python bash_history_producer.py ~&#x2F;.bash_history 在窗格3中随便输入命令，检查另外两个窗格是否有输出 bcc_01上开启生产端同上步骤，bcc_01上没有消费端 使用Prefix + d 退出session 使用tmux ls 查看session， 使用tmux at -t &lt;session_name&gt;连接session","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[]},{"title":"kafak示例-远程监控终端输入","slug":"kafak示例-远程监控终端输入","date":"2020-03-26T03:52:54.000Z","updated":"2020-03-26T03:52:54.112Z","comments":true,"path":"2020/03/26/kafak示例-远程监控终端输入/","link":"","permalink":"http://yoursite.com/2020/03/26/kafak%E7%A4%BA%E4%BE%8B-%E8%BF%9C%E7%A8%8B%E7%9B%91%E6%8E%A7%E7%BB%88%E7%AB%AF%E8%BE%93%E5%85%A5/","excerpt":"","text":"kafak示例-远程监控终端输入 使用pyinotify监听.bash_history文件，使用kafka生产消费 一、kafka安装配置 安装java环境 官网下载kafka安装包 1wget http:&#x2F;&#x2F;archive.apache.org&#x2F;dist&#x2F;kafka&#x2F;1.0.0&#x2F;kafka_2.12-1.0.0.tgz 解压压缩包 配置/config/server.properties 1advertised.listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;&#123;&#123;IP&#125;&#125;:9092 启动kafka 12345##zookeeper启动.&#x2F;bin&#x2F;zookeeper-server-start.sh .&#x2F;config&#x2F;zookeeper.properties ##server启动.&#x2F;bin&#x2F;kafka-server-start.sh .&#x2F;config&#x2F;server.properties 创建topic 1.&#x2F;bin&#x2F;kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic &#123;&#123;topic_name&#125;&#125; 二、安装python依赖12345#kafka-pythonpip install kafka-python#pyinotifypip install pyinotify 三、服务器端代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495# -*- coding: utf-8 -*-# @Description: 监听文件变更向kafka发送# @Author: kowhoy# @Last Modified by: kowhoy# @filename: bash_history_producer.pyimport pyinotifyimport timeimport osimport sysimport jsonfrom kafka import KafkaProducerimport datetimeimport socketbootstrap_servers = \"ip:9092\"topic = \"bash_history\"host_name = \"bcc_00\"host_ip = socket.gethostbyname(socket.getfqdn(socket.gethostname()))'''@name: [class]Monitor_file@date: 2020-03-26 09:41:13@desc: 监听文件@param: [str]filename @return: '''class Monitor_file: def notify_bash_history(self, filename): wm = pyinotify.WatchManager() notifier = pyinotify.Notifier(wm) wm.watch_transient_file(filename, pyinotify.IN_MODIFY, Process_transient_file) notifier.loop()'''@name: [class]Process_transient_file@date: 2020-03-26 09:44:29@desc: 文件发生变化触发@param: @return: '''class Process_transient_file(pyinotify.ProcessEvent): def process_IN_MODIFY(self, event): line = file.readlines() if line: if len(line) == 2: order_time = int(line[0][1:-1]) order_time_array = time.localtime(order_time) order_time_str = time.strftime(\"%Y-%m-%d %H:%M:%S\", order_time_array) order = line[1][:-1] msg = &#123; \"host_name\": host_name, \"host_ip\": host_ip, \"order_time\": order_time_str, \"order\": order &#125; Send_to_kafka().send_msg(msg)'''@name: [class]Send_to_kafka@date: 2020-03-26 09:47:50@desc: 向kafka发送msg@param:@return: '''class Send_to_kafka(): def __init__(self): self.producer = KafkaProducer(bootstrap_servers=bootstrap_servers) def send_msg(self, msg): msg = json.dumps(msg, ensure_ascii=False).encode(\"utf-8\") self.producer.send(topic, msg) print(\"-\"*10, \"数据已发送\", \"-\"*10) print(msg) self.producer.flush()if __name__ == \"__main__\": filename = sys.argv[1] if not os.path.exists(filename): raise FileExistsError file_stat = os.stat(filename) file_size = file_stat[6] file = open(filename, \"r\") file.seek(file_size) Monitor_file().notify_bash_history(filename) 四、客户端代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697# -*- coding: utf-8 -*-# @Description: # @Author: kowhoy# @Date: 2020-03-26 10:15:24# @Last Modified time: 2020-03-26 11:25:15from kafka import KafkaConsumerfrom kafka.structs import TopicPartitionimport pandas as pdfrom sqlalchemy import create_engineimport jsonimport sysimport osdbh_config = &#123; \"host\": \"localhost\", \"user\": \"root\", \"passwd\": \"passwd\", \"database\": \"bash_history\", \"table\": \"bash_history_log\"&#125;save_to_tb = True ## 是否存数据库topic = \"bash_history\"bootstrap_servers = [\"ip:9092\"]single_file_size = 1 #Mclass Bash_history_consumer: def __init__(self): self.topic = \"bash_history\" self.consumer = KafkaConsumer(bootstrap_servers=bootstrap_servers) self.log_file_path = \"./bash_history_log/\" log_files = os.listdir(self.log_file_path) max_log_count = max([int(filename.split(\"_\")[-1]) for filename in log_files]) if len(log_files) &gt; 0 else -1 self.count = max_log_count + 1 self.save_list = [] if save_to_tb: e = \"mysql+pymysql://%s:%s@%s/%s\"%(dbh_config[\"user\"], dbh_config[\"passwd\"], dbh_config[\"host\"], dbh_config[\"database\"]) self.dbh = create_engine(e) empty_sql = 'select * from information_schema.TABLES where TABLE_SCHEMA = \"&#123;db&#125;\" and TABLE_NAME = \"&#123;tb&#125;\"'.\\ format(db=dbh_config[\"database\"], tb=dbh_config[\"table\"]) empty_df = pd.read_sql_query(empty_sql, self.dbh) self.tb_empty = empty_df.empty def bash_consumer(self): self.consumer.assign([TopicPartition(topic=topic, partition=0)]) for msg in self.consumer: msg_offset = msg.offset msg_value = (msg.value).decode(\"utf-8\") msg_value = json.loads(msg_value) self.save_list.append(msg_value) save_data_size = sys.getsizeof(self.save_list) if save_data_size &gt;= single_file_size * 1024 * 1024: save_file = self.log_file_path + \"bash_history_log_\" + str(self.count) self.count += 1 with open(save_file, \"w+\") as f: for line in self.save_list: f.write(line.decode(\"utf-8\")+\"\\n\") self.save_list = [] print(\"*\"*10, \"写入文件\", \"*\"*10) if save_to_tb: if self.tb_empty: add_way = \"replace\" self.tb_empty = False else: add_way = \"append\" for k, v in msg_value.items(): msg_value[k] = [v] insert_df = pd.DataFrame.from_dict(msg_value) insert_df.to_sql(dbh_config[\"table\"], self.dbh, if_exists=add_way, index=False) print(\"当前size\", save_data_size, \"\\t\", msg_value)if __name__ == \"__main__\": Bash_history_consumer().bash_consumer() 五、开启任务 我是使用bcc_00作为唯一的消费者，bcc_00和bcc_01作为两个生产者，修改好代码中的配置数据 bcc_00开启消费者 1python bash_history_consumer.py bcc_00 和 bcc_01分别开启生产者 1python bash_history_producer.py ~&#x2F;.bash_history 新起窗口进行命令行操作，就可以在控制台和数据库看到相关bash 00_consumer: 00_producer: 01_producer: sql_table(两台bcc内网ip一样):","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/tags/kafka/"},{"name":"pyinotify","slug":"pyinotify","permalink":"http://yoursite.com/tags/pyinotify/"}]},{"title":"mac下chromedriver升级","slug":"mac下chromedriver升级","date":"2020-03-13T02:50:14.000Z","updated":"2020-03-13T02:50:14.554Z","comments":true,"path":"2020/03/13/mac下chromedriver升级/","link":"","permalink":"http://yoursite.com/2020/03/13/mac%E4%B8%8Bchromedriver%E5%8D%87%E7%BA%A7/","excerpt":"","text":"下载与本地chrome版本一致的chromedriver,下载地址[http://chromedriver.storage.googleapis.com/index.html] 解压到本地的 /usr/local/bin 检查版本 chromedirver -v","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/tags/%E5%B7%A5%E5%85%B7/"}]},{"title":"scala伴生类和伴生对象","slug":"scala伴生类和伴生对象","date":"2020-02-23T11:36:55.000Z","updated":"2020-02-23T11:36:55.191Z","comments":true,"path":"2020/02/23/scala伴生类和伴生对象/","link":"","permalink":"http://yoursite.com/2020/02/23/scala%E4%BC%B4%E7%94%9F%E7%B1%BB%E5%92%8C%E4%BC%B4%E7%94%9F%E5%AF%B9%E8%B1%A1/","excerpt":"","text":"Scala伴生类和伴生对象 定义： 在Scala中，类名和对象名一致时，互为伴生类和伴生对象。 例子:123456789// 伴生类class ApplyTest &#123;&#125;// 伴生对象object ApplyTest &#123;&#125; 使用: 通常在伴生对象中写apply()方法，然后在apply方法中实例伴生类，然后每次使用类名称()的时候，就已经实例化了类 例子:12345678910111213141516object ApplyApp &#123; def main(args:Array[String]):Unit = &#123; val c = ApplyTest() println(c) &#125;&#125;class ApplyTest &#123;&#125;object ApplyTest &#123; def apply() = &#123; new ApplyTest() &#125;&#125;","categories":[{"name":"Scala","slug":"Scala","permalink":"http://yoursite.com/categories/Scala/"}],"tags":[{"name":"伴生","slug":"伴生","permalink":"http://yoursite.com/tags/%E4%BC%B4%E7%94%9F/"}]}]}