{"meta":{"title":"onestarko","subtitle":"","description":"","author":"kowhoy","url":"http://yoursite.com","root":"/"},"pages":[{"title":"categories","date":"2020-02-15T00:58:48.000Z","updated":"2020-02-15T00:59:32.763Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2020-02-15T00:59:39.000Z","updated":"2020-02-15T01:57:12.301Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"MySQL题（1）","slug":"MySQL题（1）","date":"2020-07-28T06:11:56.000Z","updated":"2020-07-28T06:11:56.240Z","comments":true,"path":"2020/07/28/MySQL题（1）/","link":"","permalink":"http://yoursite.com/2020/07/28/MySQL%E9%A2%98%EF%BC%881%EF%BC%89/","excerpt":"","text":"MySQL题问题:1234567891011121314151617181920有两类应用，游戏和工具，游戏类的应用有：A1game1, Bg2ame2, Cga3me3, Dgam4e4, Egame5, Fgame6, ..., Tgame10工具类的应用有:A1tool1, Bt2ool2, Cto3ol3, Dtoo4l4, Etool5, Ftool6, Gtool7, Htool8有一张用户安装应用列表(表名: user_install_app)+---------+-----------------------------------------------------------+| user_id | app_name |+---------+-----------------------------------------------------------+| 1 | A1game1, Bg2ame2, Cga3me3, A1tool1, Cto3ol3, Gtool7 || 2 | A1game1, Fgame6, Cga3me3, Egame5, Bt2ool2, Ftool6, Gtool7 || 3 | Cga3me3 || 4 | Dtoo4l4 |+---------+-----------------------------------------------------------+1&gt; 同时装有Cga3me3 和 Bt2ool2的用户2&gt; 同时装有游戏和工具类 (且没有安装Cto3ol3) 的用户分别安装游戏和工具类应用的个数 思路: 原始数据是每个用户一行数据，后面的app_name使用’,’进行了分隔，可以先将数据进行分成多行数据， 方便进行统计。 解答:1. 将数据分隔成多行1234567891011create temporary table user_app (user_id int(11) not null, app_name varchar(50) not null);insert into user_app (SELECT a.user_id, trim( substring_index( SUBSTRING_INDEX( app_name, ',', help_topic_id + 1 ), ',', - 1 ) ) app_name FROM user_install_app a, mysql.help_topic b WHERE b.help_topic_id &lt; LENGTH(a.app_name) - length(REPLACE ( a.app_name, ',', '' )) + 1); 123456789101112131415161718192021select * from user_app;+---------+----------+| user_id | app_name |+---------+----------+| 1 | A1game1 || 1 | Bg2ame2 || 1 | Cga3me3 || 1 | A1tool1 || 1 | Cto3ol3 || 1 | Gtool7 || 2 | A1game1 || 2 | Fgame6 || 2 | Cga3me3 || 2 | Egame5 || 2 | Bt2ool2 || 2 | Ftool6 || 2 | Gtool7 || 3 | Cga3me3 || 4 | Dtoo4l4 |+---------+----------+ 2. 问题一： 同时装有Cga3me3 和 Bt2ool2的用户1234567select user_id, count(*) num from user_app where app_name &#x3D; &#39;Cga3me3&#39; or app_name &#x3D; &#39;Bt2ool2&#39; group by user_id having num &gt; 1;+---------+-----+| user_id | num |+---------+-----+| 2 | 2 |+---------+-----+ 3. 问题二: 同时装有游戏和工具类 (且没有安装Cto3ol3) 的用户分别安装游戏和工具类应用的个数1234567891011121314151617181920212223242526272829303132333435363738391. 增加一列app_typecreate temporary table user_app_v2 (user_id int(11) not null, app_name varchar(50) not null, app_type varchar(20) not null);insert into user_app_v2 (select *, case when app_name like &#39;%g%&#39; then &#39;game&#39; else &#39;tool&#39; end as &#39;app_type&#39; from user_app);select * from user_app_v2;+---------+----------+----------+| user_id | app_name | app_type |+---------+----------+----------+| 1 | A1game1 | game || 1 | Bg2ame2 | game || 1 | Cga3me3 | game || 1 | A1tool1 | tool || 1 | Cto3ol3 | tool || 1 | Gtool7 | game || 2 | A1game1 | game || 2 | Fgame6 | game || 2 | Cga3me3 | game || 2 | Egame5 | game || 2 | Bt2ool2 | tool || 2 | Ftool6 | tool || 2 | Gtool7 | game || 3 | Cga3me3 | game || 4 | Dtoo4l4 | tool |+---------+----------+----------+2. 统计select user_id, count(distinct app_type) num from user_app_v2 where app_name !&#x3D; &#39;Cga3me3&#39; group by user_id having num &gt; 1;+---------+-----+| user_id | num |+---------+-----+| 1 | 2 || 2 | 2 |+---------+-----+","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"}]},{"title":"Python数据结构的性能分析 ( Dict )","slug":"Python数据结构的性能分析-Dict","date":"2020-07-24T03:37:17.000Z","updated":"2020-07-24T03:37:17.518Z","comments":true,"path":"2020/07/24/Python数据结构的性能分析-Dict/","link":"","permalink":"http://yoursite.com/2020/07/24/Python%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%9A%84%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90-Dict/","excerpt":"","text":"Dict的性能分析 操作 时间效率 复制 O(n) 访问 O(1) 赋值 O(1) 删除 O(1) 包含(in) O(1) 迭代 O(n) Dict 和 List 查找效率对比 List的时间效率为O(n), Dict的时间效率为O(1) 验证12345678910111213141516171819202122232425262728293031323334import timeitimport randomimport matplotlib.pyplot as pltfig, ax = plt.subplots()x_list = []ls_consume_list = []d_consume_list = []for i in range(10000, 100000001, 20000): x_list.append(i) t = timeit.Timer(\"random.randrange(%d) in x\" % i, \"from __main__ import random, x\") x = list(range(i)) ls_consume = t.timeit(1000) ls_consume_list.append(ls_consume) x = &#123;j: None for j in range(i)&#125; d_consume = t.timeit(1000) d_consume_list.append(d_consume) print(\"%d, %10.3f, %10.3f\" % (i, ls_consume, d_consume)) ax.cla() ax.plot(x_list, ls_consume_list, 'bo', 5) ax.plot(x_list, d_consume_list, 'r*', 5) plt.pause(0.1) 结论 List的查询元素的复杂度为O(n)， Dict查询元素的复杂度为O(1)","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"Python数据结构的性能分析 ( List )","slug":"Python数据结构的性能分析-List","date":"2020-07-24T02:41:39.000Z","updated":"2020-07-24T02:41:39.215Z","comments":true,"path":"2020/07/24/Python数据结构的性能分析-List/","link":"","permalink":"http://yoursite.com/2020/07/24/Python%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%9A%84%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90-List/","excerpt":"","text":"List的性能分析 对列表进行扩充，通常有两种方式。1&gt; 使用append 2&gt;使用串联运算符+append的复杂度为O(1), 串联运算符的复杂度为O(k), k为被连接列表的长度 生成简单的列表有两种方式。 1&gt; [i for i in range(100)] 2&gt; list(range(100))list()方式的效率更高 对数据进行pop操作， pop()的复杂度为O(1), pop(i) 的复杂度为O(n) 验证一123456789101112131415161718192021222324252627282930313233import timeitfrom timeit import Timerdef test1(): l = [] for i in range(100): l = l + [i]def test2(): l = [] for i in range(100): l.append(i)def test3(): [i for i in range(100)]def test4(): list(range(100))t1 = Timer(\"test1()\", \"from __main__ import test1\")t2 = Timer(\"test2()\", \"from __main__ import test2\")t3 = Timer(\"test3()\", \"from __main__ import test3\")t4 = Timer(\"test4()\", \"from __main__ import test4\")print('test1: ', t1.timeit(10000))print('test2: ', t2.timeit(10000))print('test3: ', t3.timeit(10000))print('test4: ', t4.timeit(10000))# test1: 0.210552271# test2: 0.064808912# test3: 0.03384729600000003# test4: 0.01063408199999999 验证二12345678910111213141516171819202122232425262728import timeitfrom timeit import Timerimport matplotlib.pyplot as pltpop_start = Timer(\"ls.pop(0)\", \"from __main__ import ls\")pop_end = Timer(\"ls.pop()\", \"from __main__ import ls\")fig, ax = plt.subplots()x_list = []pop_start_consume = []pop_end_consume = []for i in range(1000000, 100000001, 1000000): x_list.append(i) ls = list(range(i)) ps = pop_start.timeit(1000) ls = list(range(i)) pe = pop_end.timeit(1000) pop_start_consume.append(ps) pop_end_consume.append(pe) ax.cla() ax.plot(x_list, pop_start_consume, 'bo', 5) ax.plot(x_list, pop_end_consume, 'r*', 5) plt.pause(0.1) print(\"%15.5f, %15.5f\" % (ps, pe)) 结论 同样进行1w次扩充操作，append的效率比+快了3～4倍 同样进行1w次创建列表的操作， list()方式的速度快了3倍 pop()的复杂度为O(1)， pop(i)的复杂度为O(n)","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"PageRank(1)","slug":"PageRank-1","date":"2020-07-17T10:01:31.000Z","updated":"2020-07-17T10:01:31.064Z","comments":true,"path":"2020/07/17/PageRank-1/","link":"","permalink":"http://yoursite.com/2020/07/17/PageRank-1/","excerpt":"","text":"1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192# -*- coding: utf-8 -*-# @Description:# @Name: pages_rank# @Author: kowhoy# @Date: 2020/7/17import numpy as npimport itertoolsclass PageRank: def __init__(self): self.nodes_dict = &#123;&#125; self.nodes_set = set() self.network_path = './pages.data' ''' * @Desc: 根据文件设置对象 * @Date: 5:51 下午 2020/7/17 ''' def _set_nodes(self): with open(self.network_path, 'r') as f: lines = f.readlines() for line in lines: line_arr = line.split(' -&gt; ') to_ch = line_arr[0] from_ch = line_arr[1].replace('\\n', '') self.nodes_set.add(from_ch) self.nodes_set.add(to_ch) if from_ch not in self.nodes_dict: self.nodes_dict[from_ch] = &#123;'from': 0, 'to': 0, 'from_chs': []&#125; if to_ch not in self.nodes_dict: self.nodes_dict[to_ch] = &#123;'from': 0, 'to': 0, 'from_chs': []&#125; self.nodes_dict[from_ch]['from'] += 1 self.nodes_dict[to_ch]['to'] += 1 self.nodes_dict[from_ch]['from_chs'].append(to_ch) ''' * @Desc: 根据节点对象计算概率矩阵 * @Date: 5:52 下午 2020/7/17 ''' def _set_matrix(self): nodes_list = sorted(self.nodes_set) combines = itertools.product(nodes_list, nodes_list) matrix = np.zeros([len(nodes_list), len(nodes_list)]) for combine in combines: tc = combine[0] fc = combine[1] loc = nodes_list.index(tc), nodes_list.index(fc) v = 0 if fc in self.nodes_dict[tc]['from_chs']: v = 1 / self.nodes_dict[fc]['to'] matrix[loc[0], loc[1]] = v return matrix ''' * @Desc: 计算PR * @Date: 5:53 下午 2020/7/17 ''' def _compute_page_rank(self, rate, matrix, threshold): nodes_len = len(self.nodes_set) r0 = np.zeros(nodes_len) r1 = np.ones(nodes_len) while np.sum(np.abs(r0 - r1)) &gt;= threshold: r0 = r1.copy() r1 = (1 - rate) / nodes_len + rate * np.dot(matrix, r1) print(r1) return r1 def main(self): self._set_nodes() matrix = self._set_matrix() result = self._compute_page_rank(0.5, matrix, .0001) print(\"result:\", result)if __name__ == '__main__': PageRank().main()","categories":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"PageRank","slug":"PageRank","permalink":"http://yoursite.com/tags/PageRank/"}]},{"title":"Pandas的DataFrame数据持久化方案比较","slug":"Pandas的DataFrame数据持久化方案比较","date":"2020-07-13T04:04:43.000Z","updated":"2020-07-13T05:32:39.537Z","comments":true,"path":"2020/07/13/Pandas的DataFrame数据持久化方案比较/","link":"","permalink":"http://yoursite.com/2020/07/13/Pandas%E7%9A%84DataFrame%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96%E6%96%B9%E6%A1%88%E6%AF%94%E8%BE%83/","excerpt":"","text":"Pandas的DataFrame数据持久化方案比较 统一使用30个字段，44w行的订单表 一、使用CSV文件123456789101112131415import pandas as pdfrom sqlalchemy import create_engineimport oscache_file = \"./cache.csv\"if os.path.exists(cache_file): df = pd.read_csv(cache_file)else: dbh = create_engine(\"mysql+pymysql://user:passwd@ip/db\") df = pd.read_sql_query(\"select * from orders\", dbh) df.to_csv(cache_file, encoding=\"utf8\", index=False) print(df.head(10)) 首次耗时：37.85s读取耗时：3.38s 二、使用pickle对象存储123456789101112131415161718import pandas as pdimport picklefrom sqlalchemy import create_engineimport oscache_file = \"./cache\"if os.path.exists(cache_file): with open(cache_file, \"rb\") as f: df = pickle.load(df)else: dbh = create_engine(\"mysql+pymysql://user:passwd@ip/db\") df = pd.read_sql_query(\"select * from order_profit\", dbh) with open(cache_file, \"wb\") as f: pickle.dump(df, f) print(df.head(10)) 首次耗时：24.86s读取耗时：1.11s 三、使用HDFStore存储12345678910111213141516import pandas as pdfrom sqlalchemy import create_enginecache_file = \"./orders.h5\"store = pd.HDFStore(cache_file)if \"orders\" in store: df = store[\"orders\"]else: dbh = create_engine(\"mysql+pymysql://user:passwd@ip/db\") df = pd.read_sql_query(\"select * from order_profit\", dbh) store[\"orders\"] = df print(df.head(10)) 首次耗时：24.85s读取耗时: 1.78s 四、总结 对于Pandas的DataFrame进行持久化缓存，使用了上面三种方式进行比较，除此之外还尝试了使用Alluxio的存储方式，差异不大。比较得出使用对象存储速度最快。","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"Pandas","slug":"Pandas","permalink":"http://yoursite.com/tags/Pandas/"}]},{"title":"SparkSQL 窗口函数","slug":"SparkSQL-窗口函数","date":"2020-07-10T08:43:05.000Z","updated":"2020-07-10T08:43:05.858Z","comments":true,"path":"2020/07/10/SparkSQL-窗口函数/","link":"","permalink":"http://yoursite.com/2020/07/10/SparkSQL-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0/","excerpt":"","text":"SparkSQL 窗口函数一、窗口函数的定义和与其他函数的区别 普通函数 : 作用于每一条记录，依赖于本条记录来计算出一个新列，记录数不变 聚合函数 : 作用于一组记录，计算出一个聚合值，记录数变少 窗口函数 : 作用于每一条记录，依赖于窗口中的记录来计算出一个新列，记录数不变 二、窗口函数语法结构12345&lt;窗口函数&gt;([参数]) over ([partition by &lt;分组列&gt;][order by &lt;排序列&gt;] [asc&#x2F;desc](rows | range) &lt;范围条件&gt;) 1. 窗口函数 ranking functions: 排序类函数, row_number(), rank(), dense_rank(), percent_rank(), ntile(n) analytic functions: 统计比较类函数, cume_dist, lag, lead, first_value, last_value aggregate functions: 聚合类函数, avg, sum, min, max 2. over 关键字，说明这是一个窗口函数 3. partition by 分组4. order by 排序5.rows | ranges 控制窗口边界 rows: 表示物理窗口，就是排序之后的index range: 是根据记录中具体的字段值进行比较确定边界 三、Ranking Functions func guide row_number 组内加index rank 组内排名，不会跳过并列的情况，比如两个并列第一，则接下来的rank为3，而不是2 dense_rank 同rank，会跳过并列的情况，两个并列第一，接下来的rank为2 percent_rank (组内排名 - 1) / (组内行数 - 1) ntile(n) 将组内数据分成n桶，该条记录所在的桶号(从1计数) 12345678spark.sql( s\"\"\" |select name, department, salary, row_number() over (partition by department order by salary desc) as row_number |, rank() over (partition by department order by salary desc) as rank |, dense_rank() over (partition by department order by salary desc) as dense_rank |, ntile(2) over (partition by department order by salary desc) as ntile |from salary |\"\"\".stripMargin).show() 123456789101112131415+--------+----------+------+----------+----+----------+-----+| name|department|salary|row_number|rank|dense_rank|ntile|+--------+----------+------+----------+----+----------+-----+| Berni| Sales| 4700| 1| 1| 1| 1|| Tom| Sales| 4500| 2| 2| 2| 1||Guoxiang| Sales| 4200| 3| 3| 3| 1|| Georgi| Sales| 4200| 4| 3| 3| 2|| Kyoichi| Sales| 3000| 5| 5| 4| 2|| Berni| Sales| 0| 6| 6| 5| 2|| Sumant| Finance| 3900| 1| 1| 1| 1|| Anneke| Finance| 3300| 2| 2| 2| 1|| Parto| Finance| 2700| 3| 3| 3| 2|| Jeff| Marketing| 3100| 1| 1| 1| 1||Patricio| Marketing| 2500| 2| 2| 2| 2|+--------+----------+------+----------+----+----------+-----+ 四、Analytic Functions func guide cume_dist 组内 &lt;= 本条数值的行数 / 组内总行数 lag(col, [n, default]) 当前的index &lt; n 的话， 返回 default(默认为null), 否则返回 col的值 lead 类似lag，超前 first_value 取分组内排序后，截止到当前行，第一个值 last_value 取分组内排序后, 截止到当前行, 最后一个值 12345678910spark.sql( \"\"\" |select name, department, salary, cume_dist() over (partition by department order by salary) as cume_dist |, lag(salary, 1) over (partition by department order by salary) as lag |, lag(salary, 2) over (partition by department order by salary) as lag_2 |, lead(salary, 1) over (partition by department order by salary) as lead_1 |, first_value(salary) over (partition by department order by salary) as first_value |, last_value(salary) over (partition by department order by salary) as last_value |from salary |\"\"\".stripMargin).show() 123456789101112131415+--------+----------+------+-------------------+----+-----+------+-----------+----------+| name|department|salary| cume_dist| lag|lag_2|lead_1|first_value|last_value|+--------+----------+------+-------------------+----+-----+------+-----------+----------+| Berni| Sales| 0|0.16666666666666666|null| null| 3000| 0| 0|| Kyoichi| Sales| 3000| 0.3333333333333333| 0| null| 4200| 0| 3000||Guoxiang| Sales| 4200| 0.6666666666666666|3000| 0| 4200| 0| 4200|| Georgi| Sales| 4200| 0.6666666666666666|4200| 3000| 4500| 0| 4200|| Tom| Sales| 4500| 0.8333333333333334|4200| 4200| 4700| 0| 4500|| Berni| Sales| 4700| 1.0|4500| 4200| null| 0| 4700|| Parto| Finance| 2700| 0.3333333333333333|null| null| 3300| 2700| 2700|| Anneke| Finance| 3300| 0.6666666666666666|2700| null| 3900| 2700| 3300|| Sumant| Finance| 3900| 1.0|3300| 2700| null| 2700| 3900||Patricio| Marketing| 2500| 0.5|null| null| 3100| 2500| 2500|| Jeff| Marketing| 3100| 1.0|2500| null| null| 2500| 3100|+--------+----------+------+-------------------+----+-----+------+-----------+----------+ 五、Aggregate Functions func guide sum 求和，不指定边界按值进行累计求和 avg 求平均, 同上 max 求最大, 同上 min 求最小, 同上 123456789spark.sql( \"\"\" |select |name, department, salary, sum(salary) over (partition by department order by salary) as sum, |avg(salary) over (partition by department order by salary) as avg, |min(salary) over (partition by department order by salary) as min, |max(salary) over (partition by department order by salary) as max |from salary |\"\"\".stripMargin).show() 123456789101112131415+--------+----------+------+-----+------------------+----+----+| name|department|salary| sum| avg| min| max|+--------+----------+------+-----+------------------+----+----+| Berni| Sales| 0| 0| 0.0| 0| 0|| Kyoichi| Sales| 3000| 3000| 1500.0| 0|3000||Guoxiang| Sales| 4200|11400| 2850.0| 0|4200|| Georgi| Sales| 4200|11400| 2850.0| 0|4200|| Tom| Sales| 4500|15900| 3180.0| 0|4500|| Berni| Sales| 4700|20600|3433.3333333333335| 0|4700|| Parto| Finance| 2700| 2700| 2700.0|2700|2700|| Anneke| Finance| 3300| 6000| 3000.0|2700|3300|| Sumant| Finance| 3900| 9900| 3300.0|2700|3900||Patricio| Marketing| 2500| 2500| 2500.0|2500|2500|| Jeff| Marketing| 3100| 5600| 2800.0|2500|3100|+--------+----------+------+-----+------------------+----+----+ 六、窗口边界 ROWS ｜ RANGE 控制窗口边界 ROWS: 表示物理窗口，就是依据排序后的index来划分 RANGES: 表示逻辑窗口，是根据记录字段的实际值进行比较划分的 语法： over (partition by … order by … between {start} and {end}) 五种边界 current row : 当前行 unbounded preceding : 分区第一行 unbounded following : 分区最后一行 n preceding : 当前行向前n行 n following : 当前行向后n行 unbounded : 起点 1234567spark.sql( \"\"\" |select |name, department, salary, sum(salary) over (partition by department order by salary rows between 1 preceding and 2 following) as rows, |sum(salary) over (partition by department order by salary range between 1 preceding and 2 following) as range |from salary |\"\"\".stripMargin).show() 123456789101112131415+--------+----------+------+-----+-----+| name|department|salary| rows|range|+--------+----------+------+-----+-----+| Berni| Sales| 0| 7200| 0|| Kyoichi| Sales| 3000|11400| 3000||Guoxiang| Sales| 4200|15900| 8400|| Georgi| Sales| 4200|17600| 8400|| Tom| Sales| 4500|13400| 4500|| Berni| Sales| 4700| 9200| 4700|| Parto| Finance| 2700| 9900| 2700|| Anneke| Finance| 3300| 9900| 3300|| Sumant| Finance| 3900| 7200| 3900||Patricio| Marketing| 2500| 5600| 2500|| Jeff| Marketing| 3100| 5600| 3100|+--------+----------+------+-----+-----+","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"窗口函数","slug":"窗口函数","permalink":"http://yoursite.com/tags/%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0/"},{"name":"SparkSQL","slug":"SparkSQL","permalink":"http://yoursite.com/tags/SparkSQL/"}]},{"title":"SparkStreaming流处理分析订单日志","slug":"SparkStreaming流处理分析订单日志","date":"2020-07-09T03:06:17.000Z","updated":"2020-07-09T03:06:17.576Z","comments":true,"path":"2020/07/09/SparkStreaming流处理分析订单日志/","link":"","permalink":"http://yoursite.com/2020/07/09/SparkStreaming%E6%B5%81%E5%A4%84%E7%90%86%E5%88%86%E6%9E%90%E8%AE%A2%E5%8D%95%E6%97%A5%E5%BF%97/","excerpt":"","text":"SparkStreaming流处理分析订单日志一、项目说明 订单数据发送到Kafka中，SparkStreaming消费Kafka数据，以天/时/分钟的维度统计下单单数、成交单数、成交金额，将结果写入到Redis中 二、模块分解 基础模块（配置项类、Redis连接池） mock 订单JSON数据发送到Kafka 消费Kafka进行统计，使用Zookeeper和Kafka两种方式进行Offset管理 储存统计结果到Redis 三、基础模块1. 基础项类 使用com.typesafe:config 工具类 123456789101112131415161718/*** * @Desc 根据键值获取配置项 * @Date 9:36 上午 2020/7/3 * @Param [key] * @Return java.lang.String **/ def getConfigFiled(key: String): String = &#123; val conf = ConfigFactory.load() var value: String = null; try &#123; value = conf.getString(key) &#125;catch &#123; case _:Exception =&gt; System.err.println(key + \" not exists\") &#125; value &#125; 2. Redis连接池1234567891011121314151617/*** * @Desc 获取REDIS连接 * @Date 11:23 上午 2020/7/7 * @Param [] * @Return redis.clients.jedis.Jedis **/ def getRedis(): Jedis = &#123; val poolConfig = new GenericObjectPoolConfig() poolConfig.setMaxIdle(10) poolConfig.setMaxTotal(1000) lazy val jedisPool = new JedisPool(poolConfig, CommonUtil.getConfigFiled(\"REDIS_HOST\")) val jedis = jedisPool.getResource jedis.select(CommonUtil.getConfigFiled(\"REDIS_DB\").toInt) jedis &#125; 四、Mock JSON订单数据到Kafka1. 创建KafkaProducer123456789101112131415val props = new Properties() props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\") props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\") props.put(\"bootstrap.servers\", CommonUtil.getConfigFiled(\"KAFKA_BROKER_LIST\")) props.put(\"request.required.acks\", \"1\")// request.required.acks參數說明:// 0 表示 producer不等待broker同步完成確認之後發送下一條數據// 1 表示 producer在leader已成功收到數據完成確認之後發送下一條數據// -1 表示 producer在follower副本中確認收到數據後之後發送下一條數據//// 0 -&gt; 1 -&gt; -1 性能降低、數據健壯性增強 val producer = new KafkaProducer[String, String](props) 2. 随机生成订单数据并发送123456789101112131415161718192021222324val random = new Random()val dateFormat = FastDateFormat.getInstance(\"yyyy-MM-dd HH:mm:ss\")val topic = CommonUtil.getConfigFiled(\"KAFKA_TOPIC\")for (i &lt;- 0 to 9) &#123; val time = dateFormat.format(new Date()) val userId = random.nextInt(1000).toString val courseId = random.nextInt(500).toString val fee = random.nextInt(500).toString val result = Array(\"0\", \"1\") val flag = result(random.nextInt(2)) val orderId = UUID.randomUUID().toString val map = new util.HashMap[String, Object]() map.put(\"time\", time) map.put(\"userId\", userId) map.put(\"courseId\", courseId) map.put(\"fee\", fee) map.put(\"flag\", flag) map.put(\"orderId\", orderId) val json = new JSONObject(map) producer.send(new ProducerRecord[String, String](topic, json.toString())) 3. 打包并定时执行123456#!/bin/bash/Users/k/soft/spark-2.4.5-bin-hadoop2.7/bin/spark-submit \\--class com.kowhoy.Utils.KafkaProducerApp \\--jars $(echo /Users/k/IdeaProjects/StreamingAnalysis/out/artifacts/StreamingAnalysis_jar/*.jar | tr ' ', ',') \\--packages com.typesafe:config:1.4.0 \\ /Users/k/IdeaProjects/StreamingAnalysis/out/artifacts/StreamingAnalysis_jar/StreamingAnalysis.jar 4. 可在终端查看消费数据12345678 kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic logtopic&#123;&quot;flag&quot;:&quot;1&quot;,&quot;orderId&quot;:&quot;7085f2be-3d2e-4e23-b53d-2fe39d879c06&quot;,&quot;fee&quot;:&quot;84&quot;,&quot;time&quot;:&quot;2020-07-09 09:58:03&quot;,&quot;userId&quot;:&quot;135&quot;,&quot;courseId&quot;:&quot;178&quot;&#125;&#123;&quot;flag&quot;:&quot;1&quot;,&quot;orderId&quot;:&quot;fc0c55fd-1591-4b3f-bf5d-50a771996709&quot;,&quot;fee&quot;:&quot;21&quot;,&quot;time&quot;:&quot;2020-07-09 09:58:03&quot;,&quot;userId&quot;:&quot;190&quot;,&quot;courseId&quot;:&quot;437&quot;&#125;&#123;&quot;flag&quot;:&quot;0&quot;,&quot;orderId&quot;:&quot;54c28598-fc98-480f-b6e1-8e1b7ed750fc&quot;,&quot;fee&quot;:&quot;343&quot;,&quot;time&quot;:&quot;2020-07-09 09:58:03&quot;,&quot;userId&quot;:&quot;986&quot;,&quot;courseId&quot;:&quot;415&quot;&#125;&#123;&quot;flag&quot;:&quot;1&quot;,&quot;orderId&quot;:&quot;127a06d6-af95-413c-84fb-23cf7727e0a9&quot;,&quot;fee&quot;:&quot;218&quot;,&quot;time&quot;:&quot;2020-07-09 09:58:03&quot;,&quot;userId&quot;:&quot;53&quot;,&quot;courseId&quot;:&quot;291&quot;&#125;&#123;&quot;flag&quot;:&quot;1&quot;,&quot;orderId&quot;:&quot;91bb49aa-acf7-4cb6-ad6f-b298e33873ed&quot;,&quot;fee&quot;:&quot;421&quot;,&quot;time&quot;:&quot;2020-07-09 09:58:03&quot;,&quot;userId&quot;:&quot;313&quot;,&quot;courseId&quot;:&quot;22&quot;&#125;&#123;&quot;flag&quot;:&quot;1&quot;,&quot;orderId&quot;:&quot;579b7686-a07c-49de-8546-300dcf3fd3f8&quot;,&quot;fee&quot;:&quot;136&quot;,&quot;time&quot;:&quot;2020-07-09 09:58:03&quot;,&quot;userId&quot;:&quot;304&quot;,&quot;courseId&quot;:&quot;265&quot;&#125;&#123;&quot;flag&quot;:&quot;1&quot;,&quot;orderId&quot;:&quot;6bedec73-5c02-4808-829c-2504226ffab2&quot;,&quot;fee&quot;:&quot;242&quot;,&quot;time&quot;:&quot;2020-07-09 09:58:03&quot;,&quot;userId&quot;:&quot;203&quot;,&quot;courseId&quot;:&quot;171&quot;&#125; 五、消费Kafka统计数据1. Kafka设置12345678val kafkaParams = Map[String, Object]( \"bootstrap.servers\" -&gt; CommonUtil.getConfigFiled(\"KAFKA_BROKER_LIST\"), \"key.deserializer\" -&gt; classOf[StringDeserializer], \"value.deserializer\" -&gt; classOf[StringDeserializer], \"group.id\" -&gt; groupId, \"auto.offset.reset\" -&gt; \"earliest\", \"enable.auto.commit\" -&gt; (false: java.lang.Boolean) // 关闭自动提交 ) 2. 使用Zookeeper管理Offset —- 设置fromOffset123456789101112131415161718192021222324252627282930313233343536val topic = CommonUtil.getConfigFiled(\"KAFKA_TOPIC\")val topics = List(topic)val topicDirs = new ZKGroupTopicDirs(groupId, topic)val zkTopicPath = topicDirs.consumerOffsetDirval zkClient = new ZkClient(CommonUtil.getConfigFiled(\"ZK_HOST\"))val children = zkClient.countChildren(zkTopicPath)val streamRDD = if (children &gt; 0) &#123; println(\"已经消费过...\") var fromOffsets = Map[TopicPartition, Long]() for (partitionId &lt;- 0 until children) &#123; val offset = zkClient.readData[String](zkTopicPath + \"/\" + partitionId) fromOffsets += (new TopicPartition(topic, partitionId) -&gt; offset.toLong) &#125; KafkaUtils.createDirectStream( ssc, LocationStrategies.PreferConsistent, ConsumerStrategies.Assign[String, String](fromOffsets.keys.toList, kafkaParams, fromOffsets) )&#125; else &#123; println(\"第一次进行消费...\") KafkaUtils.createDirectStream( ssc, LocationStrategies.PreferConsistent, ConsumerStrategies.Subscribe[String, String](topics, kafkaParams) )&#125; 3. 使用Zookeeper管理Offset —- 保存untilOffset12345678910val dateFormat = FastDateFormat.getInstance(\"yyyy-MM-dd HH:mm:ss\") streamRDD.foreachRDD(rdd =&gt; &#123; val offsetRange = rdd.asInstanceOf[HasOffsetRanges].offsetRanges ...... for (o &lt;- offsetRange) &#123; // 🚩🚩🚩 ZkUtils(zkClient, false).updatePersistentPath(zkTopicPath +\"/\"+o.partition, o.untilOffset.toString) &#125; &#125;) 4. 使用Kafka自带Offset管理，需要手动提交Offset12345678val timeFormat = FastDateFormat.getInstance(\"yyyy-MM-dd HH:mm:ss\") streamRDD.foreachRDD(rdd =&gt; &#123; val offsetRange = rdd.asInstanceOf[HasOffsetRanges].offsetRanges ..... //🚩🚩🚩 streamRDD.asInstanceOf[CanCommitOffsets].commitAsync(offsetRange) &#125;) 5. 统计数据并保存到Redis12345678910111213141516171819202122232425262728streamRDD.foreachRDD(rdd =&gt; &#123; val data = rdd.map(x =&gt; JSON.parseObject(x.value)) .map(log =&gt; &#123; val flag = log.getString(\"flag\") val fee = log.getLong(\"fee\") val time = log.getString(\"time\") val day = time.substring(0, 10) val hour = time.substring(11, 13) val minute = time.substring(14, 16) val success:(Long, Long) = if (flag == \"1\") (1, fee) else (0, 0) (day, hour, minute, List[Long](1, success._1, success._2)) &#125;) data.map(x =&gt; (x._1, x._4)).reduceByKey((a, b) =&gt; &#123; a.zip(b).map(x =&gt; x._1 + x._2) &#125;).foreachPartition(part =&gt; &#123; val jedis = CommonUtil.getRedis() part.foreach(x =&gt; &#123; jedis.hincrBy(\"n-ko-\"+x._1, \"total\", x._2(0)) jedis.hincrBy(\"n-ko-\"+x._1, \"success\", x._2(1)) jedis.hincrBy(\"n-ko-\"+x._1, \"fee\", x._2(2)) &#125;) println(\"saved at \" + timeFormat.format(new Date())) &#125;)&#125;) 六、项目完整代码 ⚠️ 需要在/src/main 下创建resources/application.conf 12345678HBASE_ROOT_DIR&#x3D;&quot;alluxio:&#x2F;&#x2F;ip:port&#x2F;hbase&quot;KAFKA_BROKER_LIST&#x3D;&quot;ip:port&quot;KAFKA_GROUP_ID&#x3D;&quot;xxx&quot;KAFKA_TOPIC&#x3D;&quot;xxx&quot;REDIS_HOST&#x3D;&quot;xxx&quot;REDIS_DB&#x3D;&quot;1&quot;ZK_HOST&#x3D;&quot;ip:port&quot; 完整代码 七、补充SparkStreaming整合Kafka，不同版本的使用差异","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://yoursite.com/tags/Kafka/"},{"name":"SparkStreaming","slug":"SparkStreaming","permalink":"http://yoursite.com/tags/SparkStreaming/"},{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}]},{"title":"运行于Alluxio之上的Spark任务","slug":"运行于Alluxio之上的Spark任务","date":"2020-07-03T06:12:37.000Z","updated":"2020-07-03T06:12:37.287Z","comments":true,"path":"2020/07/03/运行于Alluxio之上的Spark任务/","link":"","permalink":"http://yoursite.com/2020/07/03/%E8%BF%90%E8%A1%8C%E4%BA%8EAlluxio%E4%B9%8B%E4%B8%8A%E7%9A%84Spark%E4%BB%BB%E5%8A%A1/","excerpt":"","text":"运行于Alluxio之上的Spark任务Spark配置 ${SPARK_HOME}/conf/spark-defaults.conf 增加配置spark.driver.extraClassPath //client/alluxio-1.8.2-client.jarspark.executor.extraClassPath //client/alluxio-1.8.2-client.jar 配置更改 application.conf123HBASE_ROOT_DIR&#x3D;&quot;alluxio:&#x2F;&#x2F;localhost:19998&#x2F;hbase&quot;ZOOKEEPER_PATH&#x3D;&quot;localhost:2181&quot;OUTPUT_PATH&#x3D;&quot;alluxio:&#x2F;&#x2F;localhost:19998&#x2F;access_logs&#x2F;&quot; 异常处理：Wrong FS: alluxio://localhost:19998, expected: hdfs://localhost:9000 本地伪分布式会出现这种问题 原先123456val output = \"alluxio://localhost:19998/access_logs/data_log\" val outputPath = new Path(output) if (FileSystem.get(conf).exists(outputPath)) &#123; FileSystem.get(conf).delete(outputPath, true) &#125; 修改为12345678val output = \"alluxio://localhost:19998/access_logs/data_log\"val outputPath = new Path(output)val fileSystem = outputPath.getFileSystem(conf)if (fileSystem.exists(outputPath)) &#123; fileSystem.delete(outputPath, true)&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"},{"name":"Alluxio","slug":"Alluxio","permalink":"http://yoursite.com/tags/Alluxio/"}]},{"title":"Spark统计HBase，输出Mysql(字段自定义版本)","slug":"Spark统计HBase，输出Mysql-字段自定义版本","date":"2020-07-02T09:17:03.000Z","updated":"2020-07-02T09:17:03.321Z","comments":true,"path":"2020/07/02/Spark统计HBase，输出Mysql-字段自定义版本/","link":"","permalink":"http://yoursite.com/2020/07/02/Spark%E7%BB%9F%E8%AE%A1HBase%EF%BC%8C%E8%BE%93%E5%87%BAMysql-%E5%AD%97%E6%AE%B5%E8%87%AA%E5%AE%9A%E4%B9%89%E7%89%88%E6%9C%AC/","excerpt":"","text":"使用两种方式 SparkCore, SparkSQL 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233package com.kowhoyimport java.sql.DriverManagerimport org.apache.hadoop.conf.Configurationimport org.apache.hadoop.hbase.client.&#123;Result, Scan&#125;import org.apache.hadoop.hbase.io.ImmutableBytesWritableimport org.apache.spark.sql.&#123;DataFrame, Row, SparkSession&#125;import org.apache.hadoop.hbase.mapreduce.&#123;TableInputFormat, TableMapReduceUtil&#125;import org.apache.hadoop.hbase.util.Bytesimport org.apache.spark.rdd.RDDimport org.apache.spark.sql.types.&#123;StringType, StructField, StructType&#125;import scala.collection.mutable.ListBufferimport scala.util.&#123;Failure, Success, Try&#125;/** * @DESC 地理位置统计分析 * @Date 2020/7/2 11:35 上午 **/object RegionAnalysisApp &#123; def main(args: Array[String]): Unit = &#123; if (args.length &lt; 2) &#123; System.err.println(\"Usage: RegionAnalysisApp &lt;dimension&gt; &lt;date&gt;\") System.exit(1) &#125; val Array(dimension, date) = args val dimensionSupport = List(\"country\", \"province\", \"country&amp;province\", \"city\", \"country&amp;city\", \"operator\") if (! dimensionSupport.contains(dimension)) &#123; System.err.println(\"the dimension should in the list (\\\"country\\\", \\\"province\\\", \\\"country&amp;province\\\", \\\"city\\\", \\\"country&amp;city\\\", \\\"operator\\\") \") System.exit(1) &#125; val spark = SparkSession.builder().config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .appName(\"AnalysisApp\").master(\"local[2]\").getOrCreate() spark.sparkContext.setLogLevel(\"WARN\") // 读取HBase数据 // 1. 基础配置 val conf = new Configuration() conf.set(\"hbase.rootdir\", Utils.getConfigField(\"HBASE_ROOT_DIR\")) conf.set(\"hbase.zookeeper.quorum\", Utils.getConfigField(\"ZOOKEEPER_PATH\")) // 2. 设置表 val tableName = \"access_log_\" + date conf.set(TableInputFormat.INPUT_TABLE, tableName) // 3. 设置Scan val scan = new Scan() scan.addFamily(Bytes.toBytes(\"o\")) val dimensionArray =dimension.split(\"&amp;\") for (col &lt;- dimensionArray) &#123; scan.addColumn(Bytes.toBytes(\"o\"), Bytes.toBytes(col)) &#125; conf.set(TableInputFormat.SCAN, TableMapReduceUtil.convertScanToString(scan)) // 4. 读取数据 val logRDD = spark.sparkContext.newAPIHadoopRDD( conf, classOf[TableInputFormat], classOf[ImmutableBytesWritable], classOf[Result] ) // 设置缓存 logRDD.cache() // 统计结果 val analysisResultByCore = analysisByCore(logRDD, dimensionArray) val logDF = getLogDF(spark, logRDD, dimensionArray) val analysisBySparkSQLTableRDD = analysisBySparkSQLTable(spark, logDF, dimensionArray) analysisResultByCore.foreach(println) println(\"------------------------------------------\") analysisBySparkSQLTableRDD.foreach(println) // 保存到结果到MySQL saveResultWithListBufferRDD(analysisResultByCore, dimensionArray) &#125; /*** * @Desc 使用SparkCore统计 * @Date 3:04 下午 2020/7/2 * @Param [logRDD, dimensionArray] * @Return org.apache.spark.rdd.RDD&lt;scala.Tuple2&lt;scala.collection.mutable.ListBuffer&lt;java.lang.String&gt;,java.lang.Object&gt;&gt; **/ def analysisByCore(logRDD: RDD[(ImmutableBytesWritable, Result)], dimensionArray: Array[String]): RDD[(ListBuffer[String], Int)] = &#123; logRDD.map&#123; log =&gt; &#123; val dimensionList: ListBuffer[String] = ListBuffer[String]() for (dimension &lt;- dimensionArray) &#123; val dimensionValue = Bytes.toString(log._2.getValue(\"o\".getBytes(), dimension.getBytes())) dimensionList.append(dimensionValue) &#125; (dimensionList, 1) &#125; &#125;.reduceByKey(_+_) &#125; /*** * @Des 生成DataSet * @Date 3:43 下午 2020/7/2 * @Param [spark, logRDD, dimensionArray] * @Return org.apache.spark.sql.Dataset&lt;org.apache.spark.sql.Row&gt; **/ def getLogDF(spark: SparkSession, logRDD: RDD[(ImmutableBytesWritable, Result)], dimensionArray: Array[String]): DataFrame = &#123; val schema = StructType( dimensionArray.map(dimension =&gt; &#123; StructField(dimension, StringType, true) &#125;) ) val rowRDD = logRDD.map&#123; x =&gt; &#123; var s: Seq[String] = Seq[String]() for (dimension &lt;- dimensionArray) &#123; val dimensionValue = Bytes.toString(x._2.getValue(\"o\".getBytes(), dimension.getBytes())) s = s:+ dimensionValue &#125; Row.fromSeq(s) &#125; &#125; spark.createDataFrame(rowRDD, schema) &#125; /*** * @Desc 使用SparkSQLTable统计数据 * @Date 4:05 下午 2020/7/2 * @Param [spark, logDF, dimensionArray] * @Return void **/ def analysisBySparkSQLTable(spark: SparkSession, logDF: DataFrame, dimensionArray: Array[String]): RDD[Row] = &#123; logDF.createOrReplaceTempView(\"log\") var dimensionStr = \"\" if (dimensionArray.length == 1) &#123; dimensionStr = dimensionArray(0) + \",\" &#125; else &#123; for (dimension &lt;- dimensionArray) &#123; dimensionStr += dimension + \",\" &#125; &#125; val sql = \"select \" + dimensionStr + \" count(1) cnt from log group by \" + dimensionStr.substring(0, dimensionStr.length - 1) + \" order by cnt desc\" val resultDF = spark.sql(sql) resultDF.rdd &#125; /*** * @Desc 保存结果 * @Date 4:39 下午 2020/7/2 * @Param [resultRDD, dimensionArray] * @Return void **/ def saveResultWithListBufferRDD(resultRDD: RDD[(ListBuffer[String], Int)], dimensionArray: Array[String]): Unit = &#123; val insertColumnsBuilder = new StringBuilder(\"(\") dimensionArray.foreach(x =&gt; insertColumnsBuilder.append(x+\",\")) insertColumnsBuilder.deleteCharAt(insertColumnsBuilder.length-1).append(\", cnt)\") val columns = insertColumnsBuilder.toString() val valArr = Array.fill(dimensionArray.length+1)(\"?\") val valStr = valArr.mkString(\",\") val mysqlTableName = dimensionArray.mkString(\"_\") + \"_stat\" resultRDD.coalesce(1).foreachPartition&#123; part =&gt; &#123; Try &#123; val connection = &#123; Class.forName(\"com.mysql.jdbc.Driver\") val url = \"jdbc:mysql://localhost:3306/spark2?characterEncoding=UTF-8\" val user = \"root\" val password = \"root \" DriverManager.getConnection(url, user, password) &#125; val preAutoCommit = connection.getAutoCommit connection.setAutoCommit(false) // 关闭自动提交 val sql = s\"insert into $mysqlTableName $columns values ($valStr)\" println(sql) val pstmt = connection.prepareStatement(sql) pstmt.addBatch(s\"truncate $mysqlTableName\") part.foreach(log =&gt; &#123; for (idx &lt;- 1 to dimensionArray.length) &#123; pstmt.setString(idx, log._1(idx-1)) &#125; pstmt.setInt(dimensionArray.length+1, log._2) pstmt.addBatch() &#125;) pstmt.executeBatch() connection.commit() (connection, preAutoCommit) &#125; match &#123; case Success((connection, preAutoCommit)) =&gt; &#123; connection.setAutoCommit(preAutoCommit) if (null != connection) connection.close() &#125; case Failure(exception) =&gt; throw exception &#125; &#125; &#125; &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://yoursite.com/tags/HBase/"},{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"},{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"}]},{"title":"基于HFile同步到HBase的日志ETL","slug":"基于HFile同步到HBase的日志ETL","date":"2020-07-01T08:44:08.000Z","updated":"2020-07-02T03:18:38.215Z","comments":true,"path":"2020/07/01/基于HFile同步到HBase的日志ETL/","link":"","permalink":"http://yoursite.com/2020/07/01/%E5%9F%BA%E4%BA%8EHFile%E5%90%8C%E6%AD%A5%E5%88%B0HBase%E7%9A%84%E6%97%A5%E5%BF%97ETL/","excerpt":"","text":"基于HFile同步到HBase的日志ETL一、数据格式1234110.85.18.234 - - [30&#x2F;Jan&#x2F;2019:00:00:21 +0800] &quot;GET &#x2F;course&#x2F;list?c&#x3D;cb HTTP&#x2F;1.1&quot; 200 12800 &quot;www.imooc.com&quot; &quot;https:&#x2F;&#x2F;www.imooc.com&#x2F;course&#x2F;list?c&#x3D;data&quot; - &quot;Mozilla&#x2F;5.0 (Windows NT 10.0; WOW64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;58.0.3029.110 Safari&#x2F;537.36 SE 2.X MetaSr 1.0&quot; &quot;-&quot; 10.100.16.243:80 200 0.172 0.172218.74.48.154 - - [30&#x2F;Jan&#x2F;2019:00:00:22 +0800] &quot;GET &#x2F;.well-known&#x2F;apple-app-site-association HTTP&#x2F;1.1&quot; 200 165 &quot;www.imooc.com&quot; &quot;-&quot; - &quot;swcd (unknown version) CFNetwork&#x2F;974.2.1 Darwin&#x2F;18.0.0&quot; &quot;-&quot; 10.100.135.47:80 200 0.001 0.001113.77.139.245 - - [30&#x2F;Jan&#x2F;2019:00:00:22 +0800] &quot;GET &#x2F;static&#x2F;img&#x2F;common&#x2F;new.png HTTP&#x2F;1.1&quot; 200 1020 &quot;www.imooc.com&quot; &quot;https:&#x2F;&#x2F;www.imooc.com&#x2F;&quot; - &quot;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;73.0.3642.0 Safari&#x2F;537.36&quot; &quot;-&quot; 10.100.16.241:80 200 0.001 0.001113.77.139.245 - - [30&#x2F;Jan&#x2F;2019:00:00:22 +0800] &quot;GET &#x2F;static&#x2F;img&#x2F;menu_icon.png HTTP&#x2F;1.1&quot; 200 4816 &quot;www.imooc.com&quot; &quot;https:&#x2F;&#x2F;www.imooc.com&#x2F;&quot; - &quot;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;73.0.3642.0 Safari&#x2F;537.36&quot; &quot;-&quot; 10.100.16.243:80 200 0.001 0.001 二、整体思路 因为每组字段中也存在空格键，所以使用分隔符进行分隔解析出各个字段不可行，因此使用分组的正则匹配进行解析日志 对IP字段可以解析出 country\\province\\city\\operator信息，使用Java实现解析工具类 对UserAgent字段可以解析出browserName\\browserVersion\\osName\\osVersion信息，使用Java实现解析工具类 日志中的时间字段，不便于解析统计，将时间字段进行格式化 使用PUT的方法效率低，使用生成HFile，load到HBase的方式进行存储 三、目录结构1234567891011121314151617181920.├── java│ └── com│ └── kowhoy│ ├── domain│ │ ├── IpInfo.java│ │ └── UaInfo.java│ └── util│ ├── IpParseUtil.java│ └── UaParseUtil.java├── resources│ ├── application.conf│ ├── core-site.xml│ └── hdfs-site.xml└── scala └── com └── kowhoy ├── ETLApp.scala └── commonUtil └── Util.scala java目录下，主要是解析工具类 domain存放结构化类 util存放解析方法 scala目录下, 主要是ETL，和常用工具类 resources/application.conf 为配置文件 四、IP解析IpIfo.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package com.kowhoy.domain;/** * @ClassName IpInfo * @DESC IpInfo结构 * @Date 2020/6/30 9:39 下午 **/public class IpInfo &#123; private String country = null; private String province = null; private String city = null; private String operator = null; public String getCountry() &#123; return country; &#125; public String getProvince() &#123; return province; &#125; public String getCity() &#123; return city; &#125; public String getOperator() &#123; return operator; &#125; public void setCountry(String country) &#123; this.country = country; &#125; public void setProvince(String province) &#123; this.province = province; &#125; public void setCity(String city) &#123; this.city = city; &#125; public void setOperator(String operator) &#123; this.operator = operator; &#125; @Override public String toString() &#123; return \"IpInfo&#123;\" + \"country='\" + country + '\\'' + \", province='\" + province + '\\'' + \", city='\" + city + '\\'' + \", operator='\" + operator + '\\'' + '&#125;'; &#125;&#125; IpParseUtil.java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071package com.kowhoy.util;import com.kowhoy.domain.IpInfo;import org.lionsoul.ip2region.*;import java.io.FileNotFoundException;import java.io.IOException;import java.util.logging.Logger;/** * @ClassName IpParseUtil * @DESC Ip解析工具类 * @Date 2020/6/30 9:39 下午 **/public class IpParseUtil &#123; private static DbConfig dbConfig = null; private static DbSearcher dbSearcher = null; private static Logger logger = Logger.getLogger(IpParseUtil.class.getName()); static &#123; String dbFile = \"/Users/zhouke/tmp_data/ip2region.db\"; try &#123; dbConfig = new DbConfig(); dbSearcher = new DbSearcher(dbConfig, dbFile); &#125; catch (DbMakerConfigException e) &#123; logger.warning(\"ipParser config init exception\" + e.getMessage()); &#125; catch (FileNotFoundException e) &#123; logger.warning(\"ipParser file not found\" + e.getMessage()); &#125; &#125; /** * 根据ip解析出ip的位置信息 * @param ip: String * @return IpInfo */ public static IpInfo getIpInfo(String ip) &#123; IpInfo info = null; DataBlock block = null; if (Util.isIpAddress(ip)) &#123; try &#123; block = dbSearcher.btreeSearch(ip); &#125; catch (IOException e) &#123; logger.warning(\"ipParser parse error io:\" + e.getMessage() + \"\\t\" + ip); &#125; catch (Exception e) &#123; logger.warning(\"ipParser parse error:\" + e.getMessage() + \"\\t\" + ip); &#125; &#125; if (null != block) &#123; info = new IpInfo(); String regionStr = block.getRegion(); String[] regionData = regionStr.replace(\"|\", \",\").split(\",\"); info.setCountry(regionData[0]); info.setProvince(regionData.length &gt; 2 ? regionData[2] : \"-\"); info.setCity(regionData.length &gt; 3 ? regionData[3] : \"-\"); info.setOperator(regionData.length &gt; 4 ? regionData[4] : \"-\"); &#125; return info; &#125;&#125; 五、UserAgent解析UaInfo.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package com.kowhoy.domain;/** * @ClassName UaInfo * @DESC UserAgent类 * @Date 2020/7/1 11:02 上午 **/public class UaInfo &#123; private String browserName = null; private String browserVersion = null; private String osName = null; private String osVersion = null; public String getBrowserName() &#123; return browserName; &#125; public void setBrowserName(String browserName) &#123; this.browserName = browserName; &#125; public String getBrowserVersion() &#123; return browserVersion; &#125; public void setBrowserVersion(String browserVersion) &#123; this.browserVersion = browserVersion; &#125; public String getOsName() &#123; return osName; &#125; public void setOsName(String osName) &#123; this.osName = osName; &#125; public String getOsVersion() &#123; return osVersion; &#125; public void setOsVersion(String osVersion) &#123; this.osVersion = osVersion; &#125; @Override public String toString() &#123; return \"UaInfo&#123;\" + \"browserName='\" + browserName + '\\'' + \", browserVersion='\" + browserVersion + '\\'' + \", osName='\" + osName + '\\'' + \", osVersion='\" + osVersion + '\\'' + '&#125;'; &#125;&#125; UaParseUtil.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.kowhoy.util;import com.kowhoy.domain.UaInfo;import cz.mallat.uasparser.OnlineUpdater;import cz.mallat.uasparser.UASparser;import cz.mallat.uasparser.UserAgentInfo;import org.apache.commons.lang3.StringUtils;import java.io.IOException;/** * @ClassName UaParseUtil * @DESC UserAgent解析类 * @Date 2020/7/1 11:02 上午 **/public class UaParseUtil &#123; private static UASparser parser = null; static &#123; try &#123; parser = new UASparser(OnlineUpdater.getVendoredInputStream()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public static UaInfo getUaInfo(String ua) &#123; UaInfo info = null; try &#123; if (StringUtils.isNotEmpty(ua)) &#123; info = new UaInfo(); UserAgentInfo uaInfo = parser.parse(ua); if (null != uaInfo) &#123; info.setBrowserName(uaInfo.getUaFamily()); info.setBrowserVersion(uaInfo.getBrowserVersionInfo()); info.setOsName(uaInfo.getOsFamily()); info.setOsVersion(uaInfo.getOsName()); &#125; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return info; &#125;&#125; 六、Scala工具类Util.scala123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384package com.kowhoy.commonUtilimport java.util.zip.CRC32import com.typesafe.config.&#123;Config, ConfigFactory&#125;import org.apache.commons.lang3.StringUtilsimport org.apache.hadoop.conf.Configurationimport org.apache.hadoop.hbase.client.&#123;Admin, Connection, ConnectionFactory&#125;import org.apache.hadoop.hbase.util.Bytes/** * @DESC 基础工具类 * @Date 2020/7/1 1:37 下午 **/object Util &#123; /*** * @Desc 生成rowKey * @Date 1:42 下午 2020/7/1 * @Param [date, info] * @Return java.lang.String **/ def generatingRowKey(date:String, info:String): String = &#123; val builder = new StringBuilder(date) builder.append(\"_\") val crc32 = new CRC32() crc32.reset() if (StringUtils.isNotEmpty(info)) &#123; crc32.update(Bytes.toBytes(info)) &#125; builder.append(crc32.getValue) builder.toString() &#125; /*** * @Desc 获取配置字段 * @Date 2:27 下午 2020/7/1 * @Param [key] * @Return java.lang.Object **/ def getConfigField(key: String): String = &#123; lazy val conf: Config = ConfigFactory.load() try &#123; conf.getString(key) &#125; catch &#123; case _ =&gt; System.err.println(key + \"不存在\") \"not exists\" &#125; &#125; /*** * @Desc HBase的相关 * @Date 2:22 下午 2020/7/1 * @Param [] * @Return scala.Tuple3&lt;org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection,org.apache.hadoop.hbase.client.Admin&gt; **/ def getHBaseEngine(): (Configuration, Connection, Admin) = &#123; val conf = new Configuration() conf.set(\"hbase.rootdir\", getConfigField(\"HBASE_ROOT_DIR\")) conf.set(\"hbase.zookeeper.quorum\", getConfigField(\"ZOOKEEPER_PATH\")) var connection: Connection = null; var admin: Admin = null; try &#123; connection = ConnectionFactory.createConnection(conf) admin = connection.getAdmin &#125; catch &#123; case e: Exception =&gt; e.printStackTrace() &#125; (conf, connection, admin) &#125;&#125; 七、ETLAppETLApp.scala123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222package com.kowhoyimport java.util.Localeimport java.util.regex.&#123;Matcher, Pattern&#125;import com.kowhoy.domain.&#123;IpInfo, UaInfo&#125;import com.kowhoy.util.&#123;IpParseUtil, UaParseUtil&#125;import org.apache.commons.lang3.time.FastDateFormatimport org.apache.hadoop.fs.&#123;FileSystem, Path&#125;import org.apache.hadoop.hbase.&#123;KeyValue, TableName&#125;import org.apache.hadoop.hbase.client.&#123;Admin, ColumnFamilyDescriptorBuilder, TableDescriptorBuilder&#125;import org.apache.hadoop.hbase.io.ImmutableBytesWritableimport org.apache.hadoop.hbase.mapreduce.&#123;HFileOutputFormat2, LoadIncrementalHFiles, TableOutputFormat&#125;import org.apache.hadoop.hbase.util.Bytesimport org.apache.spark.sql.SparkSessionimport org.apache.hadoop.mapreduce.&#123;Job =&gt; NewAPIHadoopJob&#125;import org.apache.spark.internal.Loggingimport scala.collection.mutable.ListBuffer/** * @DESC ETL log through HFile saved to HBase * @Date 2020/6/30 5:49 下午 **/object ETLApp extends Logging&#123; def main(args: Array[String]): Unit = &#123; if (args.length &lt; 1) &#123; System.err.println(\"Usage: ETLApp &lt;date&gt;\") System.exit(1) &#125; // 传入时间变量 val Array(date) = args val spark = SparkSession.builder().config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .appName(\"ETLApp\").master(\"local[2]\").getOrCreate() // 日志文件路径 val logPath = \"hdfs://localhost:9000/access_logs/2020-06-17.log\" val data = spark.sparkContext.textFile(logPath).take(10) val logRDD = spark.sparkContext.parallelize(data) // 将日志RDD 使用分组正则匹配进行解析 val pattern: Pattern = Pattern.compile(\"([\\\\d\\\\.]&#123;7,&#125;) - - (\\\\[.&#123;26,&#125;\\\\]) (\\\".+?\\\"|\\\\-) (\\\\d&#123;3&#125;|\\\\-) (\\\\d+|\\\\-) (\\\".+?\\\"|\\\\-) (\\\".+?\\\"|\\\\-) - (\\\".+?\\\"|\\\\-) (\\\".+?\\\"|\\\\-) ([\\\\d\\\\.]&#123;7,&#125;:\\\\d+|\\\\-) (\\\\d+|\\\\-) (\\\\d+?\\\\.\\\\d+|\\\\-) (\\\\d+?\\\\.\\\\d+|\\\\-)\") val columnMatcherMap = Map(\"ip\"-&gt;1, \"time\"-&gt;2, \"requestData\"-&gt;3, \"status\"-&gt;4, \"bytesSent\"-&gt;5, \"host\"-&gt;6, \"referer\"-&gt;7, \"ua\"-&gt;8, \"hostIp\"-&gt;10, \"spendTime\"-&gt;12) val saveRDD = logRDD.mapPartitions(partition =&gt; &#123; partition.flatMap( log =&gt; &#123; val matcher: Matcher = pattern.matcher(log) val logMap = scala.collection.mutable.HashMap[String, String]() val rowKeyColValueList = new ListBuffer[((String, String), KeyValue)] if (matcher.matches()) &#123; for ((k, v) &lt;- columnMatcherMap) &#123; logMap.put(k, matcher.group(v)) &#125; val ip = logMap(\"ip\") val time = logMap(\"time\") val ua = logMap(\"ua\") // 解析IP val ipInfo: IpInfo = IpParseUtil.getIpInfo(ip) var (country, province, city, operator) = (\"-\", \"-\", \"-\", \"-\") if (null != ipInfo) &#123; country = ipInfo.getCountry province = ipInfo.getProvince city = ipInfo.getCity operator = ipInfo.getOperator &#125; // 解析UA val uaInfo: UaInfo = UaParseUtil.getUaInfo(ua) var (browserName, browserVersion, osName, osVersion) = (\"-\", \"-\", \"-\", \"-\") if (null != uaInfo) &#123; browserName = uaInfo.getBrowserName browserVersion = uaInfo.getBrowserVersion osName = uaInfo.getOsName osVersion = uaInfo.getOsVersion &#125; // 分隔RequestData val requestArr = logMap(\"requestData\").split(\" \") var(method, url, protocol) = (\"-\", \"-\", \"-\") if (requestArr.length &gt; 2) &#123; method = requestArr(0).replace(\"\\\"\", \"\") url = requestArr(1) protocol = requestArr(2).replace(\"\\\"\", \"\") &#125; // 格式化时间 val timeStr = time.substring(time.indexOf(\"[\")+1, time.indexOf(\"]\")) val logTime = FastDateFormat.getInstance(\"dd/MMM/yyy:HH:mm:ss Z\", Locale.ENGLISH).parse(timeStr).getTime val formatTime = FastDateFormat.getInstance(\"yyyy-MM-dd HH:mm:ss\").format(logTime) logMap.put(\"browserName\", browserName) logMap.put(\"browserVersion\", browserVersion) logMap.put(\"osName\", osName) logMap.put(\"osVersion\", osVersion) logMap.put(\"country\", country) logMap.put(\"province\", province) logMap.put(\"city\", city) logMap.put(\"operator\", operator) logMap.put(\"formatTime\", formatTime) logMap.put(\"method\", method) logMap.put(\"url\", url) logMap.put(\"protocol\", protocol) // 生成rowKey val rowKey: String = commonUtil.Util.generatingRowKey(date, logMap(\"referer\")+url+ip+ua) val rowKeyBytes = Bytes.toBytes(rowKey) for ((col, value) &lt;- logMap) &#123; val keyValue = new KeyValue(rowKeyBytes, \"o\".getBytes, Bytes.toBytes(String.valueOf(col)), Bytes.toBytes(String.valueOf(value))) rowKeyColValueList.append(((rowKey, col), keyValue)) //加入col字段为了排序用 &#125; &#125; rowKeyColValueList.toList &#125;) &#125;).sortByKey() // 写入HFile需要是排序后的 .map(x =&gt; (new ImmutableBytesWritable(Bytes.toBytes(x._1._1)), x._2)) val (conf, connection, admin) = commonUtil.Util.getHBaseEngine() val tableName: String = \"access_log_\" + date val table: TableName = TableName.valueOf(tableName) createTable(tableName, admin) // 设置输出表 conf.set(TableOutputFormat.OUTPUT_TABLE, tableName) // 设置HFileOutPutFormat需要的 job / table.descriptor / regionLocator val job = NewAPIHadoopJob.getInstance(conf) // NewAPIHadoopJob手动导入 val tableDescriptor = connection.getTable(table) val regionLocator = connection.getRegionLocator(table) HFileOutputFormat2.configureIncrementalLoad(job, tableDescriptor, regionLocator) val output = commonUtil.Util.getConfigField(\"OUTPUT_PATH\") + s\"-$date-logs\" val outputPath = new Path(output) if (FileSystem.get(conf).exists(outputPath)) &#123; FileSystem.get(conf).delete(outputPath, true) &#125; // 保存HFile, 写到output的路径上 saveRDD.saveAsNewAPIHadoopFile( output, classOf[ImmutableBytesWritable], classOf[KeyValue], classOf[HFileOutputFormat2], job.getConfiguration ) // 如果output上有文件，将文件load到HBase,并删除HFile if (FileSystem.get(conf).exists(outputPath)) &#123; val load = new LoadIncrementalHFiles(conf) load.doBulkLoad(outputPath, admin, connection.getTable(table), regionLocator) logInfo(\"删除HFile\") FileSystem.get(conf).delete(outputPath, true) &#125; logInfo(\"写入完成\") // 关闭资源 if (null != connection) &#123; connection.close() &#125; if (null != admin) &#123; admin.close() &#125; spark.stop() &#125; /** * @Desc 创建数据表 * @Date 2:39 下午 2020/7/1 * @Param [tableName, admin] * @Return void **/ def createTable(tableName: String, admin: Admin): Unit = &#123; val table = TableName.valueOf(tableName) if (admin.tableExists(table)) &#123; // 表已存在的话 先删除 admin.disableTable(table) admin.deleteTable(table) &#125; try &#123; val tableDescriptorBuilder = TableDescriptorBuilder.newBuilder(table) val columnFamilyDescriptor = ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(\"o\")).build() tableDescriptorBuilder.setColumnFamily(columnFamilyDescriptor) admin.createTable(tableDescriptorBuilder.build()) &#125; catch &#123; case e: Exception =&gt; e.printStackTrace() &#125; &#125;&#125; 八、异常处理java.io.IOException: Mkdirs failed to create 将${HADOOP_HOME}/etc/hadoop/core-site.xml 和 hdfs-site.xml拷贝的src/main/resources下 九、使用脚本提交作业123456789101112#!/bin/bashexport HADOOP_CONF_DIR=/Users/k/soft/hadoop-2.8.5/etc/hadoop/Users/k/soft/spark-2.4.5-bin-hadoop2.7/bin/spark-submit \\--class com.kowhoy.ETLApp \\--master yarn \\--deploy-mode client \\--name \"logETLApp\" \\--jars $(echo /Users/k/IdeaProjects/logETL/out/artifacts/logETL_jar/*.jar | tr ' ' ',') \\--conf spark.serializer=org.apache.spark.serializer.KryoSerializer \\/Users/k/IdeaProjects/logETL/out/artifacts/logETL_jar/logETL.jar \\$(date +\"%F\")","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://yoursite.com/tags/HBase/"},{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"},{"name":"ETL","slug":"ETL","permalink":"http://yoursite.com/tags/ETL/"},{"name":"HFile","slug":"HFile","permalink":"http://yoursite.com/tags/HFile/"}]},{"title":"Alluxio安装","slug":"Alluxio安装","date":"2020-07-01T02:33:39.000Z","updated":"2020-07-01T02:33:39.832Z","comments":true,"path":"2020/07/01/Alluxio安装/","link":"","permalink":"http://yoursite.com/2020/07/01/Alluxio%E5%AE%89%E8%A3%85/","excerpt":"","text":"Alluxio安装1. 下载安装包可以下载带有正确Hadoop版本的预编译二进制包，也可源码编译Alluxio2. 配置环境变量3. 配置文件修改1234567cp conf&#x2F;alluxio-site.properties.template conf&#x2F;alluxio-site.propertiesalluxio.underfs.address&#x3D;hdfs:&#x2F;&#x2F;&lt;NAMENODE&gt;:&lt;PORT&gt;alluxio.master.hostname&#x3D;localhost#&#96;workers&#96; 和 &#96;masters&#96; 文件根据需要配置 4. 格式化1alluxio format ####5. 启动 1alluxio-start.sh local SudoMount WebUI localhost:19999","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Alluxio","slug":"Alluxio","permalink":"http://yoursite.com/tags/Alluxio/"}]},{"title":"spark统计HBase数据，输出到MySQL","slug":"spark统计HBase数据，输出到MySQL","date":"2020-06-29T10:46:49.000Z","updated":"2020-06-29T10:46:49.258Z","comments":true,"path":"2020/06/29/spark统计HBase数据，输出到MySQL/","link":"","permalink":"http://yoursite.com/2020/06/29/spark%E7%BB%9F%E8%AE%A1HBase%E6%95%B0%E6%8D%AE%EF%BC%8C%E8%BE%93%E5%87%BA%E5%88%B0MySQL/","excerpt":"","text":"12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394package com.kowhoyimport org.apache.spark.sql.SparkSessionimport org.apache.hadoop.conf.Configurationimport org.apache.hadoop.hbase.mapreduce.&#123;TableInputFormat, TableMapReduceUtil&#125;import org.apache.hadoop.hbase.client.&#123;Scan, Result&#125;import org.apache.hadoop.hbase.io.ImmutableBytesWritableimport org.apache.hadoop.hbase.util.Bytesimport scala.util.&#123;Try, Success, Failure&#125;import java.sql.DriverManagerobject BrowserAnalysis &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession.builder().config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .appName(\"BrowserAnalysis\").master(\"local[2]\").getOrCreate() spark.sparkContext.setLogLevel(\"WARN\") val date = \"20200629\" val tableName = \"access_\" + date val conf = new Configuration() conf.set(\"hbase.rootdir\", \"hdfs://localhost:9000/hbase\") conf.set(\"hbase.zookeeper.quorum\", \"localhost:2181\") conf.set(TableInputFormat.INPUT_TABLE, tableName) // 配置Scan val scan = new Scan() scan.addFamily(\"o\".getBytes()) scan.addColumn(\"o\".getBytes(), \"browserName\".getBytes()) conf.set(TableInputFormat.SCAN, TableMapReduceUtil.convertScanToString(scan)) // 读取数据 val hBaseRDD = spark.sparkContext.newAPIHadoopRDD( conf, classOf[TableInputFormat], classOf[ImmutableBytesWritable], classOf[Result] ) // 统计 val resultRDD = hBaseRDD.map(x =&gt; &#123; val browserName = Bytes.toString(x._2.getValue(\"o\".getBytes(), \"browserName\".getBytes())) (browserName, 1) &#125;).reduceByKey(_+_) resultRDD.foreach(println) // 输出到mysql resultRDD.coalesce(1).foreachPartition(part =&gt; &#123; Try &#123; val connection = &#123; Class.forName(\"com.mysql.jdbc.Driver\") val url = \"jdbc:mysql://localhost:3306/spark?characterEncoding=UTF-8\" val user = \"root\" val password = \"root\" DriverManager.getConnection(url, user, password) &#125; val preAutoCommit = connection.getAutoCommit connection.setAutoCommit(false) val sql = \"insert into browser_stat (date, browser, cnt) values (?, ?, ?)\" val pstmt = connection.prepareStatement(sql) pstmt.addBatch(s\"delete from browser_stat where date = $date\") part.foreach(x =&gt; &#123; pstmt.setString(1, date) pstmt.setString(2, x._1) pstmt.setString(3, x._2) pstmt.addBatch() &#125;) pstmt.executeBatch() connection.commit() (connection, preAutoCommit) &#125; match &#123; case Success((connection, preAutoCommit)) =&gt; &#123; connection.setAutoCommit(preAutoCommit) if (null != connection) conneciton.close() &#125; case Failure(e) =&gt; throw e &#125; &#125;) spark.stop() &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://yoursite.com/tags/HBase/"},{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"},{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"}]},{"title":"Spark术语及Spark-on-yarn","slug":"Spark术语及Spark-on-yarn","date":"2020-06-29T02:58:34.000Z","updated":"2020-06-29T02:58:34.912Z","comments":true,"path":"2020/06/29/Spark术语及Spark-on-yarn/","link":"","permalink":"http://yoursite.com/2020/06/29/Spark%E6%9C%AF%E8%AF%AD%E5%8F%8ASpark-on-yarn/","excerpt":"","text":"术语概念机器层面 Master 节点负责管理Worker节点 Worker 节点 与Master节点通信，管理Executor进程 应用层面 Application Spark 应用程序 Driver 驱动程序 Application中的main方法及创建SparkContext 创建SparkContext是用来准备Application的运行环境 SparkContext向ClusterManager申请资源 执行完成之后关闭资源 ClusterManager 资源管理器 集群上获取资源的外部服务 分类： Standalone Spark原生的资源管理器，由Master负责资源分配 Hadoop Yarn, 由Yarn中的ResearchManager负责资源分配 Messos, 由Messos中的Messos Master负责资源分配 executor 执行器 在Worker节点上，有多个exector执行器，每个执行器是一个进程，是用来执行Task和存储数据到内存或者磁盘上， 每个执行器里面有一个线程池，每个线程来执行一个Task Job 多个Stage组成Job Stage &lt;=&gt; TaskSet 一组Task, Stage分成两种类型ShuffleMapStage、ResultStage Task 被送到某个Executor上的工作任务；单个分区数据集上的最小处理流程单元 DAG 有向无环图 RDD之间的依赖关系 DAGScheduler 有向无环图调度器 将DAG划分出Stage 划分Stage： 从后往前进行逆推划分，以宽依赖作为划分依据，遇到宽依赖就作为Stage压入栈 将Stage转化为TaskSet，提交到TaskScheduler 跟踪状态 获取结果 TaskScheduler 任务调度器 接收到TaskSet，提交给Worker， 每个TaskSet会被送到一个TaskManager，TaskManager根据就近原则给Task分配资源，TaskManager根据调度策略将Task分配给Executor进行执行 调度策略： FIFO 队列规则 FAIR 公平调度 Spark on Yarn的执行流程及两种模式1. Yarn组件 ResourceManager: 负责整个集群的资源管理和分配 NodeManager: 每个节点的资源、任务管理，负责启动和停止Container ApplicationMaster: 每个Application有一个ApplicationMaster, 负责告知NodeManager分配和启动Container Container: 抽象资源 2. yarn-cluster模式 ResourceManager —–&gt; 在集群中选择一个NodeManager分配Container， 在这个Container中启动 ApplicationMaster 进程 ApplicationMaster 初始化 SparkContext ApplicationMaster 向ResourceManager 申请Container, 并告知NodeManager启动Executor进程 SparkContext –&gt; DAGScheduler —&gt; TaskScheduler —&gt; 分配 Task –&gt; Executor执行 —&gt; 运行状态返回给Driver/Application Master 3. yarn-client 模式 ResourceManager —-&gt; 在集群中选择一个NodeManager分配Container，在这个Container中启动ApplicationMaster进程 Driver 运行在Client客户端上, 初始化 SparkContext client上的SparkContext 与 ApplicationMaster进行通信 ApplicationMaster 向 ResourceManager 申请Container，并告知NodeManager启动Executor进程 SparkContext —&gt; DAGScheduler —&gt; TaskScheduler —&gt; 分配Task —&gt; Executor执行 —&gt; 运行状态返回给 Driver/client 4. 两种模式的区别通过 --deploy-mode 指定 SparkContext的初始化位置不同， cluster模式Driver是在集群的一个Node上的， client模式Driver是在客户端上； Driver需要与Executor进行通信，所以cluster模式提交之后关闭client不会影响任务，client模式关闭掉client，任务失败； 生产正常使用cluster模式，调试可以使用client模式","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"},{"name":"Yarn","slug":"Yarn","permalink":"http://yoursite.com/tags/Yarn/"}]},{"title":"读取HBase统计分析","slug":"读取HBase统计分析","date":"2020-06-28T07:18:11.000Z","updated":"2020-06-28T07:18:11.993Z","comments":true,"path":"2020/06/28/读取HBase统计分析/","link":"","permalink":"http://yoursite.com/2020/06/28/%E8%AF%BB%E5%8F%96HBase%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90/","excerpt":"","text":"1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package com.kowhoyimport org.apache.hadoop.conf.Configurationimport org.apache.hadoop.hbase.client.&#123;Result, Scan&#125;import org.apache.hadoop.hbase.io.ImmutableBytesWritableimport org.apache.hadoop.hbase.mapreduce.&#123;TableInputFormat, TableMapReduceUtil&#125;import org.apache.hadoop.hbase.util.Bytesimport org.apache.spark.sql.SparkSession/** * @DESC 日志按国家省份统计分析 * @Date 2020/6/28 2:49 下午 **/object AnalysisAppV2 &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession.builder().config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .appName(\"AnalysisAppV2\").master(\"local[2]\").getOrCreate() spark.sparkContext.setLogLevel(\"WARN\") // 从HBase读取数据 val date = \"2020-06-28\" val tableName = \"access_\" + date val conf = new Configuration() val scan = new Scan() scan.addFamily(Bytes.toBytes(\"o\")) //列族 scan.addColumn(Bytes.toBytes(\"o\"), Bytes.toBytes(\"country\")) scan.addColumn(Bytes.toBytes(\"o\"), Bytes.toBytes(\"province\")) conf.set(\"hbase.rootdir\", \"hdfs://localhost:9000/hbase\") conf.set(\"hbase.zookeeper.quorum\", \"localhost:2181\") conf.set(TableInputFormat.INPUT_TABLE, tableName) conf.set(TableInputFormat.SCAN, TableMapReduceUtil.convertScanToString(scan)) val hBaseRDD = spark.sparkContext.newAPIHadoopRDD( conf, classOf[TableInputFormat], classOf[ImmutableBytesWritable], classOf[Result] ) hBaseRDD.cache() // 方式1, 使用SparkCore 统计 hBaseRDD.map(x =&gt; &#123; val country = Bytes.toString(x._2.getValue(\"o\".getBytes(), \"country\".getBytes())) val province = Bytes.toString(x._2.getValue(\"o\".getBytes(), \"province\".getBytes())) ((country, province), 1) &#125;).reduceByKey(_+_) .map(x =&gt; (x._2, x._1)).sortByKey(false) .map(x =&gt; (x._2, x._1)).take(10).foreach(println) println(\"---------------------------------------------------\") // 方式二, 使用SparkSql API import spark.implicits._ hBaseRDD.map(x =&gt; &#123; val country = Bytes.toString(x._2.getValue(\"o\".getBytes(), \"country\".getBytes())) val province = Bytes.toString(x._2.getValue(\"o\".getBytes(), \"province\".getBytes())) CountryProvince(country, province) &#125;).toDF.select(\"country\", \"province\") .groupBy(\"country\", \"province\").count().sort($\"count\".desc).show(10, false) println(\"---------------------------------------------------\") // 方式三, 使用SparkSql 临时表 hBaseRDD.map(x =&gt; &#123; val country = Bytes.toString(x._2.getValue(\"o\".getBytes(), \"country\".getBytes())) val province = Bytes.toString(x._2.getValue(\"o\".getBytes(), \"province\".getBytes())) CountryProvince(country, province) &#125;).toDF().createOrReplaceTempView(\"log\") spark.sql(\"select country, province, count(1) cnt from log group by country, province order by cnt desc limit 10\").show(false) hBaseRDD.unpersist(true) spark.stop() &#125; case class CountryProvince(country:String, province:String)&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://yoursite.com/tags/HBase/"},{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"}]},{"title":"日志ETL到HBase","slug":"日志ETL到HBase","date":"2020-06-28T05:50:57.000Z","updated":"2020-06-28T05:50:57.876Z","comments":true,"path":"2020/06/28/日志ETL到HBase/","link":"","permalink":"http://yoursite.com/2020/06/28/%E6%97%A5%E5%BF%97ETL%E5%88%B0HBase/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282import java.util.Localeimport java.util.regex.&#123;Matcher, Pattern&#125;import java.util.zip.CRC32import com.kowhoy.domain.&#123;IpRegionInfo, LogSchema, UserAgentInfo&#125;import com.kowhoy.util.&#123;IpParseUtil, UserAgentUtil&#125;import org.apache.commons.lang3.StringUtilsimport org.apache.commons.lang3.time.FastDateFormatimport org.apache.hadoop.conf.Configurationimport org.apache.hadoop.hbase.TableNameimport org.apache.hadoop.hbase.client.&#123;Admin, ColumnFamilyDescriptorBuilder, Connection, ConnectionFactory, Put, TableDescriptorBuilder&#125;import org.apache.hadoop.hbase.io.ImmutableBytesWritableimport org.apache.hadoop.hbase.mapreduce.TableOutputFormatimport org.apache.hadoop.hbase.util.Bytesimport org.apache.spark.sql.SparkSession/** * @DESC 日志数据ETL =&gt; HBase * @Date 2020/6/28 10:20 上午 **/object LogETLAppV2 &#123; def main(args: Array[String]): Unit = &#123; if (args.length &lt; 1) &#123; System.err.println(\"Usage: LogETLAppV2 &lt;date&gt;\") System.exit(1) &#125; val Array(date) = args val spark = SparkSession.builder().appName(\"LogETLAppV2\").master(\"local[2]\").getOrCreate() // 日志文件地址 val logPath: String = \"hdfs://localhost:9000/access_logs/2020-06-17.log\" // 日志RDD val logRDD = spark.sparkContext.textFile(logPath) // 正则模式 val pattern: Pattern = Pattern.compile(\"([\\\\d\\\\.]&#123;7,&#125;) - - (\\\\[.&#123;26,&#125;\\\\]) (\\\".+?\\\"|\\\\-) (\\\\d&#123;3&#125;|\\\\-) (\\\\d+|\\\\-) (\\\".+?\\\"|\\\\-) (\\\".+?\\\"|\\\\-) - (\\\".+?\\\"|\\\\-) (\\\".+?\\\"|\\\\-) ([\\\\d\\\\.]&#123;7,&#125;:\\\\d+|\\\\-) (\\\\d+|\\\\-) (\\\\d+?\\\\.\\\\d+|\\\\-) (\\\\d+?\\\\.\\\\d+|\\\\-)\") // 日志正则解析出各个字段 val transformRDD = logRDD.map&#123; line =&gt; &#123; val matcher: Matcher = pattern.matcher(line) if (matcher.matches()) &#123; val ip = matcher.group(1) val time = matcher.group(2) val requestData = matcher.group(3) val status = matcher.group(4) val bytesSent = matcher.group(5) val host = matcher.group(6) val referer = matcher.group(7) val ua = matcher.group(8) val hostIp = matcher.group(10) val spendTime = matcher.group(12) // 解析IP val ipInfo:IpRegionInfo = IpParseUtil.getIpRegionInfo(ip) var country = \"unknown\" var province = \"unknown\" var city = \"unknown\" var operator = \"unknown\" if (null != ipInfo) &#123; country = ipInfo.getCountry province = ipInfo.getProvince city = ipInfo.getCity operator = ipInfo.getOperator &#125; // 解析UserAgent val uaInfo:UserAgentInfo = UserAgentUtil.getUserAgentInfo(ua) var browserName = \"unknown\" var browserVersion= \"unknown\" var osName = \"unknown\" var osVersion = \"unknown\" if (null != uaInfo) &#123; browserName = uaInfo.getBrowserName browserVersion = uaInfo.getBrowserVersion osName = uaInfo.getOsName osVersion = uaInfo.getOsVersion &#125; // 切割requestData val requestArr = requestData.split(\" \") var method = \"-\" var url = \"-\" var protocol = \"-\" if (requestArr.length &gt; 2) &#123; method = requestArr(0).replace(\"\\\"\", \"\") url = requestArr(1) protocol = requestArr(2).replace(\"\\\"\", \"\") &#125; LogSchema(ip, country, province, city, operator, time, method, url, protocol, status, bytesSent, host, referer, ua, browserName, browserVersion, osName, osVersion, hostIp, spendTime) &#125; else &#123; LogSchema(\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\") &#125; &#125; &#125;.take(1000000) // sparkSql udf 格式化时间字段 var logDF = spark.createDataFrame(transformRDD) import org.apache.spark.sql.functions.udf val formatTime = udf((time:String) =&gt; &#123; try &#123; val logTimeStr = time.substring(time.indexOf(\"[\")+1, time.lastIndexOf(\"]\")) val logTime = FastDateFormat.getInstance(\"dd/MMM/yyy:HH:mm:ss Z\", Locale.ENGLISH).parse(logTimeStr).getTime FastDateFormat.getInstance(\"yyyy-MM-dd HH:mm:ss\").format(logTime) &#125; catch &#123; case e: Exception =&gt; \"unknown\" &#125; &#125;) logDF = logDF.withColumn(\"formatTime\", formatTime(logDF(\"time\"))) // 保存数据到HBase val hBaseInfoRDD = logDF.rdd.map(x =&gt; &#123; //1. 将数据转化成HashMap便于遍历 val ip = x.getAs[String](\"ip\") val country = x.getAs[String](\"country\") val province = x.getAs[String](\"province\") val city = x.getAs[String](\"city\") val operator = x.getAs[String](\"operator\") val time = x.getAs[String](\"time\") val formatTime = x.getAs[String](\"formatTime\") val method = x.getAs[String](\"method\") val url = x.getAs[String](\"url\") val protocol = x.getAs[String](\"protocol\") val status = x.getAs[String](\"status\") val bytesSent = x.getAs[String](\"bytesSent\") val host = x.getAs[String](\"host\") val referer = x.getAs[String](\"referer\") val ua = x.getAs[String](\"ua\") val browserName = x.getAs[String](\"browserName\") val browserVersion = x.getAs[String](\"browserVersion\") val osName = x.getAs[String](\"osName\") val osVersion = x.getAs[String](\"osVersion\") val hostIp = x.getAs[String](\"hostIp\") val spendTime = x.getAs[String](\"spendTime\") val columns = scala.collection.mutable.HashMap[String, String]() columns.put(\"ip\", ip) columns.put(\"country\", country) columns.put(\"province\", province) columns.put(\"city\", city) columns.put(\"operator\", operator) columns.put(\"time\", time) columns.put(\"formatTime\", formatTime) columns.put(\"method\", method) columns.put(\"url\", url) columns.put(\"protocol\", protocol) columns.put(\"status\", status) columns.put(\"bytesSent\", bytesSent) columns.put(\"host\", host) columns.put(\"referer\", referer) columns.put(\"ua\", ua) columns.put(\"browserName\", browserName) columns.put(\"browserVersion\", browserVersion) columns.put(\"osName\", osName) columns.put(\"osVersion\", osVersion) columns.put(\"hostIp\", hostIp) columns.put(\"spendTime\", spendTime) // 2. 生成RowKey val rowKey = getRowKey(date, referer+url+ip+ua) val put = new Put(Bytes.toBytes(rowKey)) for ((k, v) &lt;- columns) &#123; put.addColumn(Bytes.toBytes(\"o\"), Bytes.toBytes(String.valueOf(k)), Bytes.toBytes(String.valueOf(v))) &#125; (new ImmutableBytesWritable(rowKey.getBytes), put) &#125;) // 3. 配置连接、保存数据 val conf = new Configuration() conf.set(\"hbase.rootdir\", \"hdfs://localhost:9000/hbase\") conf.set(\"hbase.zookeeper.quorum\", \"localhost:2181\") val tableName = createTable(date, conf) conf.set(TableOutputFormat.OUTPUT_TABLE, tableName) hBaseInfoRDD.saveAsNewAPIHadoopFile( \"hdfs://localhost:9000/hbase\", classOf[ImmutableBytesWritable], classOf[Put], classOf[TableOutputFormat[ImmutableBytesWritable]], conf ) println(\"写入完成\") spark.stop() &#125; /** * @Desc 生成RowKey * @Date 11:16 上午 2020/6/28 * @Param [date, info] * @Return java.lang.String **/ def getRowKey(date: String, info: String): String = &#123; val builder = new StringBuilder(date) builder.append(\"_\") val crc32 = new CRC32() crc32.reset() if (StringUtils.isNotEmpty(info)) &#123; crc32.update(Bytes.toBytes(info)) &#125; builder.append(crc32.getValue) builder.toString() &#125; /** * @Desc 创建表 * @Date 11:28 上午 2020/6/28 * @Param [date, conf] * @Return java.lang.String **/ def createTable(date: String, conf: Configuration): String = &#123; val table = \"access_\" + date var connection: Connection = null var admin: Admin = null try &#123; connection = ConnectionFactory.createConnection(conf) admin = connection.getAdmin val tableName = TableName.valueOf(table) if (admin.tableExists(tableName)) &#123; admin.disableTable(tableName) admin.deleteTable(tableName) println(\"表删除完成\") &#125; val tableDescriptorBuilder = TableDescriptorBuilder.newBuilder(tableName) val columnDescriptor = ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(\"o\")).build() tableDescriptorBuilder.setColumnFamily(columnDescriptor) admin.createTable(tableDescriptorBuilder.build) &#125; catch &#123; case e:Exception =&gt; e.printStackTrace() &#125; finally &#123; if (null != admin) &#123; admin.close() &#125; if (null != connection) &#123; connection.close() &#125; &#125; table &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"},{"name":"HBase","slug":"HBase","permalink":"http://yoursite.com/tags/HBase/"},{"name":"日志","slug":"日志","permalink":"http://yoursite.com/tags/%E6%97%A5%E5%BF%97/"},{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"},{"name":"ETL","slug":"ETL","permalink":"http://yoursite.com/tags/ETL/"}]},{"title":"IP解析工具","slug":"IP解析工具","date":"2020-06-28T05:48:14.000Z","updated":"2020-06-28T05:48:14.412Z","comments":true,"path":"2020/06/28/IP解析工具/","link":"","permalink":"http://yoursite.com/2020/06/28/IP%E8%A7%A3%E6%9E%90%E5%B7%A5%E5%85%B7/","excerpt":"","text":"IpRegionInfo12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package com.kowhoy.domain;/** * @ClassName IpRegionInfo * @DESC 自定义IP解析地理位置类 _城市Id|国家|区域|省份|城市|ISP_ 995|中国|0|上海|上海市|联通|126445 * @Date 2020/6/16 3:38 下午 **/public class IpRegionInfo &#123; private String country; private String province; private String city; private String operator; public void setCountry(String country) &#123; this.country = country; &#125; public void setProvince(String province) &#123; this.province = province; &#125; public void setCity(String city) &#123; this.city = city; &#125; public void setOperator(String operator) &#123; this.operator = operator; &#125; public String getCountry() &#123; return country; &#125; public String getProvince() &#123; return province; &#125; public String getCity() &#123; return city; &#125; public String getOperator() &#123; return operator; &#125; @Override public String toString() &#123; return \"IpRegionInfo&#123;\" + \"country='\" + country + '\\'' + \", province='\" + province + '\\'' + \", city='\" + city + '\\'' + \", operator='\" + operator + '\\'' + '&#125;'; &#125;&#125; UserAgentUtil123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package com.kowhoy.util;import com.kowhoy.domain.UserAgentInfo;import cz.mallat.uasparser.OnlineUpdater;import cz.mallat.uasparser.UASparser;import org.apache.commons.lang3.StringUtils;import java.io.IOException;/** * @ClassName UserAgentUtil * @DESC UserAgent解析工具 * @Date 2020/6/16 2:58 下午 **/public class UserAgentUtil &#123; private static UASparser parser = null; static &#123; try &#123; parser = new UASparser(OnlineUpdater.getVendoredInputStream()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; /** * 获取UserAgent解析 * @param ua:String * @return UserAgentInfo */ public static UserAgentInfo getUserAgentInfo(String ua) &#123; UserAgentInfo info = null; try &#123; if (StringUtils.isNotEmpty(ua)) &#123; info = new UserAgentInfo(); cz.mallat.uasparser.UserAgentInfo uaInfo = parser.parse(ua); if (null != uaInfo) &#123; info.setBrowserName(uaInfo.getUaFamily()); info.setBrowserVersion(uaInfo.getBrowserVersionInfo()); info.setOsName(uaInfo.getOsFamily()); info.setOsVersion(uaInfo.getOsName()); &#125; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return info; &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}]},{"title":"UserAgent解析工具","slug":"UserAgent解析工具","date":"2020-06-28T05:46:58.000Z","updated":"2020-06-28T05:46:59.018Z","comments":true,"path":"2020/06/28/UserAgent解析工具/","link":"","permalink":"http://yoursite.com/2020/06/28/UserAgent%E8%A7%A3%E6%9E%90%E5%B7%A5%E5%85%B7/","excerpt":"","text":"UserAgentInfo12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package com.kowhoy.domain;/** * @ClassName UserAgentInfo * @DESC 自定义UserAgent工具类 * @Date 2020/6/16 2:56 下午 **/public class UserAgentInfo &#123; private String browserName; private String browserVersion; private String osName; private String osVersion; public void setBrowserName(String browserName) &#123; this.browserName = browserName; &#125; public void setBrowserVersion(String browserVersion) &#123; this.browserVersion = browserVersion; &#125; public void setOsName(String osName) &#123; this.osName = osName; &#125; public void setOsVersion(String osVersion) &#123; this.osVersion = osVersion; &#125; public String getBrowserName() &#123; return browserName; &#125; public String getBrowserVersion() &#123; return browserVersion; &#125; public String getOsName() &#123; return osName; &#125; public String getOsVersion() &#123; return osVersion; &#125; @Override public String toString() &#123; return \"UserAgentInfo&#123;\" + \"browserName='\" + browserName + '\\'' + \", browserVersion='\" + browserVersion + '\\'' + \", osName='\" + osName + '\\'' + \", osVersion='\" + osVersion + '\\'' + '&#125;'; &#125;&#125; UserAgentUtil123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package com.kowhoy.util;import com.kowhoy.domain.UserAgentInfo;import cz.mallat.uasparser.OnlineUpdater;import cz.mallat.uasparser.UASparser;import org.apache.commons.lang3.StringUtils;import java.io.IOException;/** * @ClassName UserAgentUtil * @DESC UserAgent解析工具 * @Date 2020/6/16 2:58 下午 **/public class UserAgentUtil &#123; private static UASparser parser = null; static &#123; try &#123; parser = new UASparser(OnlineUpdater.getVendoredInputStream()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; /** * 获取UserAgent解析 * @param ua:String * @return UserAgentInfo */ public static UserAgentInfo getUserAgentInfo(String ua) &#123; UserAgentInfo info = null; try &#123; if (StringUtils.isNotEmpty(ua)) &#123; info = new UserAgentInfo(); cz.mallat.uasparser.UserAgentInfo uaInfo = parser.parse(ua); if (null != uaInfo) &#123; info.setBrowserName(uaInfo.getUaFamily()); info.setBrowserVersion(uaInfo.getBrowserVersionInfo()); info.setOsName(uaInfo.getOsFamily()); info.setOsVersion(uaInfo.getOsName()); &#125; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return info; &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}]},{"title":"HBase DML API","slug":"HBase-DML-API","date":"2020-06-15T07:26:18.000Z","updated":"2020-06-15T07:26:18.528Z","comments":true,"path":"2020/06/15/HBase-DML-API/","link":"","permalink":"http://yoursite.com/2020/06/15/HBase-DML-API/","excerpt":"","text":"HBase DML API一、写数据1. 单条写12345678910111213141516@Testpublic void addSingle() throws Exception &#123; String tableName = \"sku_info\"; TableName table = TableName.valueOf(tableName); String rowKey = \"ok\"; Put put = new Put(Bytes.toBytes(rowKey)); put.addColumn(Bytes.toBytes(\"basic\"), Bytes.toBytes(\"price\"), Bytes.toBytes(\"12.8\")); put.addColumn(Bytes.toBytes(\"basic\"), Bytes.toBytes(\"code\"), Bytes.toBytes(\"O_1\")); put.addColumn(Bytes.toBytes(\"area\"), Bytes.toBytes(\"province\"), Bytes.toBytes(\"SH\")); put.addColumn(Bytes.toBytes(\"area\"), Bytes.toBytes(\"city\"), Bytes.toBytes(\"SH\")); connection.getTable(table).put(put);&#125; 2. 批量写123456789101112131415161718192021222324@Testpublic void addBatch() throws Exception &#123; String tableName = \"sku_info\"; TableName table = TableName.valueOf(tableName); List&lt;Put&gt; puts = new ArrayList&lt;Put&gt;(); Put put1 = new Put(Bytes.toBytes(\"GA\")); put1.addColumn(Bytes.toBytes(\"basic\"), Bytes.toBytes(\"price\"), Bytes.toBytes(\"13.5\")); put1.addColumn(Bytes.toBytes(\"basic\"), Bytes.toBytes(\"code\"), Bytes.toBytes(\"G_1\")); put1.addColumn(Bytes.toBytes(\"area\"), Bytes.toBytes(\"province\"), Bytes.toBytes(\"JS\")); put1.addColumn(Bytes.toBytes(\"area\"), Bytes.toBytes(\"city\"), Bytes.toBytes(\"YC\")); Put put2 = new Put(Bytes.toBytes(\"GB\")); put2.addColumn(Bytes.toBytes(\"basic\"), Bytes.toBytes(\"price\"), Bytes.toBytes(\"11.14\")); put2.addColumn(Bytes.toBytes(\"basic\"), Bytes.toBytes(\"code\"), Bytes.toBytes(\"G_2\")); put2.addColumn(Bytes.toBytes(\"area\"), Bytes.toBytes(\"province\"), Bytes.toBytes(\"JS\")); put2.addColumn(Bytes.toBytes(\"area\"), Bytes.toBytes(\"city\"), Bytes.toBytes(\"Wx\")); puts.add(put1); puts.add(put2); connection.getTable(table).put(puts);&#125; 二、修改数据12345678910@Testpublic void update() throws Exception &#123; String tableName = \"sku_info\"; TableName table = TableName.valueOf(tableName); Put put = new Put(\"GA\".getBytes()); put.addColumn(Bytes.toBytes(\"basic\"), Bytes.toBytes(\"price\"), Bytes.toBytes(\"10.9\")); connection.getTable(table).put(put);&#125; 三、获取数据1. 打印结果方法12345678910111213141516/** * 输出结果 * @param result: Result */private void printResult(Result result) &#123; Cell[] cells = result.rawCells(); for (Cell cell : cells) &#123; System.out.println(Bytes.toString(result.getRow()) + \"\\t\" + Bytes.toString(CellUtil.cloneFamily(cell)) + \"\\t\" + Bytes.toString(CellUtil.cloneQualifier(cell)) + \"\\t\" + Bytes.toString(CellUtil.cloneValue(cell)) + \"\\t\" + cell.getTimestamp() ); &#125;&#125; 2. 获取指定RowKey数据1234567891011@Testpublic void getData() throws Exception &#123; String tableName = \"sku_info\"; TableName table = TableName.valueOf(tableName); Get get = new Get(\"ok\".getBytes()); Result result = connection.getTable(table).get(get); printResult(result);&#125; 3. 获取执行RowKey,指定列数据12345678910111213@Testpublic void getDataWithPartColumns() throws Exception &#123; String tableName = \"sku_info\"; TableName table = TableName.valueOf(tableName); Get get = new Get(\"ok\".getBytes()); get.addColumn(Bytes.toBytes(\"area\"), Bytes.toBytes(\"province\")); Result result = connection.getTable(table).get(get); printResult(result);&#125; 4. 扫描全表123456789101112131415@Testpublic void scanData() throws Exception &#123; String tableName = \"sku_info\"; TableName table = TableName.valueOf(tableName); Scan scan = new Scan(); ResultScanner resultScanner = connection.getTable(table).getScanner(scan); for (Result result: resultScanner) &#123; printResult(result); System.out.println(\"--------------\"); &#125;&#125; 5. 扫描指定列12345678910111213141516171819@Testpublic void scanColumns() throws Exception &#123; String tableName = \"sku_info\"; TableName table = TableName.valueOf(tableName); Scan scan = new Scan(); scan.addColumn(Bytes.toBytes(\"area\"), Bytes.toBytes(\"city\")); scan.addColumn(Bytes.toBytes(\"basic\"), Bytes.toBytes(\"code\")); ResultScanner resultScanner = connection.getTable(table).getScanner(scan); for (Result result : resultScanner) &#123; printResult(result); System.out.println(\"-------------\"); &#125;&#125; 6. RowKey条件Scan12345678910111213141516@Testpublic void scanWithRowKeyCondition() throws Exception &#123; String tableName = \"sku_info\"; TableName table = TableName.valueOf(tableName); Scan scan = new Scan().withStartRow(Bytes.toBytes(\"GB\")).withStopRow(Bytes.toBytes(\"ok\")); ResultScanner resultScanner = connection.getTable(table).getScanner(scan); for (Result result : resultScanner) &#123; printResult(result); System.out.println(\"--------------\"); &#125;&#125; 7. 单个Filter1234567891011121314151617181920@Testpublic void singleFilter() throws Exception &#123; String tableName = \"sku_info\"; TableName table = TableName.valueOf(tableName); Scan scan = new Scan(); String reg = \"^*k\"; Filter filter = new RowFilter(CompareOperator.EQUAL, new RegexStringComparator(\"reg\")); scan.setFilter(filter); ResultScanner resultScanner = connection.getTable(table).getScanner(scan); for (Result result: resultScanner) &#123; printResult(result); System.out.println(\"-------------\"); &#125;&#125; 8.多个Filter1234567891011121314151617181920212223242526@Testpublic void muchFilter() throws Exception &#123; String tableName = \"sku_info\"; TableName table = TableName.valueOf(tableName); Scan scan = new Scan(); FilterList filters = new FilterList(FilterList.Operator.MUST_PASS_ONE); Filter filter1 = new PrefixFilter(\"o\".getBytes()); Filter filter2 = new RowFilter(CompareOperator.EQUAL, new RegexStringComparator(\"^*B\")); filters.addFilter(filter1); filters.addFilter(filter2); scan.setFilter(filters); ResultScanner resultScanner = connection.getTable(table).getScanner(scan); for (Result result: resultScanner) &#123; printResult(result); System.out.println(\"-----------\"); &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://yoursite.com/tags/HBase/"}]},{"title":"HBase DDL API","slug":"HBase-DDL-API","date":"2020-06-12T06:19:55.000Z","updated":"2020-06-15T02:41:25.456Z","comments":true,"path":"2020/06/12/HBase-DDL-API/","link":"","permalink":"http://yoursite.com/2020/06/12/HBase-DDL-API/","excerpt":"","text":"HBase DDL APIGAV12345&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;$&#123;hbase.version&#125;&lt;/version&gt;&lt;/dependency&gt; 主要依赖1234567891011121314import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HColumnDescriptor;import org.apache.hadoop.hbase.HTableDescriptor;import org.apache.hadoop.hbase.TableName;import org.apache.hadoop.hbase.client.Admin;import org.apache.hadoop.hbase.client.Connection;import org.apache.hadoop.hbase.client.ConnectionFactory;import org.apache.hadoop.hbase.util.Bytes;import org.junit.After;import org.junit.Assert;import org.junit.Before;import org.junit.Test;import java.io.IOException; Create Connection and Admin ConnectionFactory.createConnectionConfiguration 1234567891011121314@Beforepublic void setUp() &#123; Configuration configuration = new Configuration(); configuration.set(\"hbase.rootdir\", \"hdfs://localhost:9000/hbase\"); configuration.set(\"hbase.zookeeper.quorum\", \"localhost:2181\"); try &#123; connection = ConnectionFactory.createConnection(configuration); admin = connection.getAdmin(); &#125; catch (IOExpection e)&#123; e.printStackTrace(); &#125; Assert assertNotNull(admin); &#125; create table 老版本 since 2.0 version and will be removed in 3.0 version 创建表需要有表的描述 HTableDescriptor 表的描述包含了列族的描述 HColumnDescriptor 1234567891011121314151617@Testpublic void createTable() throws Exception &#123; String tableName = \"demo_table\"; TableName table = TableName.valueof(tableName); if admin.tableExists(table) &#123; System.out.println(tableName + \"表已经存在...\"); &#125; else &#123; HTableDescriptor tableDescriptor = new HTableDescriptor(table); tableDescriptor.addFamily(new HColumnDescriptor(\"info\")); tableDescriptor.addFamily(new HColumnDescriptor(\"address\")); admin.createTable(tableDescriptor); System.out.println(tableName + \"表创建完成...\"); &#125;&#125; 新版本 12345678910111213141516171819202122@Testpublic void createTableNew() throws Exception &#123; String tableName = \"sku2\"; TableName table = TableName.valueOf(tableName); if (admin.tableExists(table)) &#123; System.out.println(tableName + \"已存在...\"); &#125; else &#123; TableDescriptorBuilder tableDescriptorBuilder = TableDescriptorBuilder.newBuilder(table); ColumnFamilyDescriptor columnFamilyDescriptor1 = ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(\"basic\")).build(); ColumnFamilyDescriptor columnFamilyDescriptor2 = ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(\"area\")).build(); tableDescriptorBuilder.setColumnFamily(columnFamilyDescriptor1); tableDescriptorBuilder.setColumnFamily(columnFamilyDescriptor2); admin.createTable(tableDescriptorBuilder.build()); System.out.println(tableName + \"创建完成...\\n\"); this.descTableInfos(); &#125;&#125; desc table 2 版本listTables() since 2.0 version and will be removed in 3.0 version. Use listTableDescriptors().getColumnFamilies() 12345678910111213@Testpublic void descTablesInfos() throws Exception &#123; HTableDescriptor[] tables = admin.listTables(); for (HTableDescriptor table : tables) &#123; System.out.println(table.getNameAsString()); HColumnDescriptor[] columns = table.getColumnFamilies(); for (HColumnDescriptor column : columns) &#123; System.out.println(\"\\t\" + column.getNameAsString()); &#125; &#125;&#125; 3.0 版本listTableDescriptorsgetColumnFamilies 12345678910111213141516@Testpublic void descTableInfos() throws Exception &#123; List&lt;TableDescriptor&gt; tableDescriptors = admin.listTableDescriptors(); for (TableDescriptor tableDescriptor : tableDescriptors) &#123; TableName table = tableDescriptor.getTableName(); String tableName = table.getNameAsString(); ColumnFamilyDescriptor[] columnFamilyDescriptors = tableDescriptor.getColumnFamilies(); for (ColumnFamilyDescriptor columnFamilyDescriptor: columnFamilyDescriptors) &#123; String columnName = columnFamilyDescriptor.getNameAsString(); System.out.println(tableName + \"\\n\\t\" + columnName); &#125; &#125;&#125; delete Column Family1234567891011@Testpublic void deleteColumnFamily() throws Exception &#123; String tableName = \"member\"; TableName table = TableName.valueOf(tableName); byte[] column = Bytes.toBytes(\"info\"); admin.deleteColumnFamily(table, column);&#125; drop table12345678910111213141516171819@Testpublic void deleteTable() &#123; String tableName = \"java_api\"; TableName table = TableName.valueOf(tableName); try &#123; if (admin.tableExists(table)) &#123; admin.disableTable(table); admin.deleteTable(table); System.out.println(tableName + \"删除完成...\"); &#125; else &#123; System.out.println(tableName + \"不存在...\"); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://yoursite.com/tags/HBase/"}]},{"title":"HBASE DML","slug":"HBASE-DML","date":"2020-06-10T14:05:39.000Z","updated":"2020-06-10T14:05:39.737Z","comments":true,"path":"2020/06/10/HBASE-DML/","link":"","permalink":"http://yoursite.com/2020/06/10/HBASE-DML/","excerpt":"","text":"HBASE DML 创建数据 1put 表名,RowKey,&#39;CF:COL&#39;, VALUE 扫描表 1scan 表名 删除数据 1delete 表名, RowKey, &#39;CF:COL&#39; 修改数据 1put 表名, RowKey, &#39;CF:COL&#39;, VALUE 删除整行 1deleteall 表名, RowKey 行数 1count 表名 查看数据 1get 表名, RowKey [, &#39;CF:COL&#39;] 清空表 1truncate 表名","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HBASE","slug":"HBASE","permalink":"http://yoursite.com/tags/HBASE/"}]},{"title":"HBASE DDL","slug":"HBASE-DDL","date":"2020-06-10T13:39:17.000Z","updated":"2020-06-10T13:39:17.704Z","comments":true,"path":"2020/06/10/HBASE-DDL/","link":"","permalink":"http://yoursite.com/2020/06/10/HBASE-DDL/","excerpt":"","text":"HBASE DDL 启动HBASE SHELL 1hbase shell 查看版本、状态 1234567hbase(main):037:0&gt; version2.2.2, re6513a76c91cceda95dad7af246ac81d46fa2589, Sat Oct 19 10:10:12 UTC 2019hbase(main):036:0&gt; status1 active master, 0 backup masters, 1 servers, 1 dead, 3.0000 average loadTook 0.0105 seconds 创建表 1234567hbase(main):042:0* create &#39;member&#39;, &#39;member_id&#39;, &#39;info&#39;, &#39;address&#39;Created table memberTook 2.2730 seconds&#x3D;&gt; Hbase::Table - member## create &#39;表名&#39;, &#39;列族1&#x2F;cf&#39;, &#39;列族2&#x2F;cf&#39;... DESC表 123456789101112131415161718192021hbase(main):043:0&gt; desc &#39;member&#39;Table member is ENABLEDmemberCOLUMN FAMILIES DESCRIPTION&#123;NAME &#x3D;&gt; &#39;address&#39;, VERSIONS &#x3D;&gt; &#39;1&#39;, EVICT_BLOCKS_ON_CLOSE &#x3D;&gt; &#39;false&#39;, NEW_VERSION_BEHAVIOR &#x3D;&gt; &#39;false&#39;, KEEP_DELETED_CELLS &#x3D;&gt; &#39;FALSE&#39;, CACHE_DATA_ON_WRITE &#x3D;&gt; &#39;false&#39;, DATA_BLOCK_ENCODING &#x3D;&gt; &#39;NONE&#39;, TTL &#x3D;&gt; &#39;FOREVER&#39;, MIN_VERSIONS &#x3D;&gt; &#39;0&#39;, REPLICATION_SCOPE &#x3D;&gt; &#39;0&#39;, BLOOMFILTER &#x3D;&gt; &#39;ROW&#39;, CACHE_INDEX_ON_WRITE &#x3D;&gt; &#39;false&#39;, IN_MEMORY &#x3D;&gt; &#39;false&#39;, CACHE_BLOOMS_ON_WRITE &#x3D;&gt; &#39;false&#39;, PREFETCH_BLOCKS_ON_OPEN &#x3D;&gt; &#39;false&#39;, COMPRESSION &#x3D;&gt; &#39;NONE&#39;, BLOCKCACHE &#x3D;&gt; &#39;true&#39;, BLOCKSIZE &#x3D;&gt; &#39;65536&#39;&#125;&#123;NAME &#x3D;&gt; &#39;info&#39;, VERSIONS &#x3D;&gt; &#39;1&#39;, EVICT_BLOCKS_ON_CLOSE &#x3D;&gt; &#39;false&#39;, NEW_VERSION_BEHAVIOR &#x3D;&gt; &#39;false&#39;, KEEP_DELETED_CELLS &#x3D;&gt; &#39;FALSE&#39;, CACHE_DATA_ON_WRITE &#x3D;&gt; &#39;false&#39;, DATA_BLOCK_ENCODING &#x3D;&gt; &#39;NONE&#39;, TTL &#x3D;&gt; &#39;FOREVER&#39;, MIN_VERSIONS &#x3D;&gt; &#39;0&#39;, REPLICATION_SCOPE &#x3D;&gt; &#39;0&#39;, BLOOMFILTER &#x3D;&gt; &#39;ROW&#39;, CACHE_INDEX_ON_WRITE &#x3D;&gt; &#39;false&#39;, IN_MEMORY &#x3D;&gt; &#39;false&#39;, CACHE_BLOOMS_ON_WRITE &#x3D;&gt; &#39;false&#39;, PREFETCH_BLOCKS_ON_OPEN &#x3D;&gt; &#39;false&#39;, COMPRESSION &#x3D;&gt; &#39;NONE&#39;, BLOCKCACHE &#x3D;&gt; &#39;true&#39;, BLOCKSIZE &#x3D;&gt; &#39;65536&#39;&#125;&#123;NAME &#x3D;&gt; &#39;member_id&#39;, VERSIONS &#x3D;&gt; &#39;1&#39;, EVICT_BLOCKS_ON_CLOSE &#x3D;&gt; &#39;false&#39;, NEW_VERSION_BEHAVIOR &#x3D;&gt; &#39;false&#39;, KEEP_DELETED_CELLS &#x3D;&gt; &#39;FALSE&#39;, CACHE_DATA_ON_WRITE &#x3D;&gt; &#39;false&#39;, DATA_BLOCK_ENCODING &#x3D;&gt; &#39;NONE&#39;, TTL &#x3D;&gt; &#39;FOREVER&#39;, MIN_VERSIONS &#x3D;&gt; &#39;0&#39;, REPLICATION_SCOPE &#x3D;&gt; &#39;0&#39;, BLOOMFILTER &#x3D;&gt; &#39;ROW&#39;, CACHE_INDEX_ON_WRITE &#x3D;&gt; &#39;false&#39;, IN_MEMORY &#x3D;&gt; &#39;false&#39;, CACHE_BLOOMS_ON_WRITE &#x3D;&gt; &#39;false&#39;, PREFETCH_BLOCKS_ON_OPEN &#x3D;&gt; &#39;false&#39;, COMPRESSION &#x3D;&gt; &#39;NONE&#39;, BLOCKCACHE &#x3D;&gt; &#39;true&#39;, BLOCKSIZE &#x3D;&gt; &#39;65536&#39;&#125;3 row(s)QUOTAS0 row(s)Took 0.0436 seconds 查看表 1234567hbase(main):044:0&gt; listTABLEmemberuser2 row(s)Took 0.0048 seconds&#x3D;&gt; [&quot;member&quot;, &quot;user&quot;] 删除列族/CF 12345678910111213141516171819202122hbase(main):071:0&gt; alter &#39;member&#39;, &#39;delete&#39;&#x3D;&gt;&#39;member_id&#39;Updating all regions with the new schema...1&#x2F;1 regions updated.Done.Took 2.0179 secondshbase(main):072:0&gt; desc &#39;member&#39;Table member is ENABLEDmemberCOLUMN FAMILIES DESCRIPTION&#123;NAME &#x3D;&gt; &#39;address&#39;, VERSIONS &#x3D;&gt; &#39;1&#39;, EVICT_BLOCKS_ON_CLOSE &#x3D;&gt; &#39;false&#39;, NEW_VERSION_BEHAVIOR &#x3D;&gt; &#39;false&#39;, KEEP_DELETED_CELLS &#x3D;&gt; &#39;FALSE&#39;, CACHE_DATA_ON_WRITE &#x3D;&gt; &#39;false&#39;, DATA_BLOCK_ENCODING &#x3D;&gt; &#39;NONE&#39;, TTL &#x3D;&gt; &#39;FOREVER&#39;, MIN_VERSIONS &#x3D;&gt; &#39;0&#39;, REPLICATION_SCOPE &#x3D;&gt; &#39;0&#39;, BLOOMFILTER &#x3D;&gt; &#39;ROW&#39;, CACHE_INDEX_ON_WRITE &#x3D;&gt; &#39;false&#39;, IN_MEMORY &#x3D;&gt; &#39;false&#39;, CACHE_BLOOMS_ON_WRITE &#x3D;&gt; &#39;false&#39;, PREFETCH_BLOCKS_ON_OPEN &#x3D;&gt; &#39;false&#39;, COMPRESSION &#x3D;&gt; &#39;NONE&#39;, BLOCKCACHE &#x3D;&gt; &#39;true&#39;, BLOCKSIZE &#x3D;&gt; &#39;65536&#39;&#125;&#123;NAME &#x3D;&gt; &#39;info&#39;, VERSIONS &#x3D;&gt; &#39;1&#39;, EVICT_BLOCKS_ON_CLOSE &#x3D;&gt; &#39;false&#39;, NEW_VERSION_BEHAVIOR &#x3D;&gt; &#39;false&#39;, KEEP_DELETED_CELLS &#x3D;&gt; &#39;FALSE&#39;, CACHE_DATA_ON_WRITE &#x3D;&gt; &#39;false&#39;, DATA_BLOCK_ENCODING &#x3D;&gt; &#39;NONE&#39;, TTL &#x3D;&gt; &#39;FOREVER&#39;, MIN_VERSIONS &#x3D;&gt; &#39;0&#39;, REPLICATION_SCOPE &#x3D;&gt; &#39;0&#39;, BLOOMFILTER &#x3D;&gt; &#39;ROW&#39;, CACHE_INDEX_ON_WRITE &#x3D;&gt; &#39;false&#39;, IN_MEMORY &#x3D;&gt; &#39;false&#39;, CACHE_BLOOMS_ON_WRITE &#x3D;&gt; &#39;false&#39;, PREFETCH_BLOCKS_ON_OPEN &#x3D;&gt; &#39;false&#39;, COMPRESSION &#x3D;&gt; &#39;NONE&#39;, BLOCKCACHE &#x3D;&gt; &#39;true&#39;, BLOCKSIZE &#x3D;&gt; &#39;65536&#39;&#125;2 row(s)QUOTAS0 row(s)Took 0.0503 seconds 删除表 disable tb drop tb1234567891011121314151617181920212223242526272829303132333435hbase(main):078:0* disable &#39;member&#39;Took 0.7507 secondshbase(main):079:0&gt;hbase(main):080:0* desc &#39;member&#39;Table member is DISABLEDmemberCOLUMN FAMILIES DESCRIPTION&#123;NAME &#x3D;&gt; &#39;address&#39;, VERSIONS &#x3D;&gt; &#39;1&#39;, EVICT_BLOCKS_ON_CLOSE &#x3D;&gt; &#39;false&#39;, NEW_VERSION_BEHAVIOR &#x3D;&gt; &#39;false&#39;, KEEP_DELETED_CELLS &#x3D;&gt; &#39;FALSE&#39;, CACHE_DATA_ON_WRITE &#x3D;&gt; &#39;false&#39;, DATA_BLOCK_ENCODING &#x3D;&gt; &#39;NONE&#39;, TTL &#x3D;&gt; &#39;FOREVER&#39;, MIN_VERSIONS &#x3D;&gt; &#39;0&#39;, REPLICATION_SCOPE &#x3D;&gt; &#39;0&#39;, BLOOMFILTER &#x3D;&gt; &#39;ROW&#39;, CACHE_INDEX_ON_WRITE &#x3D;&gt; &#39;false&#39;, IN_MEMORY &#x3D;&gt; &#39;false&#39;, CACHE_BLOOMS_ON_WRITE &#x3D;&gt; &#39;false&#39;, PREFETCH_BLOCKS_ON_OPEN &#x3D;&gt; &#39;false&#39;, COMPRESSION &#x3D;&gt; &#39;NONE&#39;, BLOCKCACHE &#x3D;&gt; &#39;true&#39;, BLOCKSIZE &#x3D;&gt; &#39;65536&#39;&#125;&#123;NAME &#x3D;&gt; &#39;info&#39;, VERSIONS &#x3D;&gt; &#39;1&#39;, EVICT_BLOCKS_ON_CLOSE &#x3D;&gt; &#39;false&#39;, NEW_VERSION_BEHAVIOR &#x3D;&gt; &#39;false&#39;, KEEP_DELETED_CELLS &#x3D;&gt; &#39;FALSE&#39;, CACHE_DATA_ON_WRITE &#x3D;&gt; &#39;false&#39;, DATA_BLOCK_ENCODING &#x3D;&gt; &#39;NONE&#39;, TTL &#x3D;&gt; &#39;FOREVER&#39;, MIN_VERSIONS &#x3D;&gt; &#39;0&#39;, REPLICATION_SCOPE &#x3D;&gt; &#39;0&#39;, BLOOMFILTER &#x3D;&gt; &#39;ROW&#39;, CACHE_INDEX_ON_WRITE &#x3D;&gt; &#39;false&#39;, IN_MEMORY &#x3D;&gt; &#39;false&#39;, CACHE_BLOOMS_ON_WRITE &#x3D;&gt; &#39;false&#39;, PREFETCH_BLOCKS_ON_OPEN &#x3D;&gt; &#39;false&#39;, COMPRESSION &#x3D;&gt; &#39;NONE&#39;, BLOCKCACHE &#x3D;&gt; &#39;true&#39;, BLOCKSIZE &#x3D;&gt; &#39;65536&#39;&#125;2 row(s)QUOTAS0 row(s)Took 0.0326 secondshbase(main):081:0&gt; drop &#39;member&#39;Took 0.2391 secondshbase(main):082:0&gt; desc &#39;member&#39;ERROR: Table member does not exist.For usage try &#39;help &quot;describe&quot;&#39;Took 0.0035 secondshbase(main):083:0&gt; listTABLEuser1 row(s)Took 0.0030 seconds&#x3D;&gt; [&quot;user&quot;]","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HBASE","slug":"HBASE","permalink":"http://yoursite.com/tags/HBASE/"}]},{"title":"安装配置HBASE","slug":"安装配置HBASE","date":"2020-06-10T13:27:13.000Z","updated":"2020-06-10T13:27:13.824Z","comments":true,"path":"2020/06/10/安装配置HBASE/","link":"","permalink":"http://yoursite.com/2020/06/10/%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AEHBASE/","excerpt":"","text":"安装配置HBASE一、安装 安装包1wget https://mirrors.huaweicloud.com/apache/hbase/2.2.2/hbase-2.2.2-bin.tar.gz 解压配置.bash_profile1234567tar -zxvf hbase-2.2.2-bin.tar.gz -C ..&#x2F;vim ~&#x2F;.bash_profileexport HBASH_HOME&#x3D;..export $PATH&#x3D;$HBASH_HOME&#x2F;bin:$PATHsource ~&#x2F;.bash_profile 二、配置 vim $HBASE_HOME/conf/hbase_env.sh 1234export JAVA_HOME&#x3D;...export HBASE_MANAGES_ZK&#x3D;false &#x2F;&#x2F;不使用HBASE中的zk vim $HBASE_HOME/conf/hbase_site.xml 1234567891011121314&lt;property&gt; &lt;name&gt;hbase.rootdir&lt;&#x2F;name&gt; &lt;value&gt;hdfs:&#x2F;&#x2F;localhost:9000&#x2F;hbase&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;&#x2F;name&gt; &lt;value&gt;true&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;&#x2F;name&gt; &lt;value&gt;localhost:2181&lt;&#x2F;value&gt;&lt;&#x2F;property&gt; 三、启动 先启动HDFS, ZK start-hbase.sh jps12HMasterHRegionServer","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HBASE","slug":"HBASE","permalink":"http://yoursite.com/tags/HBASE/"}]},{"title":"网点排列组合计算距离","slug":"网点排列组合计算距离","date":"2020-06-08T10:32:02.000Z","updated":"2020-06-08T10:32:02.313Z","comments":true,"path":"2020/06/08/网点排列组合计算距离/","link":"","permalink":"http://yoursite.com/2020/06/08/%E7%BD%91%E7%82%B9%E6%8E%92%E5%88%97%E7%BB%84%E5%90%88%E8%AE%A1%E7%AE%97%E8%B7%9D%E7%A6%BB/","excerpt":"","text":"网点排列组合计算距离 根据oss_id, lon, lat三个字段来计算每两个网点之间的空间距离 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148package OssDistanceimport org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;import org.apache.spark.sql.functions.colimport org.apache.spark.sql.types.&#123;DoubleType, IntegerType&#125;import scala.collection.mutable.ListBufferimport Math.&#123;PI, pow, cos, sin, asin, sqrt&#125;object OssDistance &#123; def main(args: Array[String]): Unit = &#123; if (args.length &lt; 2) &#123; System.err.println(\"Usage OssDistance &lt;ossLimitNum&gt; &lt;savePath&gt;\") System.exit(1) &#125; val Array(ossLimitNum, savePath) = args val spark = SparkSession.builder().getOrCreate() val ossDF = _getOssDF(spark, ossLimitNum.toInt) val (ossIds, ossLocation) = _getOssData(ossDF) val combinations = _getOssIdsCombinations(ossIds) val combinationsRDD = spark.sparkContext.parallelize(combinations) val ossLocationBroadcast = spark.sparkContext.broadcast(ossLocation) val ossDistance = combinationsRDD.map&#123; ossCombination =&gt; &#123; val ossAId = ossCombination._1 val ossBId = ossCombination._2 val ossLocationData = ossLocationBroadcast.value val ossALon = ossLocationData(ossAId)(\"lon\") val ossALat = ossLocationData(ossAId)(\"lat\") val ossBLon = ossLocationData(ossBId)(\"lon\") val ossBLat = ossLocationData(ossBId)(\"lat\") val distance = _computeDistance(ossALon, ossALat, ossBLon, ossBLat) ossDistanceR(ossAId, ossBId, distance) &#125; &#125; val ossDistanceDF = spark.createDataFrame(ossDistance) ossDistanceDF.write.format(\"json\").save(savePath) ossDistanceDF.show(10) &#125; /** * @Desc 网点数据 * @Date 2:23 下午 2020/6/8 * @Param [spark] * @Return DataSet **/ def _getOssDF(spark: SparkSession, ossLimitNum:Int): DataFrame = &#123; spark.read.format(\"jdbc\") .option(\"url\", \"jdbc:mysql://\"+Tools.getConfig(\"host\")+\"/dw_2_basic\") .option(\"driver\", \"com.mysql.jdbc.Driver\") .option(\"dbtable\", Tools.getConfig(\"defaultDb\")) .option(\"user\", Tools.getConfig(\"USER\")) .option(\"password\", Tools.getConfig(\"password\")) .load().select( col(\"oms网点ID\").cast(IntegerType).as(\"oss_id\"), col(\"经度\").cast(DoubleType).as(\"lon\"), col(\"纬度\").cast(DoubleType).as(\"lat\") ).limit(ossLimitNum) &#125; /** * @Desc 根据网点DF 输出网点ID列表和网点位置Map * @Date 2:22 下午 2020/6/8 * @Param [ossDF] * @Return (ListBuffer[Int], Map[Int, Map[String, Double]]) **/ def _getOssData(ossDF:DataFrame): (ListBuffer[Int], Map[Int, Map[String, Double]]) = &#123; val ossList = ossDF.collectAsList() val ossCount = ossDF.count().toInt var ossLocation: Map[Int, Map[String, Double]] = Map() val ossIds: ListBuffer[Int] = ListBuffer() for (i &lt;- 0 to ossCount-1) &#123; val ossRow = ossList.get(i) val ossId:Int = ossRow(0).asInstanceOf[Int] val lon:Double = ossRow(1).asInstanceOf[Double] val lat:Double = ossRow(2).asInstanceOf[Double] ossLocation += (ossId -&gt; Map(\"lon\" -&gt; lon, \"lat\" -&gt; lat)) ossIds.append(ossId) &#125; (ossIds, ossLocation) &#125; /** * @Desc 网点ID排列组合 * @Date 2:34 下午 2020/6/8 * @Param [ossIds] * @Return scala.collection.mutable.ListBuffer&lt;scala.Tuple2&lt;java.lang.Object,java.lang.Object&gt;&gt; **/ def _getOssIdsCombinations(ossIds: ListBuffer[Int]): ListBuffer[(Int, Int)] = &#123; val ossIdsLength = ossIds.length val combinations: ListBuffer[(Int, Int)] = ListBuffer() for (i &lt;- 0 to ossIdsLength - 1) &#123; val ossId = ossIds(i) val leftOss = ossIds.slice(i+1, ossIdsLength) for (id &lt;- leftOss) &#123; combinations.append((ossId, id)) &#125; &#125; combinations &#125; /** * @Desc 网点ID之间的空间距离 * @Date 2:39 下午 2020/6/8 * @Param [lon1, lat1, lon2, lat2] * @Return double **/ def _computeDistance(lonA:Double, latA:Double, lonB:Double, latB:Double): Double = &#123; val Array(lon1, lat1, lon2, lat2) = Array(lonA, latA, lonB, latB).map(_ * PI / 180) val dLon = lon2 - lon1 val dLat = lat2 - lat1 val a = pow(sin(dLat/2), 2) + cos(lat1) * cos(lat2) * pow(sin(dLon/2), 2) val c = 2 * asin(sqrt(a)) val r = 6371 c * r &#125; case class ossDistanceR(oss1: Int, oss2: Int, distance: Double)&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkStreaming消费Kakfa输出日志","slug":"SparkStreaming消费Kakfa输出日志","date":"2020-06-04T08:49:12.000Z","updated":"2020-06-04T08:49:12.265Z","comments":true,"path":"2020/06/04/SparkStreaming消费Kakfa输出日志/","link":"","permalink":"http://yoursite.com/2020/06/04/SparkStreaming%E6%B6%88%E8%B4%B9Kakfa%E8%BE%93%E5%87%BA%E6%97%A5%E5%BF%97/","excerpt":"","text":"SparkStreaming消费Kakfa输出日志SparkStreaming代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273package workuseimport kafka.serializer.StringDecoderimport org.apache.spark.SparkConfimport org.apache.spark.streaming.kafka.KafkaUtilsimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object StreamingConsumeKafka &#123; def main(args: Array[String]): Unit = &#123; if (args.length &lt; 3) &#123; System.err.println(\"Usage: StreamingConsumeKafka &lt;bootstrap.servers&gt; &lt;topics&gt; &lt;filterType&gt;\") System.exit(1) &#125; val Array(bootstrapServers, topicsStr, filterType) = args val topics:Set[String] = topicsStr.split(\",\").toSet val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(\"StreamingConsumeKafka\") val ssc = new StreamingContext(sparkConf, Seconds(60)) ssc.sparkContext.setLogLevel(\"WARN\") val kafkaParams:Map[String, String] = Map(\"bootstrap.servers\"-&gt;bootstrapServers) val streamData = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topics) val resData = streamData.map(_._2).map&#123; logStr =&gt; &#123; val logData = logStr.split(\"\\t\") val ip = logData(0) val time = logData(1) val path = logData(2) val reqMethod = path.split(\" \")(0) val pagePath = path.split(\" \")(1) val coursePage = pagePath.split(\"/\")(2) val courseId = coursePage.substring(0, coursePage.indexOf(\".\")).toInt val statusCode = logData(3).toInt val queryPath = logData(4) val searchEngine = queryPath.split('.')(1) val keyword = queryPath.split(\"=\")(1) WebVisitLog(ip, time, reqMethod, path, pagePath, courseId, statusCode, queryPath, searchEngine, keyword) &#125; &#125;.filter&#123; visitLog =&gt; &#123; val pagePath = visitLog.pagePath pagePath.split(\"/\")(1) == filterType &#125; &#125; resData.print() ssc.start() ssc.awaitTermination() &#125; case class WebVisitLog( ip: String, time: String, reqMethod: String, path: String, pagePath: String, courseId: Int, statusCode: Int, queryPath: String, searchEngine: String, keyWord: String )&#125; 依赖jar包 spark-streaming-kafka-0-8_2.11-2.4.5.jar metrics-core-2.2.0.jar kafka_2.11-0.8.2.1.jar 可以添加到${SPARK_HOME}/jars submit命令1spark-submit --class workuse.StreamingConsumeKafka --name StreamingConsumeKafka --master \"local[2]\" --jars ~/jars/spark-streaming-kafka-0-8_2.11-2.4.5.jar ~/wk_jars/workuse-1.0.jar localhost:9092 flume_log class","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"sparkStreaming","slug":"sparkStreaming","permalink":"http://yoursite.com/tags/sparkStreaming/"}]},{"title":"Python+Flume+Kafka 日志生成采集","slug":"Python-Flume-Kafka-日志生成采集","date":"2020-06-03T07:28:28.000Z","updated":"2020-06-03T07:28:28.513Z","comments":true,"path":"2020/06/03/Python-Flume-Kafka-日志生成采集/","link":"","permalink":"http://yoursite.com/2020/06/03/Python-Flume-Kafka-%E6%97%A5%E5%BF%97%E7%94%9F%E6%88%90%E9%87%87%E9%9B%86/","excerpt":"","text":"Python+Flume+Kafka 日志生成采集生成日志123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869#!/usr/bin/env python# coding=utf-8import loggingimport osimport randomimport datetimelog_path = os.getcwd()ip_list = [23, 12, 103, 88, 120, 48, 22, 101, 96, 130, 104, 74, 53, 13, 41]path_list = ['/class/12.html', '/class/123.html', '/class/142.html', '/class/90.html', '/class/241.html', '/class/88.html', '/learn/123.html', '/learn/140.html', '/buy/131.html', '/buy/103.html']status_code = [200, 404, 500, 502, 303]source_list = [\"https://www.baidu.com/s?wd=\", \"https://www.sogou.com/web?query=\", \"https://cn.bing.com/search?q=\"]keywords = [\"kafka\", \"spark\", \"flink\", \"flume\", \"hadoop\"]class Logger: def __init__(self, loggername): self.logger = logging.getLogger(loggername) self.logger.setLevel(logging.INFO) log_path = os.getcwd() logname = log_path + \"/access.log\" fh = logging.FileHandler(logname, encoding=\"utf-8\") fh.setLevel(logging.INFO) ch = logging.StreamHandler() ch.setLevel(logging.INFO) #formatter = logging.Formatter('&#123;ip&#125;\\t%(asctime)s\\t&#123;path&#125;'.format(ip=\"192.168.214.59\", path=\"GET /class/192.html HTTP/1.1\")) formatter = logging.Formatter('%(message)s') fh.setFormatter(formatter) #ch.setFormatter(formatter) self.logger.addHandler(fh) #self.logger.addHandler(ch) def get_log(self): return self.logger def add_log(self, log_num=100): message = \"&#123;ip&#125;\\t&#123;log_time&#125;\\t&#123;path&#125;\\t&#123;code&#125;\\t&#123;source&#125;\" for i in range(100): ips = random.sample(ip_list, 4) ip = \".\".join([str(n) for n in ips]) log_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") path = \"GET &#123;p&#125; HTTP/1.1\".format(p=random.sample(path_list, 2)[0]) code = random.sample(status_code, 1)[0] source = random.sample(source_list, 1)[0] + random.sample(keywords, 1)[0] msg = message.format(ip=ip, log_time=log_time, path=path, code=code, source=source) self.logger.info(msg)if __name__ == \"__main__\": Logger(\"lg\").add_log() crontab定时执行 Flume收集日志到Kafka远程通过ip访问，配置修改server.propertiesadvertised.listeners=PLAINTEXT://IP:9092 exec-memory-kafka.conf123456789101112131415exec-kafka-agent.sources &#x3D; exec-kafka-sourceexec-kafka-agent.sinks &#x3D; exec-kafka-sinkexec-kafka-agent.channels &#x3D; exec-kafka-channelexec-kafka-agent.sources.exec-kafka-source.type &#x3D; execexec-kafka-agent.sources.exec-kafka-source.command &#x3D; tail -F &#x2F;home&#x2F;kowhoy&#x2F;code&#x2F;log&#x2F;access.logexec-kafka-agent.sinks.exec-kafka-sink.type &#x3D; org.apache.flume.sink.kafka.KafkaSinkexec-kafka-agent.sinks.exec-kafka-sink.kafka.bootstrap.servers &#x3D; localhost:9092exec-kafka-agent.sinks.exec-kafka-sink.kafka.topic &#x3D; flume_logexec-kafka-agent.channels.exec-kafka-channel.type &#x3D; memoryexec-kafka-agent.sources.exec-kafka-source.channels &#x3D; exec-kafka-channelexec-kafka-agent.sinks.exec-kafka-sink.channel &#x3D; exec-kafka-channel 后台运行1flume-ng agent --conf $FLUME_HOME/conf --name exec-kafka-agent --conf-file exec-memory-kafka.conf -Dflume.root.logger=INFO,console &amp; 消费1kafka-console-consumer.sh --bootstrap-server ecs01:9092 --topic flume_log --from-beginning","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"http://yoursite.com/tags/Flume/"},{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"Kafka","slug":"Kafka","permalink":"http://yoursite.com/tags/Kafka/"}]},{"title":"SparkStreaming整合Kafka","slug":"SparkStreaming整合Kafka","date":"2020-06-03T02:39:01.000Z","updated":"2020-07-08T02:22:38.346Z","comments":true,"path":"2020/06/03/SparkStreaming整合Kafka/","link":"","permalink":"http://yoursite.com/2020/06/03/SparkStreaming%E6%95%B4%E5%90%88Kafka/","excerpt":"","text":"SparkStreaming整合Kafka Spark2.3版本之前可以支持broker0-8和0-10两个streaming-kafka版本，2.3版本之后不再支持0-8 0-8版本可以使用 Receviers-based 和 Direct 两种方式接收Kafka数据 0-10版本使用Kafka010操作 spark-streaming-kafka-0-8 可以使用 Receviers-based 和 Direct 两种方式 Receivers-based是基于receivers和高级API进行接收数据 特点 这种方式，使用接收器，接收器一直处于等待接收的状态，就会浪费资源； 在默认配置情况下，如果发生故障可能会丢失数据，不过可以通过SparkStreaming中启动write Ahead Logs，将kafka的数据写入到日志中(如hdfs)，发生故障时候可以通过日志恢复 GAV123groupId &#x3D; org.apache.spark artifactId &#x3D; spark-streaming-kafka-0-8_2.11 version &#x3D; 2.2.2 方法1234import org.apache.spark.streaming.kafka._ val kafkaStream &#x3D; KafkaUtils.createStream(streamingContext, [ZK quorum], [consumer group id], [per-topic number of Kafka partitions to consume]) 示例123456789101112131415161718192021import org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import org.apache.spark.streaming.kafka.KafkaUtilsobject KafkaReceiverApp &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(\"KafkaReceiver\") val ssc = new StreamingContext(sparkConf, Seconds(5)) ssc.sparkContext.setLogLevel(\"ERROR\") val topicMap = Map[String, Int](\"kafkaspark\"-&gt;1) val streamData = KafkaUtils.createStream(ssc, \"localhost:2181\", \"groupId\", topicMap) streamData.print() ssc.start() ssc.awaitTermination() &#125;&#125; 注意 Kafka中的topic分区和SparkStreaming中生成的RDD的分区不相关，增加KafkaUtils.createStream()中特定topic分区的数量只会增加在单个接收器中使用topic的线程数量，不会在处理数据的时候增加spark的并行性 可以使用不同的组和topic来创建不同的recevier来接收数据 如果要开启write ahead logs，使用KafkaUtils.createStream(…, StorageLevel.MEMORY_AND_DISK_SER) Direct Approach无接收器，定期查询的方式 特点 确保了更强的端到端。这种方式不是使用接收器来接收数据，是定期向kafka查询每个主题+分区的最新变化量 简化了并行性：不需要创建多个输入Kafka流并合并，使用directStream， SparkStreaming 将创建与要使用Kafka分区一样多的RDD分区，所以这些分区是可以并行化读取的 效率更好，第一种方式如果要确保故障数据可恢复，就得开启write ahead logs，这种方式就会数据复制过来还得再存一遍日志。direct方式，只要数据停留在kafka里面就好了 GAV123groupId &#x3D; org.apache.sparkartifactId &#x3D; spark-streaming-kafka-0-8_2.12version &#x3D; 2.4.5 方法12345import org.apache.spark.streaming.kafka._ val directKafkaStream &#x3D; KafkaUtils.createDirectStream[ [key class], [value class], [key decoder class], [value decoder class] ]( streamingContext, [map of Kafka parameters], [set of topics to consume]) 在kafka参数中，必须指定metadata.broker.list 或者 bootstrap.servers 默认情况下，会总最新的偏移量进行消耗，如果要从开始位置消费，需要将auto.offset.reset设置为smallest 示例123456789101112131415161718192021import kafka.serializer.StringDecoderimport org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import org.apache.spark.streaming.kafka.KafkaUtilsobject KafkaReceiverApp &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(\"KafkaReceiver\") val ssc = new StreamingContext(sparkConf, Seconds(5)) ssc.sparkContext.setLogLevel(\"ERROR\") val streamData = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, Map(\"metadata.broker.list\"-&gt;\"localhost:9092\", \"auto.offset.reset\"-&gt;\"smallest\"), Set(\"kafkaspark\")) streamData.print() ssc.start() ssc.awaitTermination() &#125;&#125; spark-streaming-kafka-0-10 只支持Direct模式 GAV123groupId &#x3D; org.apache.sparkartifactId &#x3D; spark-streaming-kafka-0-10_2.12 # 2.12 scala的版本version &#x3D; $&#123;spark-version&#125; ⚠️注意： 不需要再手动添加 org.apache.kafka组建的相关依赖了，比如：kafka-client, 因为spark-straming-kafka-0-10已经和相关的依赖了 创建Direct Stream1234567891011121314151617181920212223import org.apache.kafka.clients.consumer.ConsumerRecordimport org.apache.kafka.common.serialization.StringDeserializerimport org.apache.spark.streaming.kafka010._import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistentimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribeval kafkaParams = Map[String, Object]( \"bootstrap.servers\" -&gt; \"localhost:9092,anotherhost:9092\", \"key.deserializer\" -&gt; classOf[StringDeserializer], // key的反序列化 \"value.deserializer\" -&gt; classOf[StringDeserializer], // value的反序列化 \"group.id\" -&gt; \"use_a_separate_group_id_for_each_stream\", \"auto.offset.reset\" -&gt; \"latest\", // offset设置 [latest, none, earilest] \"enable.auto.commit\" -&gt; (false: java.lang.Boolean) // 是否自动提交，如果有stream会产生操作的话，可以不使用自动提交offset，而是处理完数据之后，手动提交offset 防止数据重复消费)val topics = Array(\"topicA\", \"topicB\")val stream = KafkaUtils.createDirectStream[String, String]( streamingContext, PreferConsistent, //分区策略 [PreferConsistent, PreferBrokers, PreferFixed] Subscribe[String, String](topics, kafkaParams) //消费策略)stream.map(record =&gt; (record.key, record.value)) 获取到offset1234567stream.foreachRDD &#123; rdd =&gt; val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges rdd.foreachPartition &#123; iter =&gt; val o: OffsetRange = offsetRanges(TaskContext.get.partitionId) println(s\"$&#123;o.topic&#125; $&#123;o.partition&#125; $&#123;o.fromOffset&#125; $&#123;o.untilOffset&#125;\") &#125;&#125; 保存offset 有三种方式 CheckPoints (不推荐使用) Kafka自身 外部存储 在kafka-0-8中，可以使用zookeeper里管理offset 目录结构是/consumers/&lt;group.id&gt;/offsets/ &lt;topic&gt;/&lt;partitionId&gt; 但是由于 zookeeper 的写入能力并不会随着 zookeeper 节点数量的增加而扩大，因而，当存在频繁的 Offset 更新时，ZOOKEEPER 集群本身可能成为瓶颈。因而，不推荐采用这种方式。 在0-10中，kafka自身的一个特殊 Topic（__consumer_offsets）中：这种方式支持大吞吐量的Offset 更新，又不需要手动编写 Offset 管理程序或者维护一套额外的集群，因而是合适的实现方式。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/tags/kafka/"},{"name":"sparkStreaming","slug":"sparkStreaming","permalink":"http://yoursite.com/tags/sparkStreaming/"}]},{"title":"SparkStreaming整合Flume ","slug":"SparkStreaming整合Flume","date":"2020-06-01T08:31:17.000Z","updated":"2020-06-01T08:31:17.492Z","comments":true,"path":"2020/06/01/SparkStreaming整合Flume/","link":"","permalink":"http://yoursite.com/2020/06/01/SparkStreaming%E6%95%B4%E5%90%88Flume/","excerpt":"","text":"SparkStreaming整合Flume 可以通过Push 和 Pull两种方式整合Push: 是Flume Push到Spark, 缺点是只能一个executor进行处理，造成压力Pull: 是Spark主动去Pull推荐使用Pull方式 Push方式，先执行Spark,再执行Flume Pull方式，先执行Flume,再执行Spark Push方式Flume.confnetcat-memory-avro.conf 12345678910111213141516netcat-avro-agent.sources &#x3D; netcat-avro-sourcenetcat-avro-agent.sinks &#x3D; netcat-avro-sinknetcat-avro-agent.channels &#x3D; netcat-avro-channelnetcat-avro-agent.sources.netcat-avro-source.type &#x3D; netcatnetcat-avro-agent.sources.netcat-avro-source.bind &#x3D; localhostnetcat-avro-agent.sources.netcat-avro-source.port &#x3D; 44444netcat-avro-agent.sinks.netcat-avro-sink.type &#x3D; avronetcat-avro-agent.sinks.netcat-avro-sink.hostname &#x3D; localhostnetcat-avro-agent.sinks.netcat-avro-sink.port &#x3D; 55555netcat-avro-agent.channels.netcat-avro-channel.type &#x3D; memorynetcat-avro-agent.sources.netcat-avro-source.channels &#x3D; netcat-avro-channelnetcat-avro-agent.sinks.netcat-avro-sink.channel &#x3D; netcat-avro-channel GAV123groupId &#x3D; org.apache.sparkartifactId &#x3D; spark-streaming-flume_2.12version &#x3D; 2.4.5 FlumePushStreaming.scala1234567891011121314151617181920212223package sk_demoimport org.apache.spark.SparkConfimport org.apache.spark.streaming.flume.FlumeUtilsimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object FlumePushStreaming &#123; def main(args: Array[String]): Unit &#x3D; &#123; val sparkConf &#x3D; new SparkConf().setAppName(&quot;FlumePushStreaming&quot;).setMaster(&quot;local[2]&quot;) val ssc &#x3D; new StreamingContext(sparkConf, Seconds(5)) ssc.sparkContext.setLogLevel(&quot;WARN&quot;) val streamData &#x3D; FlumeUtils.createStream(ssc, &quot;localhost&quot;, 55555) val res &#x3D; streamData.flatMap(x &#x3D;&gt; new String(x.event.getBody.array()).toString.split(&quot; &quot;)).map((_, 1)).reduceByKey(_+_) res.print() ssc.start() ssc.awaitTermination() &#125;&#125; Pull方式Flume.confnetcat-memory-spark.conf 12345678910111213141516netcat-spark-agent.sources &#x3D; netcat-spark-sourcenetcat-spark-agent.sinks &#x3D; netcat-spark-sinknetcat-spark-agent.channels &#x3D; netcat-spark-channelnetcat-spark-agent.sources.netcat-spark-source.type &#x3D; netcatnetcat-spark-agent.sources.netcat-spark-source.bind &#x3D; localhostnetcat-spark-agent.sources.netcat-spark-source.port &#x3D; 44444netcat-spark-agent.sinks.netcat-spark-sink.type &#x3D; org.apache.spark.streaming.flume.sink.SparkSinknetcat-spark-agent.sinks.netcat-spark-sink.hostname &#x3D; localhostnetcat-spark-agent.sinks.netcat-spark-sink.port &#x3D; 55555netcat-spark-agent.channels.netcat-spark-channel.type &#x3D; memorynetcat-spark-agent.sources.netcat-spark-source.channels &#x3D; netcat-spark-channelnetcat-spark-agent.sinks.netcat-spark-sink.channel &#x3D; netcat-spark-channel GAV1234567groupId &#x3D; org.apache.sparkartifactId &#x3D; spark-streaming-flume-sink_2.12version &#x3D; 2.4.5groupId &#x3D; org.apache.commonsartifactId &#x3D; commons-lang3version &#x3D; 3.5 FlumePullStreaming.scala123456789101112131415161718192021222324package sk_demoimport org.apache.spark.SparkConfimport org.apache.spark.streaming.flume.FlumeUtilsimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object FlumePullStreaming &#123; def main(args: Array[String]): Unit &#x3D; &#123; val sparkConf &#x3D; new SparkConf().setAppName(&quot;FlumePullStreaming&quot;).setMaster(&quot;local[2]&quot;) val ssc &#x3D; new StreamingContext(sparkConf, Seconds(5)) ssc.sparkContext.setLogLevel(&quot;WARN&quot;) val streamData &#x3D; FlumeUtils.createPollingStream(ssc, &quot;localhost&quot;, 55555) val res &#x3D; streamData.flatMap(x &#x3D;&gt; new String(x.event.getBody.array()).toString.split(&quot; &quot;)).map((_, 1)).reduceByKey(_+_) res.print() ssc.start() ssc.awaitTermination() &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"sparkStreaming","slug":"sparkStreaming","permalink":"http://yoursite.com/tags/sparkStreaming/"}]},{"title":"WEB日志采集","slug":"WEB日志采集","date":"2020-05-29T13:11:42.000Z","updated":"2020-05-29T13:11:42.045Z","comments":true,"path":"2020/05/29/WEB日志采集/","link":"","permalink":"http://yoursite.com/2020/05/29/WEB%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86/","excerpt":"","text":"WEB日志采集一、浏览器日志采集 页面浏览日志 解决PV(Page View)、UV(Unique Visitors) 页面流量、页面来源的问题 页面交互日志 用户兴趣点、体验优化点 二、页面浏览日志1. 页面HTTP请求过程浏览器发送请求 –1⃣️–&gt; 服务器接受请求,返回HTML –2⃣️–&gt; 浏览器接受响应、解析渲染 过程1⃣️中包含: - 请求行 (请求方法/URL/HTTP版本号) - 请求报头 (附加信息，如Cookie) - 请求正文 过程2⃣️中包含: - 状态行 (200/ 404/..) - 响应报头 （Cookie..） - 响应正文 2. 页面浏览日志的采集过程浏览器发送请求 —&gt;服务器接受请求,返回HTML + 内部处理(HTML嵌入日志采集代码) —&gt;浏览器接受响应、解析渲染 —&gt;执行日志采集代码向日志服务器发送日志请求(立即返回请求成功响应，避免影响正常加载) —&gt;日志服务器解析/预处理/存储 3. 页面交互日志的采集过程开发/生成 采集代码(绑定监测的交互行为) —&gt;当页面产生指定行为，触发采集代码 —&gt;采集代码采集 + 发送转储 4. 页面日志清洗/预处理 识别流量攻击、网络爬虫、流量作弊等，非正常页面日志导致统计偏差大，\b校验识别出非正常并清洗 数据缺项补正，比如用户登录之后，补充登录之前的用户信息 无效数据剔除，配置错误/过时的日志得清除，避免资源(计算/存储)浪费 日志分离，敏感数据等进行细分隔离分发","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"日志采集","slug":"日志采集","permalink":"http://yoursite.com/tags/%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86/"}]},{"title":"FlumePushStream","slug":"FlumePushStream","date":"2020-05-28T09:43:12.000Z","updated":"2020-05-28T09:43:12.317Z","comments":true,"path":"2020/05/28/FlumePushStream/","link":"","permalink":"http://yoursite.com/2020/05/28/FlumePushStream/","excerpt":"","text":"FlumePushStreamFlumeStreamingApp.scala1234567891011121314151617181920212223242526272829303132package sk_demoimport org.apache.spark.SparkConfimport org.apache.spark.streaming.flume.FlumeUtilsimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object FlumeStreamingApp &#123; def main(args:Array[String]): Unit = &#123; if (args.length &lt; 2) &#123; System.err.println(\"Usage: FlumeStreamingApp &lt;hostname&gt; &lt;port&gt;\") System.exit(1) &#125; val Array(hostname, port) = args val sparkConf = new SparkConf() val ssc = new StreamingContext(sparkConf, Seconds(5)) ssc.sparkContext.setLogLevel(\"WARN\") val flumeStream = FlumeUtils.createStream(ssc, hostname, port.toInt) val res = flumeStream.flatMap(x =&gt; new String(x.event.getBody().array())).split(\" \").map((_, 1)).reduceByKey(_+_) res.print() ssc.start() ssc.awaitTermination() &#125;&#125; flume-streaming.conf12345678910111213141516flume_streaming_agent.sources &#x3D; flume_streaming_sourceflume_streaming_agent.sinks &#x3D; flume_streaming_sinkflume_streaming_agent.channels &#x3D; flume_streaming_channelflume_streaming_agent.sources.flume_streaming_source.type &#x3D; netcatflume_streaming_agent.sources.flume_streaming_source.bind &#x3D; localhostflume_streaming_agent.sources.flume_streaming_source.port &#x3D; 44444flume_streaming_agent.sinks.flume_streaming_sink.type &#x3D; avroflume_streaming_agent.sinks.flume_streaming_sink.hostname &#x3D; localhostflume_streaming_agent.sinks.flume_streaming_sink.port &#x3D; 55555flume_streaming_agent.channels.flume_streaming_channel.type &#x3D; memoryflume_streaming_agent.sources.flume_streaming_source.channels &#x3D; flume_streaming_channelflume_streaming_agent.sinks.flume_streaming_sink.channel &#x3D; flume_streaming_channel ./spark-submit --packages org.apache.spark:spark-streaming-flume_2.11:2.4.5 --class workuse.FlumeStreamingApp --name FlumeStreaming --master &quot;local[2]&quot; /home/kowhoy/jars/workuse.jar localhost 55555","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"FlumePushStream","slug":"FlumePushStream","permalink":"http://yoursite.com/tags/FlumePushStream/"}]},{"title":"Streaming2Sql","slug":"Streaming2Sql","date":"2020-05-28T07:35:59.000Z","updated":"2020-05-28T07:35:59.561Z","comments":true,"path":"2020/05/28/Streaming2Sql/","link":"","permalink":"http://yoursite.com/2020/05/28/Streaming2Sql/","excerpt":"","text":"Streaming2Sql12345678910111213141516171819202122232425262728293031323334353637package sk_demoimport java.sql.DriverManagerimport org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object Streaming2Sql &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(\"Streaming2Sql\") val ssc = new StreamingContext(sparkConf, Seconds(5)) val wordStream = ssc.socketTextStream(\"localhost\", 9999) val wordCount = wordStream.flatMap(_.split(\" \")).map((_, 1)).reduceByKey(_+_) wordCount.foreachRDD(rdd =&gt; rdd.foreachPartition(line =&gt; &#123; Class.forName(\"com.mysql.jdbc.Driver\") val conn = DriverManager.getConnection(\"jdbc:mysql://localhost:3306/test\", \"root\", \"wojiushiwo \") try &#123; for (row &lt;- line) &#123; val sql = \"insert into streaming_save (word, count, save_time) values ('\" + row._1 + \"','\" + row._2 +\"', now())\" conn.prepareStatement(sql).executeUpdate() &#125; &#125; finally &#123; conn.close() &#125; &#125;)) ssc.start() ssc.awaitTermination() &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Streaming2Sql","slug":"Streaming2Sql","permalink":"http://yoursite.com/tags/Streaming2Sql/"}]},{"title":"SparkStreaming Transform","slug":"SparkStreaming-Transform","date":"2020-05-28T05:37:01.000Z","updated":"2020-05-28T05:37:01.920Z","comments":true,"path":"2020/05/28/SparkStreaming-Transform/","link":"","permalink":"http://yoursite.com/2020/05/28/SparkStreaming-Transform/","excerpt":"","text":"SparkStreaming Transform 黑名单过滤 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package sk_demoimport org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object FilterBlackListApp &#123; def main(args: Array[String]): Unit = &#123; if (args.length &lt; 3) &#123; System.err.println(\"Usage: FilterBlackListApp &lt;hostname&gt; &lt;port&gt; &lt;hdfsPath&gt;\") System.exit(1) &#125; val Array(hostname, port, hdfsPath) = args val sparkConf = new SparkConf() val ssc = new StreamingContext(sparkConf, Seconds(5)) ssc.sparkContext.setLogLevel(\"WARN\") val blackListRDD = ssc.sparkContext.textFile(hdfsPath).flatMap(_.split(\" \")).map((_, true)) val logStream = ssc.socketTextStream(hostname, port.toInt) val logInfo = logStream.map &#123; log =&gt; &#123; val logAction = log.split(\",\") (logAction(0), UserLog(logAction(0), logAction(1))) &#125; &#125; val filteredLog = logInfo.transform( logRDD =&gt; &#123; val joinedRDD = logRDD.leftOuterJoin(blackListRDD) val filteredRDD = joinedRDD.filter &#123; log =&gt; &#123; if (log._2._2.getOrElse(flase)) &#123; false &#125; else &#123; true &#125; &#125; &#125; filteredRDD.map(line =&gt; line._2._1) &#125; ) filteredLog.print() ssc.start() ssc.awaitTermination() &#125; case class UserLog(username:String, actionLog:String)&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"transform","slug":"transform","permalink":"http://yoursite.com/tags/transform/"}]},{"title":"mapWithState","slug":"mapWithState","date":"2020-05-27T10:14:14.000Z","updated":"2020-05-27T10:14:14.383Z","comments":true,"path":"2020/05/27/mapWithState/","link":"","permalink":"http://yoursite.com/2020/05/27/mapWithState/","excerpt":"","text":"mapWithState 只显示更新部分的累计 12345678910111213141516171819202122232425262728293031323334353637package sk_demoimport org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, State, StateSpec, StreamingContext&#125;object MapWithStateApp &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(\"MapWithStateApp\") val ssc = new StreamingContext(sparkConf, Seconds(2)) ssc.sparkContext.setLogLevel(\"WARN\") ssc.checkpoint(\"/tmp_data/tmp_checkpoint\") val socketLines = ssc.socketTextStream(\"localhost\", 9999) val batchStat = socketLines.flatMap(_.split(\" \")).map((_, 1)).reduceByKey(_+_) val mappingFunc = (word:String, one:Option[Int], state:State[Int]) =&gt; &#123; val sum = one.getOrElse(0) + state.getOption.getOrElse(0) val output = (word, sum) state.update(sum) output &#125; val result = batchStat.mapWithState(StateSpec.function(mappingFunc)) result.print() ssc.start() ssc.awaitTermination() &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"mapWithState","slug":"mapWithState","permalink":"http://yoursite.com/tags/mapWithState/"}]},{"title":"updateStateByKey","slug":"updateStateByKey","date":"2020-05-27T10:11:57.000Z","updated":"2020-05-27T10:11:58.002Z","comments":true,"path":"2020/05/27/updateStateByKey/","link":"","permalink":"http://yoursite.com/2020/05/27/updateStateByKey/","excerpt":"","text":"updateStateByKey 对于状态的累计更新 123456789101112131415161718192021222324252627282930313233343536373839package sk_demoimport org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object UpdateStateByKeyApp &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setAppName(\"UpdateStateByKeyApp\").setMaster(\"local[2]\") val ssc = new StreamingContext(sparkConf, Seconds(2)) ssc.checkpoint(\"/tmp_data/tmp_checkpoint\") val socketLines = ssc.socketTextStream(\"localhost\", 9999) val result = socketLines.flatMap(_.split(\" \")).map((_, 1)).updateStateByKey(updateStateFunc) result.print() ssc.start() ssc.awaitTermination() &#125; /** * @Param currValues: 当前key对应的所有值, preValue: 当前key的历史状态 * @Return Option[Int] */ def updateStateFunc(currValues:Seq[Int], preValue:Option[Int]): Option[Int] = &#123; var currValuesSum = 0 for (currVal &lt;- currValues) &#123; currValuesSum += currVal &#125; val allSum = preValue.getOrElse(0) + currValuesSum Option(allSum) &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"updateStateByKey","slug":"updateStateByKey","permalink":"http://yoursite.com/tags/updateStateByKey/"}]},{"title":"SparkCore 生产实例","slug":"SparkCore-生产实例","date":"2020-05-22T09:15:39.000Z","updated":"2020-05-22T09:15:39.839Z","comments":true,"path":"2020/05/22/SparkCore-生产实例/","link":"","permalink":"http://yoursite.com/2020/05/22/SparkCore-%E7%94%9F%E4%BA%A7%E5%AE%9E%E4%BE%8B/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258package workuseimport org.apache.spark.rdd.RDDimport org.apache.spark.sql.SparkSessionimport scala.collection.mutable.ListBufferobject core_work &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession.builder().appName(\"core_work\").getOrCreate() val sc = spark.sparkContext sc.setLogLevel(\"ERROR\") val logRDD = sc.textFile(\"/Users/zhouke/tmp_data/user_visit_action.txt\") // top10 商品, 根据点击、订单、支付 统计 点击数*0.3 + 下单数 * 0.4 + 支付数 * 0.4 val top10Sku = getTop10Sku(logRDD)// sc.parallelize(top10Sku).saveAsTextFile(\"hdfs://localhost:9000/demo/top10Res\")// val top10DF = spark.createDataFrame(top10Sku)//// top10DF.selectExpr(\"round(score, 2) as sc\", \"*\").drop(\"score\")// .write// .format(\"jdbc\")// .option(\"url\", \"jdbc:mysql://localhost:3306\")// .option(\"dbtable\", \"test.top10sku\")// .option(\"driver\", \"com.mysql.jdbc.Driver\")// .option(\"user\", \"root\")// .option(\"password\", \"abc \")// .save() // top10商品的 每个品类点击top10的session val top10session = getTop10Session(spark, logRDD) top10Sku.foreach(println) top10session.foreach(println) &#125; /** * @Desc 热度前10的商品 * @Date 10:38 上午 2020/5/21 * @Param [logRDD] * @Return workuse.core_work.Top10Rs[] **/ def getTop10Sku(logRDD: RDD[String]): Array[Top10Rs] = &#123; val actionRDD = logRDD.map&#123; line =&gt; &#123; val log = line.split(\"_\") UserAction( log(0), log(1).toLong, log(2), log(3).toLong, log(4), log(5), log(6).toLong, log(7).toLong, log(8), log(9), log(10), log(11), log(12).toLong ) &#125; &#125; val infoRDD = actionRDD.flatMap &#123; action =&gt; &#123; if (action.click_category_id != -1) &#123; List(CategoryCountInfo(action.click_category_id+\"\", 1, 0, 0)) &#125; else if (action.order_category_ids != \"null\") &#123; val ids = action.order_category_ids.split(\",\") val actionList = ListBuffer[CategoryCountInfo]() for (id &lt;- ids) &#123; actionList += CategoryCountInfo(id, 0, 1, 0) &#125; actionList &#125; else if (action.pay_category_ids != \"null\") &#123; val ids = action.pay_category_ids.split(\",\") val actionList = ListBuffer[CategoryCountInfo]() for (id &lt;- ids) &#123; actionList += CategoryCountInfo(id, 0, 0, 1) &#125; actionList &#125;else &#123; Nil &#125; &#125; &#125; val categoryRDD = infoRDD.groupBy(_.categoryId) val categoryCombineRDD = categoryRDD.mapValues &#123; logs =&gt; &#123; logs.reduce &#123; (x, y) =&gt; &#123; CategoryCountInfo(x.categoryId, x.clickCount+y.clickCount, x.orderCount+y.orderCount, x.payCount+y.payCount) &#125; &#125; &#125; &#125; val top10Res = categoryCombineRDD.map&#123; sumLog =&gt; &#123; val lg = sumLog._2 Top10Rs(lg.categoryId, lg.clickCount, lg.orderCount, lg.payCount, lg.clickCount*0.3+lg.orderCount*0.4+lg.payCount*0.4) &#125; &#125;.sortBy(_.score, false).take(10) top10Res &#125; def getTop10Session(spark: SparkSession, logRDD: RDD[String]): RDD[(String, List[(String, Int)])] = &#123; val actionRDD = logRDD.map&#123; line =&gt; &#123; val log = line.split(\"_\") UserAction( log(0), log(1).toLong, log(2), log(3).toLong, log(4), log(5), log(6).toLong, log(7).toLong, log(8), log(9), log(10), log(11), log(12).toLong ) &#125; &#125; val countRDD = actionRDD.flatMap &#123; action =&gt; &#123; if (action.click_category_id != -1) &#123; List(CategoryCountInfo(action.click_category_id+\"\", 1, 0, 0)) &#125; else if (action.order_category_ids != \"null\") &#123; val ids = action.order_category_ids.split(\",\") val countList = ListBuffer[CategoryCountInfo]() for (id &lt;- ids) &#123; countList += CategoryCountInfo(id, 0, 1, 0) &#125; countList &#125; else if (action.pay_category_ids != \"null\") &#123; val ids = action.pay_category_ids.split(\",\") val countList = ListBuffer[CategoryCountInfo]() for (id &lt;- ids) &#123; countList += CategoryCountInfo(id, 0, 0, 1) &#125; countList &#125; else &#123; Nil &#125; &#125; &#125; val groupByCategoryRDD = countRDD.groupBy(_.categoryId) val categorySum = groupByCategoryRDD.mapValues&#123; logs =&gt; &#123; logs.reduce &#123; (x, y) =&gt; CategoryCountInfo(x.categoryId, x.clickCount+y.clickCount, x.orderCount+y.orderCount, x.payCount+y.payCount) &#125; &#125; &#125; val top10 = categorySum.map(_._2).sortBy&#123; line =&gt; &#123; line.clickCount * 0.3 + line.orderCount * 0.4 + line.payCount * 0.4 &#125; &#125;.take(10) val top10Ids = top10.map(_.categoryId) val broadcastIds = spark.sparkContext.broadcast(top10Ids) val top10Logs = actionRDD.filter &#123; line =&gt; &#123; if (line.click_category_id != -1) &#123; broadcastIds.value.contains(line.click_category_id.toString) &#125; else &#123; false &#125; &#125; &#125; val categoryAndSession = top10Logs.map&#123; action =&gt; &#123; (action.click_category_id+\"-\"+action.session_id, 1) &#125; &#125; val sumRDD = categoryAndSession.reduceByKey(_+_) val catRDD = sumRDD.map&#123; line =&gt; &#123; (line._1.split(\"-\")(0), (line._1.split(\"-\")(1), line._2)) &#125; &#125; val groupRDD = catRDD.groupByKey() val resRDD = groupRDD.mapValues &#123; datas =&gt; &#123; datas.toList.sortWith &#123; case(x, y) =&gt; &#123; x._2 &gt; y._2 &#125; &#125;.take(10) &#125; &#125; resRDD &#125; case class UserAction( date: String, user_id: Long, session_id: String, page_id: Long, action_time: String, search_keyword: String, click_category_id: Long, click_product_id: Long, order_category_ids: String, order_product_ids: String, pay_category_ids: String, pay_product_ids: String, city_id: Long ) case class CategoryCountInfo( categoryId: String, clickCount: Long, orderCount: Long, payCount: Long ) case class Top10Rs( categoryId:String, clickCount: Long, orderCount: Long, payCount: Long, score: Double )&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"SparkCore","slug":"SparkCore","permalink":"http://yoursite.com/tags/SparkCore/"}]},{"title":"flume跨机器传输","slug":"flume跨机器传输","date":"2020-05-22T02:52:52.000Z","updated":"2020-05-22T02:52:52.678Z","comments":true,"path":"2020/05/22/flume跨机器传输/","link":"","permalink":"http://yoursite.com/2020/05/22/flume%E8%B7%A8%E6%9C%BA%E5%99%A8%E4%BC%A0%E8%BE%93/","excerpt":"","text":"Flume跨机器传输 ecs00:公网ip: 102.102.102.102内网ip: 172.100.100.100 ecs01:公网ip: 103.103.103.103内网ip: 172.100.100.101 目标ecs00的44444端口的netcat ==&gt; ecs01的logger ecs00-netcat-memory-avro.conf12345678910111213141516netcat-avro-agent.sources &#x3D; netcat-avro-sourcenetcat-avro-agent.sinks &#x3D; netcat-avro-sinknetcat-avro-agent.channels &#x3D; netcat-avro-channelnetcat-avro-agent.sources.netcat-avro-source.type &#x3D; netcatnetcat-avro-agent.sources.netcat-avro-source.bind &#x3D; localhostnetcat-avro-agent.sources.netcat-avro-source.port &#x3D; 44444netcat-avro-agent.sinks.netcat-avro-sink.type &#x3D; avronetcat-avro-agent.sinks.netcat-avro-sink.hostname &#x3D; 103.103.103.103netcat-avro-agent.sinks.netcat-avro-sink.port &#x3D; 44444netcat-avro-agent.channels.netcat-avro-channel.type &#x3D; memorynetcat-avro-agent.sources.netcat-avro-source.channels &#x3D; netcat-avro-channelnetcat-avro-agent.sinks.netcat-avro-sink.channel &#x3D; netcat-avro-channel ecs01-avro-memory-logger.conf1234567891011121314avro-logger-agent.sources &#x3D; avro-logger-sourceavro-logger-agent.sinks &#x3D; avro-logger-sinkavro-logger-agent.channels &#x3D; avro-logger-channelavro-logger-agent.sources.avro-logger-source.type &#x3D; avroavro-logger-agent.sources.avro-logger-source.bind &#x3D; 172.100.100.101avro-logger-agent.sources.avro-logger-source.port &#x3D; 44444avro-logger-agent.sinks.avro-logger-sink.type &#x3D; loggeravro-logger-agent.channels.avro-logger-channel.type &#x3D; memoryavro-logger-agent.sources.avro-logger-source.channels &#x3D; avro-logger-channelavro-logger-agent.sinks.avro-logger-sink.channel &#x3D; avro-logger-channel 启动: ecs01 上 1flume-ng agent --conf $&#123;FLUME_HOME&#125;/conf --name avro-logger-agent --conf-file ./avro-memory-logger.conf -Dflume.root.logger=INFO,console ecs00上 1flume-ng agent --conf $&#123;FLUME_HOME&#125;/conf --name netcat-avro-agent --conf-file ./netcat-memory-avro.conf -Dflume.root.logger=INFO,console ecs00上 1telnet localhost 44444","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"http://yoursite.com/tags/Flume/"}]},{"title":"socket/file/queue_streaming","slug":"socket-file-queue-streaming","date":"2020-05-20T08:13:34.000Z","updated":"2020-05-20T08:13:35.007Z","comments":true,"path":"2020/05/20/socket-file-queue-streaming/","link":"","permalink":"http://yoursite.com/2020/05/20/socket-file-queue-streaming/","excerpt":"","text":"socket_streaming1234567891011121314151617181920212223242526package workuseimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import org.apache.spark.SparkConfobject socket_streaming &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setAppName(\"socket_streaming\") val ssc = new StreamingContext(sparkConf, Seconds(4)) val lineStream = ssc.socketTextStream(\"localhost\", 9999) val wordStreams = lineStream.flatMap(_.split(\" \")) val wordAndOneStreams = wordStreams.map((_, 1)) val wordAndCountStreams = wordAndOneStreams.reduceByKey(_+_) wordAndCountStreams.print() ssc.start() ssc.awaitTermination() &#125;&#125; file_streaming1234567891011121314151617181920212223242526package workuseimport org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object file_streaming &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setAppName(\"fileStream\") val ssc = new StreamingContext(sparkConf, Seconds(4)) val dirStreams = ssc.textFileStream(\"hdfs://localhost:9000/fileStream\") val wordStreams = dirStreams.flatMap(_.split(\",\")) val wordAndOneStreams = wordStreams.map((_, 1)) val wordAndCountStreams = wordAndOneStreams.reduceByKey(_+_) wordAndCountStreams.print() ssc.start() ssc.awaitTermination() &#125;&#125; queue_streaming123456789101112131415161718192021222324252627282930313233package workuseimport org.apache.spark.SparkConfimport org.apache.spark.rdd.RDDimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import scala.collection.mutableobject rdd_queue_streaming &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setAppName(\"RDD_queue\") val ssc = new StreamingContext(sparkConf, Seconds(4)) val rddQueue = new mutable.Queue[RDD[Int]]() val inputStream = ssc.queueStream(rddQueue, oneAtATime= false) val mappedStream = inputStream.map((_, 1)) val reduceStream = mappedStream.reduceByKey(_+_) reduceStream.print() ssc.start() for (i &lt;- 1 to 5) &#123; rddQueue += ssc.sparkContext.makeRDD(1 to 300, 10) Thread.sleep(2000) &#125; &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"sparkStreaming","slug":"sparkStreaming","permalink":"http://yoursite.com/tags/sparkStreaming/"}]},{"title":"spark_core图谱","slug":"spark-core图谱","date":"2020-05-18T06:34:48.000Z","updated":"2020-05-18T06:34:48.266Z","comments":true,"path":"2020/05/18/spark-core图谱/","link":"","permalink":"http://yoursite.com/2020/05/18/spark-core%E5%9B%BE%E8%B0%B1/","excerpt":"","text":"","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark_transformation","slug":"spark-transformation","date":"2020-05-15T07:21:22.000Z","updated":"2020-05-15T07:21:22.568Z","comments":true,"path":"2020/05/15/spark-transformation/","link":"","permalink":"http://yoursite.com/2020/05/15/spark-transformation/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590package sk_demoimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object transformations &#123; val _sc_conf = new SparkConf() .setAppName(\"spark_transformation\") .setMaster(\"local\") val sc = new SparkContext(_sc_conf) def main(args: Array[String]): Unit = &#123; /** * Return a new distributed dataset formed by passing each element of the source through a function func. **/ map() /** * Similar to map, but runs separately on each partition (block) of the RDD, * so func must be of type Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt; when running on an RDD of type T. * */ mapPartitions() /** * Similar to mapPartitions, * but also provides func with an integer value representing the index of the partition, * so func must be of type (Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt; when running on an RDD of type T. * */ mapPartitionsWithIndex() /** * Similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item). * */ flatMap() /** * Return an RDD created by coalescing all elements within each partition into an array * */ glom() /** * Return an RDD of grouped items. Each group consists of a key and a sequence of elements * */ groupBy() /** * Return a new dataset formed by selecting those elements of the source on which func returns true. * */ filter() /** * Sample a fraction fraction of the data, with or without replacement, using a given random number generator seed. * */ sample() /** * Return a new dataset that contains the distinct elements of the source dataset. * */ distinct() /** * Decrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently after filtering down a large dataset. * */ coalesce() /** * Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them. This always shuffles all data over the network. * */ repartition() /** * Return this RDD sorted by the given key function. * */ sortBy() /** * Return an RDD created by piping elements to a forked external process * */ pipe() /** * Return the union of this RDD and another one. Any identical elements will appear multiple * */ union() /** * Return an RDD with the elements from `this` that are not in `other`. * */ subtract() /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * */ intersection() /** * Return the Cartesian product of this RDD and another one * */ cartesian() /** * Zips this RDD with another one, returning key-value pairs with the first element in each RDD * */ zip() /** * Return a copy of the RDD partitioned using the specified partitioner. * */ partitionBy() /** * Merge the values for each key using an associative and commutative reduce function. * */ reduceByKey() /** * Group the values for each key in the RDD into a single sequence. * */ groupByKey() /** * Aggregate the values of each key, using given combine functions and a neutral \"zero value\". * */ aggregateByKey() /** * Merge the values for each key using an associative function and a neutral \"zero value\" * */ foldByKey() /** * Sort the RDD by key, so that each partition contains a sorted range of the elements. * */ sortByKey() /** * Pass each value in the key-value pair RDD through a map function without changing the keys * */ mapValues() /** * Return an RDD containing all pairs of elements with matching keys in `this` and `other`. * */ join() /** * For each key k in `this` or `other`, return a resulting RDD that contains a tuple with the * list of values for that key in `this` as well as `other`. * */ cogroup() &#125; /** * @Desc RDD中的每个元素进行 func 操作, 返回一个新的RDD * @Date 10:57 上午 2020/5/15 * @Param [] * @Return void **/ def map(): Unit = &#123; val rdd = sc.parallelize(1 to 10) val res = rdd.map(_+2) res.foreach(println) &#125; /** * @Desc 将一个分区上的所有元素，使用一次函数，而不是map的每个元素都使用一次元素 * @append 效率要比map好，但是要注意数据量大而导致的oom * @Date 11:25 上午 2020/5/15 * @Param [] * @Return void **/ def mapPartitions(): Unit = &#123; val rdd = sc.parallelize(1 to 10) val rdd2 = rdd.mapPartitions(mapPartitionsFunc) rdd2.foreach(println) &#125; /*** * @Desc * @Date 11:31 上午 2020/5/15 * @Param [iter] * @Return scala.collection.Iterator&lt;java.lang.Object&gt; **/ def mapPartitionsFunc(iter:Iterator[Int]) :Iterator[Int]= &#123; var res = List[Int]() while (iter.hasNext) &#123; val iterVal = iter.next() res :+= (iterVal + 2) &#125; res.iterator &#125; /*** * @Desc 类似mapPartitions, 多了分区的index * @Date 11:39 上午 2020/5/15 * @Param [] * @Return void **/ def mapPartitionsWithIndex(): Unit = &#123; val rdd = sc.parallelize(1 to 10, 2) val res = rdd.mapPartitionsWithIndex(mapPartitionsWithIndexFunc) res.foreach(println) &#125; /*** * @Desc * @Date 11:39 上午 2020/5/15 * @Param [idx, iter] * @Return scala.collection.Iterator&lt;scala.Tuple2&lt;java.lang.Object,java.lang.Object&gt;&gt; **/ def mapPartitionsWithIndexFunc(idx:Int, iter:Iterator[Int]): Iterator[(Int, Int)] = &#123; var res = List[(Int, Int)]() while (iter.hasNext) &#123; val iterVal = iter.next() res :+= (idx, iterVal+2) &#125; res.iterator &#125; /*** * @Desc 扁平化的操作，所以输出的是一个序列，不是单个的值 * @Date 11:46 上午 2020/5/15 * @Param [] * @Return void **/ def flatMap(): Unit = &#123; val rdd = sc.parallelize(Array((\"A\", 1), (\"B\", 2), (\"C\", 2))) val mapRes = rdd.map(x =&gt; x._1 + \"-&gt;\" + x._2) val flatMapRes = rdd.flatMap(x =&gt; x._1 + \"-&gt;\" + x._2) mapRes.foreach(println) flatMapRes.foreach(println) &#125; /** * @Desc 按分区转化为数组类型的RDD * @Date 1:36 下午 2020/5/15 * @Param [] * @Return void **/ def glom(): Unit = &#123; val rdd = sc.parallelize(Array(1, 2, 3, 4, 1, 2, 3, 0), 2) val res = rdd.glom().map(_.max) res.foreach(println) &#125; /** * @Desc 根据每个元素的计算进行分组 * @Date 1:45 下午 2020/5/15 * @Param [] * @Return void **/ def groupBy(): Unit = &#123; val rdd = sc.parallelize(1 to 10) val res = rdd.groupBy(_%2) res.foreach(println) &#125; /** * @Desc 返回条件为true的 * @Date 1:49 下午 2020/5/15 * @Param [] * @Return void **/ def filter(): Unit = &#123; val rdd = sc.parallelize(1 to 10) val res = rdd.filter(_%2 == 0) res.foreach(println) &#125; /** * @Desc 取样 (是否重复取样) * @param fraction expected size of the sample as a fraction of this RDD's size * without replacement: probability that each element is chosen; fraction must be [0, 1] * with replacement: expected number of times each element is chosen; fraction must be greater * than or equal to 0 * @Date 1:51 下午 2020/5/15 * @Param [] * @Return void **/ def sample(): Unit = &#123; val rdd = sc.parallelize(1 to 10) val res = rdd.sample(false, 0.5) res.foreach(println) &#125; /** * @Desc 去重后的RDD 参数 numPartitions * @Date 2:02 下午 2020/5/15 * @Param [] * @Return void **/ def distinct(): Unit = &#123; val rdd = sc.parallelize(Array(1, 2, 3, 1, 2, 4, 5)) val res = rdd.distinct(2) res.foreach(println) &#125; /** * @Desc 降低分区数 * @Date 2:06 下午 2020/5/15 * @Param [] * @Return void **/ def coalesce(): Unit = &#123; val rdd = sc.parallelize(1 to 16, 4) val res = rdd.coalesce(3) println(res.partitions.size) &#125; /** * @Desc 重新分区,随机排列，分区之间保持平衡 * @Date 2:09 下午 2020/5/15 * @Param [] * @Return void **/ def repartition(): Unit = &#123; val rdd = sc.parallelize(1 to 16, 4) val res = rdd.repartition(2) println(res.partitions.size) &#125; /** * @Desc 根据给定key排序 * @Date 2:12 下午 2020/5/15 * @Param [] * @Return void **/ def sortBy(): Unit = &#123; val rdd = sc.parallelize(Array((\"A\", 1), (\"C\", 3), (\"B\", 4), (\"D\", 2))) val res = rdd.sortBy(_._1) res.foreach(println) &#125; /** * @Desc 在管道中执行shell/perl脚本 * @Date 2:15 下午 2020/5/15 * @Param [] * @Return void **/ def pipe(): Unit = &#123; val rdd = sc.parallelize(Array(\"A\", \"B\", \"C\")) val res = rdd.pipe(\"shell_path.sh\") res.foreach(println) &#125; /** * @Desc 连接RDD * @Date 2:18 下午 2020/5/15 * @Param [] * @Return void **/ def union(): Unit = &#123; val rdd1 = sc.parallelize(1 to 6) val rdd2 = sc.parallelize(5 to 10) val res = rdd1.union(rdd2) res.foreach(println) &#125; /** * @Desc 将rdd1中也在rdd2中的元素去掉 * @Date 2:22 下午 2020/5/15 * @Param [] * @Return void **/ def subtract(): Unit = &#123; val rdd1 = sc.parallelize(1 to 6) val rdd2 = sc.parallelize(5 to 10) val res = rdd1.subtract(rdd2) res.foreach(println) &#125; /** * @Desc 交集 * @Date 2:25 下午 2020/5/15 * @Param [] * @Return void **/ def intersection(): Unit = &#123; val rdd1 = sc.parallelize(1 to 6) val rdd2 = sc.parallelize(5 to 10) val res = rdd1.intersection(rdd2) res.foreach(println) &#125; /** * @Desc 笛卡尔积 * @Date 2:27 下午 2020/5/15 * @Param [] * @Return void **/ def cartesian(): Unit = &#123; val rdd1 = sc.parallelize(1 to 6) val rdd2 = sc.parallelize(5 to 10) val res = rdd1.cartesian(rdd2) res.foreach(println) &#125; /** * @Desc k v组合 * @Date 2:32 下午 2020/5/15 * @Param [] * @Return void **/ def zip(): Unit = &#123; val rdd1 = sc.parallelize(Array(\"A\", \"B\", \"C\")) val rdd2 = sc.parallelize(Array(1, 2, 3)) val res = rdd1.zip(rdd2) res.foreach(println) &#125; /** * @Desc 根据partitioner 进行分区 * @Date 2:38 下午 2020/5/15 * @Param [] * @Return void **/ def partitionBy(): Unit = &#123; val rdd = sc.parallelize(Array((\"A\", 1), (\"B\", 1), (\"C\", 2), (\"D\",3))) val res = rdd.partitionBy(new org.apache.spark.HashPartitioner(2)) res.foreach(println) &#125; /** * @Desc * @Date 2:41 下午 2020/5/15 * @Param [] * @Return void **/ def reduceByKey(): Unit = &#123; val rdd = sc.parallelize(Array((\"A\", 1), (\"B\", 1), (\"A\", 2), (\"D\", 3))) val res = rdd.reduceByKey((x, y) =&gt; x + y) res.foreach(println) &#125; /** * @Desc * @Date 2:45 下午 2020/5/15 * @Param [] * @Return void **/ def groupByKey(): Unit = &#123; val rdd = sc.parallelize(Array((\"A\", 1), (\"B\", 1), (\"A\", 2), (\"D\", 3))) val res = rdd.groupByKey() res.foreach(println) &#125; /** * @Desc * @Date 2:56 下午 2020/5/15 * @Param [] * @Return void **/ def aggregateByKey(): Unit = &#123; val rdd = sc.parallelize(Array(1, 2, 3, 4, 5, 6, 7, 8), 2) val res1 = rdd.aggregate(0)(math.max(_, _), _+_) val res2 = rdd.aggregate(5)(math.max(_, _), _+_) println(res1, res2) &#125; /** * @Desc * @Date 3:01 下午 2020/5/15 * @Param [] * @Return void **/ def foldByKey(): Unit = &#123; val rdd = sc.parallelize(List((1, 2), (2, 3), (1, 3), (2, 5)), 2) val res = rdd.foldByKey(0)(_+_) res.foreach(println) &#125; def combineByKey(): Unit = &#123; &#125; /** * @Desc * @Date 3:09 下午 2020/5/15 * @Param [] * @Return void **/ def sortByKey(): Unit = &#123; val rdd = sc.parallelize(Array((\"A\", 1), (\"C\", 2), (\"B\", 3))) val res = rdd.sortByKey(true) res.foreach(println) &#125; /** * @Desc v map * @Date 3:11 下午 2020/5/15 * @Param [] * @Return void **/ def mapValues():Unit = &#123; val rdd = sc.parallelize(Array((\"A\", 1), (\"C\", 2), (\"B\", 3))) val res = rdd.mapValues(_+2) res.foreach(println) &#125; /** * @Desc join * @Date 3:13 下午 2020/5/15 * @Param [] * @Return void **/ def join(): Unit = &#123; val rdd1 = sc.parallelize(Array((1,\"a\"),(2,\"b\"),(3,\"c\"))) val rdd2 = sc.parallelize(Array((1,4),(2,5),(3,6))) val res = rdd1.join(rdd2) res.foreach(println) &#125; /** * @Desc * @Date 3:16 下午 2020/5/15 * @Param [] * @Return void **/ def cogroup(): Unit = &#123; val rdd = sc.parallelize(Array((1,\"a\"),(2,\"b\"),(3,\"c\"))) val rdd1 = sc.parallelize(Array((1,4),(2,5),(3,6))) val res = rdd.cogroup(rdd1) res.foreach(println) &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"},{"name":"transformation","slug":"transformation","permalink":"http://yoursite.com/tags/transformation/"}]},{"title":"datax的基本使用","slug":"datax的基本使用","date":"2020-05-12T07:39:02.000Z","updated":"2020-05-12T07:39:02.922Z","comments":true,"path":"2020/05/12/datax的基本使用/","link":"","permalink":"http://yoursite.com/2020/05/12/datax%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","excerpt":"","text":"datax的基本使用 datax是一种中心化处理形式，分为reader和writer连接到datax，然后进行转储操作 mysql2mysql123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&#123; \"job\": &#123; \"content\": [ &#123; \"reader\": &#123; \"name\":\"mysqlreader\", \"parameter\": &#123; \"column\":[ \"id\", \"name\" ], \"connection\":[ &#123; \"jdbcUrl\":[ \"jdbc:mysql://127.0.0.1:3306/test\" ], \"table\":[ \"d1\" ] &#125; ], \"password\":\"root\", \"username\":\"root\" &#125; &#125;, \"writer\": &#123; \"name\":\"mysqlwriter\", \"parameter\":&#123; \"column\":[ \"id\", \"name\" ], \"connection\":[ &#123; \"jdbcUrl\":\"jdbc:mysql://127.0.0.1:3306/test\", \"table\":[\"d2\"] &#125; ], \"password\":\"root\", \"username\":\"root\" &#125; &#125; &#125; ], \"setting\":&#123; \"speed\":&#123; \"channel\":\"1\" &#125; &#125; &#125;&#125; file2hive1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&#123; \"job\":&#123; \"content\": [ &#123; \"reader\": &#123; \"name\": \"txtfilereader\", \"parameter\": &#123; \"path\": [\"/home/kowhoy/tmp/10_days_orders_2.csv\"], \"encoding\": \"UTF-8\", \"column\": [ &#123; \"index\":0, \"type\": \"long\", &#125;, &#123; \"index\": 1, \"type\": \"long\" &#125;, &#123; \"index\": 2, \"type\": \"string\" &#125;, &#123; \"index\": 3, \"type\": \"long\" &#125;, &#123;\"index\": 4, \"type\": \"long\"&#125;, &#123;\"index\": 5, \"type\": \"long\"&#125;, &#123;\"index\": 6, \"type\": \"long\"&#125;, &#123;\"index\": 7, \"type\": \"long\"&#125;, &#123;\"index\": 8, \"type\": \"long\"&#125;, &#123;\"index\": 9, \"type\": \"long\"&#125;, &#123;\"index\": 10, \"type\": \"long\"&#125;, &#123;\"index\": 11, \"type\": \"long\"&#125;, &#123;\"index\": 12, \"type\": \"long\"&#125;, &#123;\"index\": 13, \"type\": \"long\"&#125;, &#123;\"index\": 14, \"type\": \"long\"&#125;, &#123;\"index\": 15, \"type\": \"long\"&#125;, &#123;\"index\": 16, \"type\": \"long\"&#125;, &#123;\"index\": 17, \"type\": \"long\"&#125;, &#123;\"index\": 18, \"type\": \"long\"&#125;, &#123;\"index\": 19, \"type\": \"string\"&#125;, &#123;\"index\": 20, \"type\": \"double\"&#125;, &#123;\"index\": 21, \"type\": \"double\"&#125;, &#123;\"index\": 22, \"type\": \"double\"&#125;, &#123;\"index\": 23, \"type\": \"double\"&#125;, &#123;\"index\": 24, \"type\": \"long\"&#125;, &#123;\"index\": 25, \"type\": \"long\"&#125;, &#123;\"index\": 26, \"type\": \"long\"&#125;, &#123;\"index\": 27, \"type\": \"double\"&#125;, &#123;\"index\": 28, \"type\": \"double\"&#125;, &#123;\"index\": 29, \"type\": \"long\"&#125;, &#123;\"index\": 30, \"type\": \"long\"&#125;, &#123;\"index\": 31, \"type\": \"long\"&#125;, &#123;\"index\": 32, \"type\": \"long\"&#125;, &#123;\"index\": 33, \"type\": \"long\"&#125;, &#123;\"index\": 34, \"type\": \"long\"&#125;, &#123;\"index\": 35, \"type\": \"long\"&#125;, &#123;\"index\": 36, \"type\": \"long\"&#125;, &#123;\"index\": 37, \"type\": \"double\"&#125;, &#123;\"index\": 38, \"type\": \"long\"&#125;, &#123;\"index\": 39, \"type\": \"long\"&#125;, &#123;\"index\": 40, \"type\": \"long\"&#125;, &#123;\"index\": 41, \"type\": \"string\"&#125;, &#123;\"index\": 42, \"type\": \"string\"&#125;, &#123;\"index\": 43, \"type\": \"long\"&#125;, &#123;\"index\": 44, \"type\": \"long\"&#125;, &#123;\"index\": 45, \"type\": \"string\"&#125; ], \"fieldDelimiter\": \",\" &#125; &#125;, \"writer\": &#123; \"name\": \"hdfswriter\", \"parameter\": &#123; \"defaultFS\": \"hdfs://ecs:9000\", \"fileType\": \"text\", \"path\": \"/user/hive/warehouse/orders.db/datax_file_tab\", \"fileName\": \"datax_file_tab\", \"column\": [ &#123;\"name\": \"order_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"channel_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"order_sn\", \"type\": \"STRING\"&#125;, &#123;\"name\": \"is_serv_order\", \"type\": \"INT\"&#125;, &#123;\"name\": \"is_personal\", \"type\": \"INT\"&#125;, &#123;\"name\": \"order_type\", \"type\": \"INT\"&#125;, &#123;\"name\": \"mode_type\", \"type\": \"INT\"&#125;, &#123;\"name\": \"order_src\", \"type\": \"INT\"&#125;, &#123;\"name\": \"ao_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"member_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"employee_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"branch_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"company_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"city_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"osp_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"oss_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"oss_ao_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"osg_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"staff_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"sku_code\", \"type\": \"STRING\"&#125;, &#123;\"name\": \"sku_base_price\", \"type\": \"FLOAT\"&#125;, &#123;\"name\": \"sku_litre_price\", \"type\": \"FLOAT\"&#125;, &#123;\"name\": \"sku_price\", \"type\": \"FLOAT\"&#125;, &#123;\"name\": \"sku_litre\", \"type\": \"FLOAT\"&#125;, &#123;\"name\": \"oil_amount\", \"type\": \"INT\"&#125;, &#123;\"name\": \"amount\", \"type\": \"INT\"&#125;, &#123;\"name\": \"status\", \"type\": \"INT\"&#125;, &#123;\"name\": \"longitude\", \"type\": \"FLOAT\"&#125;, &#123;\"name\": \"latitude\", \"type\": \"FLOAT\"&#125;, &#123;\"name\": \"up_order_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"payment_type\", \"type\": \"INT\"&#125;, &#123;\"name\": \"payment_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"payment_status\", \"type\": \"INT\"&#125;, &#123;\"name\": \"payment_amout\", \"type\": \"INT\"&#125;, &#123;\"name\": \"refund_type\", \"type\": \"INT\"&#125;, &#123;\"name\": \"refund_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"refund_status\", \"type\": \"INT\"&#125;, &#123;\"name\": \"refund_amount\", \"type\": \"FLOAT\"&#125;, &#123;\"name\": \"settlement_rate\", \"type\": \"INT\"&#125;, &#123;\"name\": \"settlement_status\", \"type\": \"INT\"&#125;, &#123;\"name\": \"truck_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"create_time\", \"type\": \"STRING\"&#125;, &#123;\"name\": \"update_time\", \"type\": \"STRING\"&#125;, &#123;\"name\": \"delete_flag\", \"type\": \"INT\"&#125;, &#123;\"name\": \"index_num\", \"type\": \"INT\"&#125;, &#123;\"name\": \"order_date\", \"type\": \"STRING\"&#125; ], \"writeMode\": \"append\", \"fieldDelimiter\": \",\" &#125; &#125; &#125; ], \"setting\": &#123; \"speed\": &#123; \"channel\": 1 &#125; &#125; &#125;&#125; sql2hive1234567891011121314151617181920212223242526272829303132333435363738394041424344&#123; \"job\":&#123; \"content\": [ &#123; \"reader\": &#123; \"name\": \"mysqlreader\", \"parameter\": &#123; \"username\": \"hive\", \"password\": \"wojiushiwo\", \"connection\": [ &#123; \"querySql\": [ \"select order_id, channel_id, order_sn, is_serv_order, is_personal, order_type from datax_sql_tab;\" ], \"jdbcUrl\": [ \"jdbc:mysql://127.0.0.1:3306/test\" ] &#125; ] &#125; &#125;, \"writer\": &#123; \"name\": \"hdfswriter\", \"parameter\": &#123; \"defaultFS\": \"hdfs://39.99.221.146:9000\", \"fileType\": \"text\", \"path\": \"/user/hive/warehouse/orders.db/datax_sql_tab\", \"fileName\": \"datax_sql_tab\", \"column\": [ &#123;\"name\": \"order_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"channel_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"order_sn\", \"type\": \"STRING\"&#125;, &#123;\"name\": \"is_serv_order\", \"type\": \"INT\"&#125;, &#123;\"name\": \"is_personal\", \"type\": \"INT\"&#125;, &#123;\"name\": \"order_type\", \"type\": \"INT\"&#125; ], \"writeMode\": \"append\", \"fieldDelimiter\": \",\" &#125; &#125; &#125; ], \"setting\": &#123; \"speed\": &#123; \"channel\": 1 &#125; &#125; &#125;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"datax","slug":"datax","permalink":"http://yoursite.com/tags/datax/"}]},{"title":"mysql_存储过程/触发器示例","slug":"mysql-存储过程-触发器示例","date":"2020-05-09T10:15:44.000Z","updated":"2020-05-09T10:15:44.250Z","comments":true,"path":"2020/05/09/mysql-存储过程-触发器示例/","link":"","permalink":"http://yoursite.com/2020/05/09/mysql-%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B-%E8%A7%A6%E5%8F%91%E5%99%A8%E7%A4%BA%E4%BE%8B/","excerpt":"","text":"存储过程1234567891011121314151617create procedure three2(in d double(5, 2),out v1 double(5,2),out v2 double(5,2),out v3 double(5,2),out v4 double(5,2))BEGINselect max(price_val) into v1 from price_log;select min(price_val) into v2 from price_log;select avg(price_val) into v3 from price_log;select d into v4;end;call three2(12, @v1, @v2, @v3, @v4);select @v1, @v2, @v3, @v4; 触发器123create trigger demo before update on sort_test for each rowset new.node &#x3D; 100;","categories":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"HQL执行流程","slug":"HQL执行流程","date":"2020-05-09T09:45:14.000Z","updated":"2020-05-09T09:45:14.944Z","comments":true,"path":"2020/05/09/HQL执行流程/","link":"","permalink":"http://yoursite.com/2020/05/09/HQL%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B/","excerpt":"","text":"","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"hive存储模型","slug":"hive存储模型","date":"2020-05-09T09:40:47.000Z","updated":"2020-05-09T09:40:47.342Z","comments":true,"path":"2020/05/09/hive存储模型/","link":"","permalink":"http://yoursite.com/2020/05/09/hive%E5%AD%98%E5%82%A8%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"hive存储模型一、内部表与外部表 内部表的一切都在hive上管理，删除了内部表，将丢失数据外部表hive上只是提供操作以及结构，数据分开存储，删除外部表，数据不会丢失，重新建表即可正常使用 1. 建立内部表123456789create table atm (id int,name string,address string,money float)row format delimitedfileds terminated by ','stored as textfile; 1load data local inpath &#39;file_path&#39; into table atm; 2. 建立外部表123456789create external table ex_atm(id int,name string,address string,money float)row format delimitedfields terminated by ','stored as textfile 1load data local inpath &#39;file_path&#39; into table ex_atm; 二、分区与分桶 分区表创建表时执行一个分区标识，传入数据的时候指定分区标识，没有高级到使用数据中的列作为分区标识，分区就是将数据按标识放到hdfs上不同文件夹下，在查询的时候可以减少数据量，而不是全表扫描分桶表可以方便抽样 1. 分区表的创建123456789create table atm (id int,address string,money float)partitoned by (name string)row format delimitedfields terminated by &#39;,&#39;stored as textfile; 1load data local inpath &#39;file_path&#39; into table atm partiton (name&#x3D;&quot;zh&quot;); 1select * from atm where name &#x3D; &#39;zh&#39;; 1show partitions atm; 2. 分桶表的创建1234567create table bucket_atm (id int, name string, address string, money float)clustered by (name) into 3 bucketsrow format delimitedfields terminated by &#39;,&#39;stored as textfile; 1insert overwrite table bucket_atm select * from atm; 1234567select id, name from table tablesample(bucket x out of y on column);--- x: 从第几个分桶开始抽取--- y: 每隔几个分桶抽取4分桶 x &#x3D; 1, y &#x3D; 2就抽取 1,3分桶","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"数据库的优化器","slug":"数据库的优化器","date":"2020-05-09T03:39:13.000Z","updated":"2020-05-09T03:39:13.957Z","comments":true,"path":"2020/05/09/数据库的优化器/","link":"","permalink":"http://yoursite.com/2020/05/09/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8/","excerpt":"","text":"数据库主要由三部分组成，分别是解析器、优化器和执行引擎。 RBORBO(Rule-Based Optimizer) 基于规则的优化器。是根据已经制定好的一些优化规则对关系表达式进行转换，最终生成一个最优的执行计划。它是一种经验式的优化方法，优化规则都是预先定义好的，只需要将SQL按照优化规则的顺序往上套就行，一旦满足某个规则则进行优化。这样的结果就是同样一条SQL，无论读取的表中的数据是怎样的，最后生成的执行计划都是一样的（优化规则都一样）。而且SQL的写法不同也很有可能影响最终的执行计划，从而影响SQL的性能（基于优化规则顺序执行）。所以说，虽然RBO是一个老司机，知道常见的套路，但是当路况不同时，也无法针对性的达到最佳的效果。 CBOCBO（Cost-Based Optimizer）基于代价的优化器。根据优化规则对关系表达式进行转换，生成多个执行计划，最后根据统计信息和代价模型计算每个执行计划的Cost。从中挑选Cost最小的执行计划作为最终的执行计划。从描述来看，CBO是优于RBO的，RBO只认规则，对数据不敏感，而在实际的过程中，数据的量级会严重影响同样SQL的性能。所以仅仅通过RBO生成的执行计划很有可能不是最优的。而CBO依赖于统计信息和代价模型，统计信息的准确与否、代价模型是否合理都会影响CBO选择最优计划。目前各大数据库和大数据计算引擎都已经在使用CBO了，比如Oracle、Hive、Spark、Flink等等。 动态CBO动态CBO，就是在执行计划生成的过程中动态优化的方式。随着大数据技术的飞速发展，静态的CBO已经无法满足我们SQL优化的需要了，静态的统计信息无法提供准确的参考，在执行计划的生成过程中动态统计才会得到最优的执行计划。","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"优化器","slug":"优化器","permalink":"http://yoursite.com/tags/%E4%BC%98%E5%8C%96%E5%99%A8/"}]},{"title":"python操作zookeeper","slug":"python操作zookeeper","date":"2020-05-07T06:45:17.000Z","updated":"2020-05-07T06:45:17.729Z","comments":true,"path":"2020/05/07/python操作zookeeper/","link":"","permalink":"http://yoursite.com/2020/05/07/python%E6%93%8D%E4%BD%9Czookeeper/","excerpt":"","text":"一、安装1pip install kazoo 二、连接Zookeeper123from kazoo.client import KazooClientzk = KazooClient(hosts='39.99.221.106') 默认端口为2181，注意防火墙/安全组 三、创建节点 path： 节点路径 value： 节点对应的值，注意值的类型是 bytes ephemeral： 若为 True 则创建一个临时节点，session 中断后自动删除该节点。默认 False sequence: 若为 True 则在你创建节点名后面增加10位数字（例如：你创建一个 testplatform/test 节点，实际创建的是 testplatform/test0000000003，这串数&gt;字是顺序递增的）。默认 False makepath： 若为 False 父节点不存在时抛 NoNodeError。若为 True 父节点不存在则创建父节点。默认 False 12345678from kazoo.client import KazooClientzk = KazooClient(hosts=\"39.99.221.106\")zk.start()zk.create(\"/testplatform/test\", b\"this is test\", makepath=True)zk.stop() 四、查看节点 get_children() 查看子节点, get() 查看值 12345678910from kazoo.client import KazooClientzk = KazooClient(hosts=\"39.99.221.106\")zk.start()node = zk.get_children(\"/testplatform\")value = zk.get(\"/testplatform/test\")zk.stop() 五、更改节点12345678910from kazoo.client import KazooClientzk = KazooClient(hosts=\"39.99.221.106\")zk.start()zk.set(\"/testplatform/test\", b\"testabc\")value = zk.get(\"/testplatform/test\")zk.stop() 六、删除节点1234567from kazoo.client import KazooClientzk = KazooClient(hosts=\"39.99.221.106\")zk.start()zk.delete(\"/testplatform/test\", recursive=False)zk.stop() 参数 recursive：若为 False，当需要删除的节点存在子节点，会抛异常 NotEmptyError 。若为True，则删除 此节点 以及 删除该节点的所有子节点 七、watches 实践12345678910from kazoo.client import KazooClientzk = KazooClient(hosts=\"39.99.221.106\")zk.start()def test(event): print(\"触发事件\")if __name__ == \"__main__\": zk.get(\"/testplatform/test\", watch=test)","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"},{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}]},{"title":"ZOOKEEPER基础","slug":"ZOOKEEPER基础","date":"2020-05-07T06:06:44.000Z","updated":"2020-05-07T06:22:21.418Z","comments":true,"path":"2020/05/07/ZOOKEEPER基础/","link":"","permalink":"http://yoursite.com/2020/05/07/ZOOKEEPER%E5%9F%BA%E7%A1%80/","excerpt":"","text":"ZOOKEEPER 监听器原理:（1）在Zookeeper的API操作中，创建main()主方法即主线程； （2）在main线程中创建Zookeeper客户端（zkClient），这时会创建两个线程： 线程connet负责网络通信连接，连接服务器； 线程Listener负责监听；（3）客户端通过connet线程连接服务器； 图中getChildren(&quot;/&quot; , true) ，&quot; / &quot;表示监听的是根目录，true表示监听，不监听用false（4）在Zookeeper的注册监听列表中将注册的监听事件添加到列表中，表示这个服务器中的/path，即根目录这个路径被客户端监听了； （5）一旦被监听的服务器根目录下，数据或路径发生改变，Zookeeper就会将这个消息发送给Listener线程； （6）Listener线程内部调用process方法，采取相应的措施，例如更新服务器列表等。 选举机制：1)半数机制:集群中半数以上机器存活，集群可用。所以 Zookeeper 适合安装奇数台 服务器。2)Zookeeper 虽然在配置文件中并没有指定 Master 和 Slave。但是，Zookeeper 工作时， 是有一个节点为 Leader，其他则为 Follower，Leader 是通过内部的选举机制临时产生的。 (1)服务器 1 启动，发起一次选举。服务器 1 投自己一票。此时服务器 1 票数一票，不够半数以上(3 票)，选举无法完成，服务器 1 状态保持为 LOOKING;(2)服务器 2 启动，再发起一次选举。服务器 1 和 2 分别投自己一票并交换选票信息: 此时服务器 1 发现服务器 2 的 ID 比自己目前投票推举的(服务器 1)大，更改选票为推举 服务器 2。此时服务器 1 票数 0 票，服务器 2 票数 2 票，没有半数以上结果，选举无法完成，服务器 1，2 状态保持 LOOKING(3)服务器 3 启动，发起一次选举。此时服务器 1 和 2 都会更改选票为服务器 3。此次投票结果:服务器 1 为 0 票，服务器 2 为 0 票，服务器 3 为 3 票。此时服务器 3 的票数已 经超过半数，服务器 3 当选 Leader。服务器 1，2 更改状态为 FOLLOWING，服务器 3 更改 状态为 LEADING;(4)服务器 4 启动，发起一次选举。此时服务器 1，2，3 已经不是 LOOKING 状态， 不会更改选票信息。交换选票信息结果:服务器 3 为 3 票，服务器 4 为 1 票。此时服务器 4 服从多数，更改选票信息为服务器 3，并更改状态为 FOLLOWING;(5)服务器 5 启动，同 4 一样当小弟。 集群最少需要机器数:3 ###写数据流程","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}]},{"title":"zookeeper安装部署","slug":"zookeeper安装部署","date":"2020-05-07T03:53:34.000Z","updated":"2020-05-07T03:53:34.490Z","comments":true,"path":"2020/05/07/zookeeper安装部署/","link":"","permalink":"http://yoursite.com/2020/05/07/zookeeper%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/","excerpt":"","text":"zookeeper安装部署一、本地模式安装部署1. 下载安装1wget https://mirrors.huaweicloud.com/apache/zookeeper/zookeeper-3.4.13/zookeeper-3.4.13.tar.gz 1tar -zxvf zookeeper-3.4.13.tar.gz -C ../ 12cd zookeeper-3.4.13mkdir zkData 12cd conf/cp zoo_sample.cfg zoo.cfg 12vim zoo.cfgdataDir=/home/kowhoy/software/zookeeper-3.4.13/zkData 2. 使用12345bin&#x2F;zkServer.sh start #启动serverbin&#x2F;zkServer.sh status #查看状态bin&#x2F;zkServer.sh stop #停止stopbin&#x2F;zkCli.sh #启动客户端 二、分布式安装部署1. 下载解压2. myid 创建zkData1234cd zkDatavim myid2 ##依次 3， 4 3. zoo.cfg1234567dataDir&#x3D;&#x2F;home&#x2F;kowhoy&#x2F;software&#x2F;zookeeper-3.4.13&#x2F;zkDataquorumListenOnAllIPs&#x3D;trueserver.2&#x3D;ecs01:2888:3888server.3&#x3D;ecs00:2888:3888server.4&#x3D;bcc00:2888:3888 1234567891011server.A&#x3D;B:C:DA 是一个数字，表示这个是第几号服务器;集群模式下配置一个文件 myid，这个文件在 dataDir 目录下，这个文件里面有一个数据 就是 A 的值，Zookeeper 启动时读取此文件，拿到里面的数据与 zoo.cfg 里面的配置信息比 较从而判断到底是哪个 server。B 是这个服务器的地址; C 是这个服务器 Follower 与集群中的 Leader 服务器交换信息的端口; D 是万一集群中的 Leader 服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。 4.同步到其他机器上5. 分别启动1./zkServer.sh start 6. 查看状态1./zkServer.sh status","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}]},{"title":"HIVE安装配置","slug":"HIVE安装配置","date":"2020-05-07T02:10:43.000Z","updated":"2020-05-07T02:10:43.794Z","comments":true,"path":"2020/05/07/HIVE安装配置/","link":"","permalink":"http://yoursite.com/2020/05/07/HIVE%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/","excerpt":"","text":"HIVE安装配置一、前置环境 JAVA HADOOP MYSQL 二、安装 下载解压 123wget https://mirrors.huaweicloud.com/apache/hive/hive-2.3.6/apache-hive-2.3.6-bin.tar.gztar -zxvf apache-hive-2.3.6-bin.tar.gz -C ../ 配置环境变量 vim ~/.bash_profile 12HIVE_HOME&#x3D;&#x2F;home&#x2F;kowhoy&#x2F;software&#x2F;apache-hive-2.3.6-binexport PATH&#x3D;$HIVE_HOME&#x2F;bin:$PATH 三、配置 拷贝配置文件 1234567cd $&#123;HIVE_HOME&#125;/confcp hive-env.sh.template hive-env.shcp hive-exec-log4j2.properties.template hive-exec-log4j2.propertiescp hive-log4j2.properties.template hive-log4j2.propertiescp hive-default.xml.template hive-site.xmlcp beeline-log4j2.properties.template beeline-log4j2.properties vim hive-env.sh 12export JAVA_HOME&#x3D;&#x2F;home&#x2F;kowhoy&#x2F;software&#x2F;jdk1.8.0_191export HADOOP_HOME&#x3D;&#x2F;home&#x2F;kowhoy&#x2F;software&#x2F;hadoop-2.8.5 vim hive-site.xml 添加 123456789&lt;property&gt;&lt;name&gt;system:java.io.tmpdir&lt;&#x2F;name&gt;&lt;value&gt;&#x2F;tmp&#x2F;hive&#x2F;java&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt;&lt;name&gt;system:user.name&lt;&#x2F;name&gt;&lt;value&gt;$&#123;user.name&#125;&lt;&#x2F;value&gt;&lt;&#x2F;property&gt; 修改12345678910111213141516171819&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionURL&lt;&#x2F;name&gt;&lt;value&gt;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;metastore?createDatabaseIfNotExist&#x3D;true&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;&#x2F;name&gt;&lt;value&gt;com.mysql.jdbc.Driver&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;&#x2F;name&gt;&lt;value&gt;hive&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;&#x2F;name&gt;&lt;value&gt;passwd&lt;&#x2F;value&gt;&lt;&#x2F;property&gt; 四、添加jar包1cp mysql-connector-java-5.1.48-bin.jar ../apache-hive-2.3.6-bin/lib/ 五、hdfs创建文件夹并设置权限123456hdfs dfs -mkdir &#x2F;hivehdfs dfs -mkdir &#x2F;hive&#x2F;tmphdfs dfs -mkdir &#x2F;hive&#x2F;loghdfs dfs -mkdir &#x2F;hive&#x2F;warehousehdfs dfs -chmod 777 &#x2F;hive&#x2F;tmp 六、异常处理 出现123Failed to open new session: java.lang.RuntimeException:org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User:xxx not allowed to impersonate anonymous vim ${HADOOP_HOME}/etc/hadoop/core-site.xml 123456789&lt;property&gt;&lt;name&gt;hadoop.proxyuser.xxx.hosts&lt;&#x2F;name&gt;&lt;value&gt;*&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt;&lt;name&gt;hadoop.proxyuser.xxx.groups&lt;&#x2F;name&gt;&lt;value&gt;*&lt;&#x2F;value&gt;&lt;&#x2F;property&gt; 报错信息中的xxx 与配置中的xxx一致 七、初始化数据库1$&#123;HIVE_HOME&#125;&#x2F;bin&#x2F;schematool --dbType mysql --initSchema 八、使用 hive-client 1hive beenline 开启 1$&#123;HIVE_HOME&#125;/bin/hiveserver2 &amp; 连接 1$&#123;HIVE_HOME&#125;/bin/beeline -u jdbc:hive2://localhost:10000 username passwd","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HIVE","slug":"HIVE","permalink":"http://yoursite.com/tags/HIVE/"}]},{"title":"Pandas脑图","slug":"Pandas脑图","date":"2020-04-26T10:02:45.000Z","updated":"2020-04-26T10:02:45.508Z","comments":true,"path":"2020/04/26/Pandas脑图/","link":"","permalink":"http://yoursite.com/2020/04/26/Pandas%E8%84%91%E5%9B%BE/","excerpt":"","text":"","categories":[{"name":"Pandas","slug":"Pandas","permalink":"http://yoursite.com/categories/Pandas/"},{"name":"Python","slug":"Pandas/Python","permalink":"http://yoursite.com/categories/Pandas/Python/"}],"tags":[{"name":"Pandas","slug":"Pandas","permalink":"http://yoursite.com/tags/Pandas/"}]},{"title":"Numpy脑图","slug":"Numpy脑图","date":"2020-04-26T10:01:45.000Z","updated":"2020-04-26T10:01:45.596Z","comments":true,"path":"2020/04/26/Numpy脑图/","link":"","permalink":"http://yoursite.com/2020/04/26/Numpy%E8%84%91%E5%9B%BE/","excerpt":"","text":"","categories":[{"name":"Numpy","slug":"Numpy","permalink":"http://yoursite.com/categories/Numpy/"},{"name":"Python","slug":"Numpy/Python","permalink":"http://yoursite.com/categories/Numpy/Python/"}],"tags":[{"name":"Numpy","slug":"Numpy","permalink":"http://yoursite.com/tags/Numpy/"}]},{"title":"matplotlib基础图表","slug":"matplotlib基础图表","date":"2020-04-26T09:53:07.000Z","updated":"2020-04-26T10:13:45.715Z","comments":true,"path":"2020/04/26/matplotlib基础图表/","link":"","permalink":"http://yoursite.com/2020/04/26/matplotlib%E5%9F%BA%E7%A1%80%E5%9B%BE%E8%A1%A8/","excerpt":"","text":"matplotlib基础图表一、根据x,y绘制12345678910111213import numpy as npfrom matplotlib import pyplot as pltx = np.arange(1, 11)y = x * 2 + 1plt.title(\"demo1\")plt.xlabel(\"x\")plt.ylabel(\"y\")plt.plot(x, y)plt.show() 12345678910111213import numpy as npfrom matplotlib import pyplot as pltx = np.arange(0, 3 * np.pi, 0.1)y = np.sin(x)plt.title(\"demo\")plt.xlabel(\"x\")plt.ylabel(\"y\")plt.plot(x, y, \"om\") #o指以圆点展示, m表示玫红色plt.show() 123456789101112131415161718192021222324252627282930import numpy as npfrom matplotlib import pyplot as pltp1_x = np.arange(1, 11)p1_y = p1_x * 5 + 8p2_x = np.arange(1, 11)p2_y = p2_x ** 3 + 10p3_x = np.arange(0, 4 * np.pi, 0.1)p3_y = np.sin(p3_x)p4_x = np.arange(0, 4 * np.pi, 0.1)p4_y = np.cos(p4_x)plt.subplot(2, 2, 1) # 两行两列第一个plt.title(\"p1\")plt.plot(p1_x, p1_y, \"om\")plt.subplot(2, 2, 2) # 两行两列第二个plt.title(\"p2\")plt.plot(p2_x, p2_y, \"pm\")plt.subplot(2, 2, 3)plt.title(\"p3\")plt.plot(p3_x, p3_y)plt.subplot(2, 2, 4)plt.title(\"p4\")plt.plot(p4_x, p4_y) 二、柱状图1234567891011121314import numpy as npfrom matplotlib import pyplot as pltb1_x = np.arange(1, 10, 2)b1_y = b1_x * 2 + 10b2_x = np.arange(2, 11, 2)b2_y = b2_x * 3 + 2plt.bar(b1_x, b1_y, color=\"m\", align='center')plt.bar(b2_x, b2_y, color=\"b\", align=\"center\")plt.title(\"bar_demo\")plt.show() 三、 直方图12345678910import numpy as npfrom matplotlib import pyplot as plta = np.random.randint(50, size=100)plt.hist(a, bins=[0, 10, 20, 30, 40, 50])plt.title(\"histogram\")plt.show() 四、散点图12345678import numpy as npfrom matplotlib import pyplot as pltx = np.random.randn(1000)y = np.random.randn(1000)plt.plot(x, y, \"om\")plt.show() 12345678import numpy as npfrom matplotlib import pyplot as pltx = np.random.randn(1000)y = np.random.randn(1000)plt.scatter(x, y)plt.show()","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"},{"name":"matplotlib","slug":"matplotlib","permalink":"http://yoursite.com/tags/matplotlib/"}]},{"title":"hadoop云服务器分布式安装配置","slug":"hadoop云服务器分布式安装配置","date":"2020-04-22T06:10:42.000Z","updated":"2020-04-22T06:10:42.489Z","comments":true,"path":"2020/04/22/hadoop云服务器分布式安装配置/","link":"","permalink":"http://yoursite.com/2020/04/22/hadoop%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/","excerpt":"","text":"hadoop云服务器分布式安装配置 在三台云服务器上搭建hadoop集群, 1master + 2slave使用版本: hadoop-3.2.1、jdk1.8.0_191、 scala-2.13.1⚠️ jdk11版本会出现Caused by: java.lang.NoClassDefFoundError: javax/activation/DataSource错误，还是选择了低版本 一、服务器创建用户设置密码、配置互相之间免密登录** ⚠️ 服务器的hostname 以及 /etc/hosts中配置的hostname 均不可带_下划线 ** 一、下载安装包(使用国内镜像)script12345wget https://mirrors.huaweicloud.com/java/jdk/8u191-b12/jdk-8u191-linux-x64.tar.gzwget https://downloads.lightbend.com/scala/2.13.1/scala-2.13.1.tgzwget https://mirrors.huaweicloud.com/apache/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz 二、解压配置环境变量script12345678910export SCALA_HOME=/home/kowhoy/software/scala-2.13.1export PATH=$SCALA_HOME/bin:$PATHexport HADOOP_HOME=/home/kowhoy/software/hadoop-3.2.1export PATH=$HADOOP_HOME/bin:$PATHJAVA_HOME=/home/kowhoy/software/jdk1.8.0_191JRE_HOME=/home/kowhoy/software/jdk1.8.0_191/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=$JAVA_HOME/bin:$PATH 三台服务器均进行以上操作 三、配置master,同步给slave 配置文件的根目录 $HADOOP_HOME/etc/hadoop配置文件中涉及到的文件夹，需要自行创建 hadoop-env.sh script1export JAVA_HOME=/home/kowhoy/software/jdk1.8.0_191 core-site.xml 12345678910111213&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;description&gt;hdfsurl&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/kowhoy/software/hadoop-3.2.1/tmp&lt;/value&gt; &lt;description&gt;临时文件路径&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml 1234567891011121314151617181920212223242526&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;master:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/kowhoy/software/hadoop-3.2.1/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/kowhoy/software/hadoop-3.2.1/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt; &lt;value&gt;/home/kowhoy/software/hadoop-3.2.1/dfs/namesecondary&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;decription&gt;master + slave个数&lt;/decription&gt; &lt;/property&gt;&lt;/configuration&gt; mapred-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml 12345678910111213&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;master&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; workers写配置的slave 12ecs00bcc00 masters文件是没有的，默认master的hosts配置应该是 master，如果不是，可能是在masters文件中配置，未测试 以上配置可scp到slave上，注意如果hostname不一致，需要修改 四、启动集群 格式化 script1hadoop namenode -format 启动 script1$&#123;HADOOP_HOME&#125;/sbin/start_all.sh jps查看 script12345678910#master25189 NameNode25638 ResourceManager25417 SecondaryNameNode26638 Jps#slave21406 NodeManager23646 Jps21278 DataNode 五、补充如有错误，查看${HADOOP_HOME}/logs下的日志，datanode相关日志是在slave上","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"}]},{"title":"Scala模式匹配","slug":"Scala模式匹配","date":"2020-04-21T07:01:32.000Z","updated":"2020-04-21T07:01:32.883Z","comments":true,"path":"2020/04/21/Scala模式匹配/","link":"","permalink":"http://yoursite.com/2020/04/21/Scala%E6%A8%A1%E5%BC%8F%E5%8C%B9%E9%85%8D/","excerpt":"","text":"模式匹配12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879package com.kowhoy.scalaimport scala.util.Random// 模式匹配object MatchDemo &#123; def main(args: Array[String]): Unit = &#123; // 基础匹配 basic_match() // 条件匹配 condition_match() // 类型匹配 type_match(List(1,2,4)) type_match(12) type_match(1.2) // array匹配 array_match(Array()) // 异常处理 error_exception_match() &#125; def basic_match(): Unit = &#123; val names = Array(\"A\", \"B\", \"C\") val name = names(Random.nextInt(names.length)) name match &#123; case \"A\" =&gt; println(\"AAAA\") case \"B\" =&gt; println(\"BBBB\") case _ =&gt; println(\"CCCCC\") &#125; &#125; def condition_match(): Unit = &#123; val names = Array(\"A\", \"B\", \"C\") val name = names(Random.nextInt(names.length))// name = \"C\" val buff = \"red\" name match &#123; case \"A\" =&gt; println(\"A\") case \"B\" =&gt; println(\"B\") case _ if(buff == \"red\") =&gt; println(\"D\") case _ =&gt; println(\"C\") &#125; &#125; def type_match(data:Any): Unit = &#123; data match &#123; case x:Int =&gt; println(\"INT\") case x:String =&gt; println(\"String\") case x:List[Any] =&gt; println(\"List\") case _ =&gt; println(\"other\") &#125; &#125; def array_match(array:Array[String]): Unit = &#123; array match &#123; case Array(\"abc\") =&gt; println(\"type1_abc\") case Array(x, y) =&gt; println(\"type2:\", x, y) case Array(x, _*) =&gt; println(\"type3\", x) case _ =&gt; println(\"other\") &#125; &#125; def error_exception_match(): Unit = &#123; try &#123; val d = 10 / 0 &#125; catch &#123; case e: ArithmeticException =&gt; println(\"error0\") case e: Exception =&gt; println(e.getStackTrace) &#125; finally &#123; println(\"Aaaaaaa\") &#125; &#125;&#125;","categories":[{"name":"Scala","slug":"Scala","permalink":"http://yoursite.com/categories/Scala/"}],"tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"}]},{"title":"柯里化","slug":"柯里化","date":"2020-04-15T10:29:12.000Z","updated":"2020-04-15T10:29:12.110Z","comments":true,"path":"2020/04/15/柯里化/","link":"","permalink":"http://yoursite.com/2020/04/15/%E6%9F%AF%E9%87%8C%E5%8C%96/","excerpt":"","text":"柯里化定义 将原来多个参数的函数，分成多个单个参数的函数 好处 复杂逻辑简单化 示例12345678910111213141516171819package com.kowhoy.scala//柯里化object keli &#123; def main(args: Array[String]): Unit = &#123; var inc = (x:Int, y:Int) =&gt; x + y println(inc(1, 2)) println(inc_kl(1)(2)) var firstNum = inc_kl((1))_ var res = firstNum(2) println(res) &#125; def inc_kl(x:Int)(y:Int) = &#123; x + y &#125;&#125; 注意 当只传递部分参数生产函数的时候，需要用占位符_补充后面的参数","categories":[{"name":"Scala","slug":"Scala","permalink":"http://yoursite.com/categories/Scala/"}],"tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"}]},{"title":"匿名函数","slug":"匿名函数","date":"2020-04-15T10:28:45.000Z","updated":"2020-04-15T10:28:45.241Z","comments":true,"path":"2020/04/15/匿名函数/","link":"","permalink":"http://yoursite.com/2020/04/15/%E5%8C%BF%E5%90%8D%E5%87%BD%E6%95%B0/","excerpt":"","text":"匿名函数 简写函数定义, 匿名函数不可以写成传名调用 12345678910111213141516171819202122232425262728293031package com.kowhoy.scala//匿名函数object niming &#123; def main(args: Array[String]): Unit = &#123; var initNum = 0 var counter = () =&gt; &#123; initNum += 1 initNum &#125; var sendValue = (x: Int) =&gt; &#123; for (i &lt;- 0 to 5) &#123; println(x) &#125; &#125; sendValue(counter()) sendName(counter()) &#125; def sendName(x: =&gt; Int) = &#123; for (i &lt;- 0 to 5) &#123; println(x) &#125; &#125;&#125;","categories":[{"name":"Scala","slug":"Scala","permalink":"http://yoursite.com/categories/Scala/"}],"tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"}]},{"title":"传值传名调用","slug":"传值传名调用","date":"2020-04-15T10:28:02.000Z","updated":"2020-04-15T10:28:02.982Z","comments":true,"path":"2020/04/15/传值传名调用/","link":"","permalink":"http://yoursite.com/2020/04/15/%E4%BC%A0%E5%80%BC%E4%BC%A0%E5%90%8D%E8%B0%83%E7%94%A8/","excerpt":"","text":"传值传名调用 传值 name:type 计算好值进行调用 传名 name: =&gt; type在调用处再进行计算 12345678910111213141516171819202122232425262728293031323334package com.kowhoy.scala// 传值调用和传名调用object sendWay &#123; var initNum = 0 def main(args: Array[String]):Unit = &#123; sendName(counter) println(\"--\" * 20) initNum = 0 sendValue(counter) &#125; def counter:Int = &#123; initNum += 1 initNum &#125; def sendName(x: =&gt; Int) :Unit = &#123; for (i &lt;- 0 to 5) &#123; println(x) &#125; &#125; def sendValue(x: Int) :Unit = &#123; for (i &lt;- 0 to 5) &#123; println(x) &#125; &#125;&#125; 12345678910111213123456----------------------------------------111111","categories":[{"name":"Scala","slug":"Scala","permalink":"http://yoursite.com/categories/Scala/"}],"tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"}]},{"title":"scala变长参数","slug":"scala变长参数","date":"2020-04-15T10:27:19.000Z","updated":"2020-04-15T10:27:19.716Z","comments":true,"path":"2020/04/15/scala变长参数/","link":"","permalink":"http://yoursite.com/2020/04/15/scala%E5%8F%98%E9%95%BF%E5%8F%82%E6%95%B0/","excerpt":"","text":"变长参数 函数的参数可以设置为多个同种类型或不同类型的参数 1234567891011121314151617181920212223242526272829303132333435363738package com.kowhoy.scala//变长参数, 可变参数object varargs &#123; def main(args: Array[String]): Unit = &#123; var numsSum = sumargs(1, 1, 3, 4, 5, 6) println(numsSum) var minusRight = minusargs(\"right\", 5, 4, 3, 2, 1) // 5 4 3 (2 -1 ) =&gt; 5 - 4 - (3-1) =&gt; 5 - 4 - 2 =&gt; 5 - 2 =&gt; 3 var minusLeft = minusargs(\"left\", 5, 4, 3, 2, 1) println(minusRight, minusLeft) anyArgs(\"abc\", 123) &#125; def sumargs(args: Int*):Int = &#123; var res = args.reduceLeft(_+_) res &#125; def minusargs(minuDirction:String, args: Int*):Int = &#123; var res = 0 if (minuDirction == \"right\") &#123; res = args.reduceRight(_-_) &#125; else if (minuDirction == \"left\") &#123; res = args.reduceLeft(_-_) &#125; res &#125; def anyArgs(args: Any*):Unit = &#123; args.foreach(println) &#125;&#125; reduceLeft / reduceRight","categories":[{"name":"Scala","slug":"Scala","permalink":"http://yoursite.com/categories/Scala/"}],"tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"}]},{"title":"Scala中的lazy使用","slug":"Scala中的lazy使用","date":"2020-04-15T10:26:21.000Z","updated":"2020-04-15T10:26:21.267Z","comments":true,"path":"2020/04/15/Scala中的lazy使用/","link":"","permalink":"http://yoursite.com/2020/04/15/Scala%E4%B8%AD%E7%9A%84lazy%E4%BD%BF%E7%94%A8/","excerpt":"","text":"Scala中的lazy使用什么是lazy lazy用在赋值的时候，目的是为了在赋值变量真正被使用的时候，才会进行变量赋值处理 示例12345678910111213141516171819202122232425262728293031323334353637383940414243//lazy赋值package com.kowhoy.scalaobject Lazy &#123; def main(args: Array[String]): Unit = &#123; withNoLazy.run() println(\"--\"*20) withLazy.run() &#125;&#125;object withNoLazy &#123; def init(): Int = &#123; println(\"非惰性赋值\") 0 &#125; def run(): Unit = &#123; val initNum = init() println(\"Start...\") println(initNum) println(\"End...\") &#125;&#125;object withLazy &#123; def init(): Int = &#123; println(\"惰性赋值\") 0 &#125; def run(): Unit = &#123; lazy val initNum = init() println(\"Start...\") println(initNum) println(\"End...\") &#125;&#125; 上文的输出如下123456789非惰性赋值Start...0End...----------------------------------------Start...惰性赋值0End... 注意 使用lazy赋值的变量只能是val声明的lazy modifier allowed only with value definitions","categories":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/categories/scala/"}],"tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"}]},{"title":"mysql分组top优化","slug":"mysql分组top优化","date":"2020-04-14T08:26:35.000Z","updated":"2020-04-14T08:26:35.915Z","comments":true,"path":"2020/04/14/mysql分组top优化/","link":"","permalink":"http://yoursite.com/2020/04/14/mysql%E5%88%86%E7%BB%84top%E4%BC%98%E5%8C%96/","excerpt":"","text":"mysql分组top优化 mysql分组取top1 通常是用max() 然后内连 可以使用变量的方式避免join操作 问题场景同一组oss_id, osp_id, pro_sku_code, start_time 会有多种 price_val,先需要依据update_time字段取每一组的最后一条更新内容 原始写法123456789101112select a.osp_id, a.oss_id, a.pro_sku_code, a.price_val, a.start_time from pricing.price_log a,( select max(update_time) as max_up, oss_id, osp_id, pro_sku_code, start_time from pricing.price_log where price_type = 10 AND STATUS = 1 AND delete_flag = 0 group by oss_id, osp_id, pro_sku_code, start_time ) b where a.oss_id = b.oss_id and a.osp_id = b.osp_id and a.pro_sku_code = b.pro_sku_code and a.start_time = b.start_time and a.update_time = b.max_up and a.price_type = 10 and a.status = 1 and a.delete_flag = 0 150w数据10min执行时间 * 使用变量写法1234567891011121314151617181920set @sameno := 1;set @oss_id := -1;set @osp_id := -1;set @pro_sku_code := \"\";set @start_time := \"1970-01-01\";select oss_id, osp_id, pro_sku_code, start_time, price_val from (select @sameno := case when @oss_id = oss_id and @osp_id = osp_id and @pro_sku_code = pro_sku_code and @start_time = start_time then @sameno + 1 else 1 end as sameno, @oss_id := oss_id oss_id, @osp_id := osp_id osp_id, @pro_sku_code := pro_sku_code pro_sku_code, @start_time := start_time start_time, price_valfrom(selectoss_id, osp_id, pro_sku_code, start_time, price_valfrom price_log where price_type = 10 and status = 1 and delete_flag = 0 order byoss_id, osp_id, pro_sku_code, start_time, update_time desc ) a) b where sameno = 1 30s左右 * ⚠️ 需要先设定头部初始变量，否则首次执行结果非预期，多次执行效果不一致，很奇怪 使用python操作 不可使用pd.read_sql_query() 有异常 1234567891011121314151617181920212223242526272829sql = ''' select oss_id, osp_id, pro_sku_code, start_time, price_val from ( select @sameno := case when @oss_id = oss_id and @osp_id = osp_id and @pro_sku_code = pro_sku_code and @start_time = start_time then @sameno + 1 else 1 end as sameno, @oss_id := oss_id oss_id, @osp_id := osp_id osp_id, @pro_sku_code := pro_sku_code pro_sku_code, @start_time := start_time start_time, price_val from (select oss_id, osp_id, pro_sku_code, start_time, price_val from price_log where price_type = 10 and status = 1 and delete_flag = 0 order by oss_id, osp_id, pro_sku_code, start_time, update_time desc ) a) b where sameno = 1 ''' db_conf = conf.db_conf oms_conf = db_conf['oms_db'] db = pymysql.connect(oms_conf['host'], oms_conf['user'], oms_conf['passwd'], 'pricing') heads = [\"set @sameno := 1;\", \"set @oss_id := -1;\", \"set @osp_id := -1;\", \"set @pro_sku_code := '';\", \"set @start_time := '1970-01-01'\"] cursor = db.cursor() for col in heads: cursor.execute(col) cursor.execute(sql) res = cursor.fetchall() 还可以使用程序内进行筛选，不过效率并不高，但是比max要好","categories":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"配置数据管理查询","slug":"配置数据管理查询","date":"2020-04-10T10:05:02.000Z","updated":"2020-04-10T10:16:55.898Z","comments":true,"path":"2020/04/10/配置数据管理查询/","link":"","permalink":"http://yoursite.com/2020/04/10/%E9%85%8D%E7%BD%AE%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86%E6%9F%A5%E8%AF%A2/","excerpt":"","text":"配置数据管理查询 背景: 数据库以及服务器的配置信息比较多，临时使用频率也高,找起来不方便 pylsy configparser catconfig.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import sysfrom pylsy import pylsytableimport configparsercp &#x3D; configparser.ConfigParser()cp.read(&quot;&#x2F;Users&#x2F;zhouke&#x2F;code&#x2F;catconfig&#x2F;catconfig.cfg&quot;)cp_sections &#x3D; cp.sections()config_dict &#x3D; &#123;&#125;for section in cp_sections: name &#x3D; section.split(&quot;:&quot;)[0] if name not in config_dict: config_dict[name] &#x3D; [] config_data &#x3D; cp._sections[section] config_dict[name].append(config_data)match_dict &#x3D; &#123; &quot;db_51&quot;:[&quot;51&quot;, &quot;生产&quot;], &quot;db_69&quot;:[&quot;db_69&quot;], &quot;db_oms&quot;:[&quot;oms&quot;], &quot;db_oil&quot;:[&quot;oil&quot;], &quot;db_oil_test&quot;:[&quot;oil_test&quot;], &quot;db_ygh&quot;:[&quot;ygh&quot;, &quot;petrol&quot;], &quot;db_cd&quot;:[&quot;db_cd&quot;, &quot;db_禅道&quot;, &quot;cd&quot;, &quot;禅道&quot;], &quot;db_yzg&quot;:[&quot;yzg&quot;], &quot;db_yzg_test&quot;:[&quot;yzg_test&quot;], &quot;mail_data&quot;:[&quot;mail&quot;], &quot;web_datav&quot;:[&quot;datav&quot;], &quot;web_ali&quot;:[&quot;zy_ali&quot;], &quot;tm&quot;:[&quot;teamview&quot;, &quot;teamviewer&quot;, &quot;tm&quot;], &quot;s_69&quot;:[&quot;s_69&quot;, &quot;69&quot;], &quot;s_jump&quot;:[&quot;jump&quot;, &quot;jumper&quot;, &quot;2222&quot;], &quot;s_cd&quot;:[&quot;s_cd&quot;, &quot;s_禅道&quot;], &quot;s_54&quot;:[&quot;54&quot;, &quot;s_54&quot;], &quot;s_56&quot;:[&quot;56&quot;, &quot;s_56&quot;, &quot;win&quot;, &quot;windows&quot;], &quot;s_ali&quot;:[&quot;ali&quot;], &quot;s_bcc&quot;:[&quot;bcc&quot;, &quot;baidu&quot;, &quot;s_bcc&quot;], &quot;db_bcc&quot;:[&quot;db_bcc&quot;], &quot;wx&quot;:[&quot;wx&quot;, &quot;wx_mail&quot;], &quot;bk&quot;:[&quot;bucket&quot;, &quot;bos&quot;], &quot;tab&quot;:[&quot;tableau&quot;, &quot;license&quot;, &quot;tab&quot;]&#125;find_dict &#x3D; &#123;&#125;for name, alias_list in match_dict.items(): for idx, alias in enumerate(alias_list): find_dict[alias] &#x3D; name find_dict[name.split(&quot;_&quot;)[0] + alias] &#x3D; namesearch_str &#x3D; sys.argv[1]if search_str not in find_dict: print(&quot;🈚️&quot;)else: config_res &#x3D; config_dict[find_dict[search_str]] table_dict &#x3D; &#123;&#125; for idx, row in enumerate(config_res): for k, v in row.items(): if k not in table_dict: table_dict[k] &#x3D; [] table_dict[k].append(v) table_head &#x3D; list(table_dict.keys()) table &#x3D; pylsytable(table_head) for k, v in table_dict.items(): table.add_data(k, v) print(table) catconfig.cfg 123456789[db_a:1]host &#x3D; 111user &#x3D; 111passwd &#x3D; 111[db_a:2]host &#x3D; 111user &#x3D; 222passwd &#x3D; 222 catconfig.sh 12#!&#x2F;bin&#x2F;bashpython &#x2F;Users&#x2F;zhouke&#x2F;code&#x2F;catconfig&#x2F;catconfig.py $1 添加全局命令 1sudo ln -s &#x2F;Users&#x2F;zhouke&#x2F;code&#x2F;catconfig&#x2F;catconfig.sh &#x2F;usr&#x2F;local&#x2F;bin&#x2F;catconfig 使用 1catconfig a ## a: 配置的映射字段 效果","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/tags/%E5%B7%A5%E5%85%B7/"}]},{"title":"uwsgi+nginx部署flask","slug":"uwsgi-nginx部署flask","date":"2020-04-10T09:54:21.000Z","updated":"2020-04-10T10:14:43.268Z","comments":true,"path":"2020/04/10/uwsgi-nginx部署flask/","link":"","permalink":"http://yoursite.com/2020/04/10/uwsgi-nginx%E9%83%A8%E7%BD%B2flask/","excerpt":"","text":"uwsgi + nginx部署flask一、安装uwsgi 使用python版本对应的pip进行安装uwsgi 1pip install uwsgi 在项目下创建uwsgi.ini文件 12345678910111213[uwsgi]http &#x3D; 0.0.0.0:5000pythonpath &#x3D; &#x2F;home&#x2F;anaconda3&#x2F;bin&#x2F;pythonchdir &#x3D; &#x2F;home&#x2F;code&#x2F;apiwsgi-file &#x3D; app.pycallable &#x3D; appprocesses &#x3D; 4threads &#x3D; 2stats &#x3D; 127.0.0.1:9191pidfile &#x3D; uwsgi.piddeamonize &#x3D; .&#x2F;log&#x2F;uwsgi.loglazy-apps &#x3D; truetouch-chain-reload &#x3D; true 后台运行uwsgi 1uwsgi -d --ini uwsgi.ini 二、安装配置nginx 下载安装包 1wget http:&#x2F;&#x2F;nginx.org&#x2F;download&#x2F;nginx-1.17.9.tar.gz 解压，不要放到/usr/local下，会安装到这个目录 1tar -zxvf nginx-1.17.9.tar.gz 查看环境是否满足 123456789101112131415cd nginx-1.17.9&#x2F;.&#x2F;configure##不出现error则满足##否则检查依赖库#依赖库安装sudo yum install gcc-c++sudo yum install pcresudo yum install pcre-develsudo yum install zlibsudo yum install zlib-develsudo yum install opensslsudo yum install openssl-devel 无错误进行编译 123makesudo make install 三、配置nginx 自定义配置文件 /usr/local/nginx/conf/conf.d/api.conf 12345678910111213141516171819202122232425262728upstream project&#123; server localhost:5000;&#125;server &#123; listen 8080; server_name IP&#x2F;localhost; access_log &#x2F;home&#x2F;api&#x2F;access.log; error_log &#x2F;home&#x2F;api&#x2F;error.log; location &#x2F; &#123; proxy_pass http:&#x2F;&#x2F;project; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504; proxy_max_temp_file_size 0; proxy_connect_timeout 90; proxy_send_timeout 90; proxy_read_timeout 90; proxy_buffer_size 4k; proxy_buffers 4 32k; proxy_busy_buffers_size 64k; proxy_temp_file_write_size 64k; &#125;&#125; 配置/usr/local/nginx/conf/nginx.conf 12##在http下面加上include &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;conf&#x2F;conf.d&#x2F;*.conf; 配置开机启动 1234567891011121314151617vim &#x2F;lib&#x2F;systemd&#x2F;system&#x2F;nginx.service[Unit]Description&#x3D;nginxAfter&#x3D;network.target[Service]Type&#x3D;forkingExecStart&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;nginxExecReload&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;nginx reloadExecStop&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;nginx quitPrivateTmp&#x3D;true[Install]WantedBy&#x3D;multi-user.targetsystemctl enable nginx.service 启动nginx 1systemctl start nginx","categories":[{"name":"nginx","slug":"nginx","permalink":"http://yoursite.com/categories/nginx/"},{"name":"uwsgi","slug":"nginx/uwsgi","permalink":"http://yoursite.com/categories/nginx/uwsgi/"}],"tags":[{"name":"uwsgi","slug":"uwsgi","permalink":"http://yoursite.com/tags/uwsgi/"},{"name":"nginx","slug":"nginx","permalink":"http://yoursite.com/tags/nginx/"}]},{"title":"Elasticsearch基础使用","slug":"Elasticsearch基础使用","date":"2020-04-02T08:23:17.000Z","updated":"2020-04-02T10:22:04.764Z","comments":true,"path":"2020/04/02/Elasticsearch基础使用/","link":"","permalink":"http://yoursite.com/2020/04/02/Elasticsearch%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/","excerpt":"","text":"Elasticsearch基础使用一、安装 需要java环境 下载安装包 1wget https:&#x2F;&#x2F;mirrors.huaweicloud.com&#x2F;elasticsearch&#x2F;7.6.1&#x2F;elasticsearch-7.6.1-linux-x86_64.tar.gz 解压，配置环境变量 12345tar -zxvf elasticsearch-7.6.1-linux-x86_64.tar.gz#~&#x2F;.bash_profileexport ELASTICSEARCH_PATH&#x3D;&#x2F;home&#x2F;kowhoy&#x2F;software&#x2F;elasticsearch-7.6.1export PATH&#x3D;$&#123;ELASTICSEARCH_PATH&#125;&#x2F;bin:$PATH 启动 1elasticsearch 检查 12#另启终端curl &#39;http:&#x2F;&#x2F;localhost:9200&#x2F;?pretty&#39; 二、基础概念 elasticsearch中的概念与关系型数据库的概念类比: 关系型数据库 elasticsearch databases indices tables types rows documents cloumns fields 三、基础使用 本篇使用的是insomnia工具目的是创建一个类似以下关系型数据库结构的Elasticsearch 类型 名称 database-name history-demo table-name shell column-1-name command column-2-name create-by 创建index 12345678910111213141516171819202122232425262728@method: PUT@url: http://localhost:9200/history_demo@return: &#123; \"acknowledged\": true, \"shards_acknowledged\": true, \"index\": \"history_demo\"&#125;or##写入数据的同时也创建index@method: PUT@url: http://localhost:9200/history_demo/shell/1@data: &#123;\"command\":\"df -h\"&#125;@return: &#123; \"_index\": \"history_demo\", \"_type\": \"shell\", \"_id\": \"1\", \"_version\": 1, \"result\": \"created\", \"_shards\": &#123; \"total\": 2, \"successful\": 1, \"failed\": 0 &#125;, \"_seq_no\": 0, \"_primary_term\": 1&#125; 查看indices 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748##查看指定的index@method: GET@url: http://localhost:9200/history_demo@return: &#123; \"history_demo\": &#123; \"aliases\": &#123;&#125;, \"mappings\": &#123; \"shell\": &#123; \"properties\": &#123; \"command\": &#123; \"type\": \"text\", \"fields\": &#123; \"keyword\": &#123; \"type\": \"keyword\", \"ignore_above\": 256 &#125; &#125; &#125; &#125; &#125; &#125;, \"settings\": &#123; \"index\": &#123; \"creation_date\": \"1585796252661\", \"number_of_shards\": \"5\", \"number_of_replicas\": \"1\", \"uuid\": \"FrQU3tV4Smu5XioGH6qUjw\", \"version\": &#123; \"created\": \"6060199\" &#125;, \"provided_name\": \"history_demo\" &#125; &#125; &#125;&#125;## 查看所有的indices@method: GET@url: http://localhost:9200/_cat/indices@remark: 使用`_cat/indices`@return:yellow open oss JYYZYHlxQsCYpUfJGjNnfw 5 1 0 0 1.2kb 1.2kbyellow open doc 0JUEuXarQ0aBaB1YHD2KcA 5 1 1 0 4.7kb 4.7kbyellow open history_demo FrQU3tV4Smu5XioGH6qUjw 5 1 1 0 4.4kb 4.4kbyellow open posts c18QVZ00Snyp1OOomxjJuA 5 1 1 0 5.5kb 5.5kbgreen open .kibana_1 jLWNf4ysRDSUvR3qfmv5Ow 1 0 2 0 9.3kb 9.3kbyellow open blog x_vNxVVHT6qgQY3_RKiaHg 5 1 6 0 28.3kb 28.3kbyellow open person 1ctpvuoqTV-vTO4q9G9cnQ 5 1 2 0 8.1kb 8.1kb 插入documents 123456789101112131415161718192021222324252627282930313233343536373839## 插入单条数据@method: PUT/POST@url: http://localhost:9200/history_demo/shell/3@data: &#123;\"command\":\"df -h\"&#125;@return: &#123; \"_index\": \"history_demo\", \"_type\": \"shell\", \"_id\": \"3\", \"_version\": 1, \"result\": \"created\", \"_shards\": &#123; \"total\": 2, \"successful\": 1, \"failed\": 0 &#125;, \"_seq_no\": 0, \"_primary_term\": 1&#125;## 插入指定ID的多条数据@method: PUT/POST@url: http://localhost:9200/history_demo/shell/_bulk@data: &#123;\"index\":&#123;\"_id\":6&#125;&#125;&#123;\"command\":\"mysql\"&#125;&#123;\"index\":&#123;\"_id\":7&#125;&#125;&#123;\"command\":\"logatsh\"&#125;#!!注意 这个json_data末尾要多留一行@return: ...## 插入不指定ID的多条数据@method: PUT/POST@url: http://localhost:9200/history_demo/shell/_bulk@data:&#123;\"index\":&#123;&#125;&#125;&#123;\"command\":\"mysql\"&#125;&#123;\"index\":&#123;&#125;&#125;&#123;\"command\":\"logatsh\"&#125; 查询 1234567891011## 查询所有的documents@method: GET@url: http://localhost:9200/history_demo/shell/_search@data: &#123;\"query\":&#123;\"match_all\":&#123;&#125;&#125;&#125;@return: ...## 按条件查询@method: GET@url: http://localhost:9200/history_demo/shell/_search@data: &#123;\"query\":&#123;\"match\":&#123;\"command\":\"mysql\"&#125;&#125;&#125;@return: ... 删除 12345678910## 指定ID进行delete@method: DELETE@url: http://localhost:9200/history_demo/shell/1@return: ...## 根据条件删除@method:POST@url: http://localhost:9200/history_demo/shell/_delete_by_query@data: &#123;\"query\":&#123;\"match\":&#123;\"command\":\"mysql\"&#125;&#125;&#125;@return: ... 修改 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051## 指定ID进行修改@method:POST@url: http://localhost:9200/history_demo/shell/2/_update@data: &#123; \"script\": &#123; \"source\":\"ctx._source.command=params.command\", \"lang\": \"painless\", \"params\": &#123; \"command\":\"history\" &#125; &#125;&#125;@return: ...@remark: ctx指当前事务, _source指当前的document, lang指使用painless脚本语言来编写script## 仅增加字段进行修改@method: POST@url: http://localhost:9200/history_demo/shell/2/_update@data: &#123; \"script\": &#123; \"source\": \"ctx._source.bash_time='2020-02-02'\" &#125;&#125;@return: ...## 仅删除字段进行修改@method: POST@url: http://localhost:9200/history_demo/shell/2/_update@data: &#123; \"script\": &#123; \"source\": \"ctx._source.remove('bash_time')\" &#125;&#125;@return: ...## 根据条件进行修改@method: POST@url: http://localhost:9200/history_demo/shell/_update_by_query@data: &#123; \"query\": &#123; \"match\":&#123;\"command\":\"df -h\"&#125; &#125;, \"script\": &#123; \"source\": \"ctx._source.command=params.command\", \"lang\": \"painless\", \"params\": &#123; \"command\": \"tail data.log\" &#125; &#125;&#125;@return: ..","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://yoursite.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://yoursite.com/tags/Elasticsearch/"}]},{"title":"flask思路整理","slug":"flask思路整理","date":"2020-03-27T10:13:06.000Z","updated":"2020-03-27T10:35:34.834Z","comments":true,"path":"2020/03/27/flask思路整理/","link":"","permalink":"http://yoursite.com/2020/03/27/flask%E6%80%9D%E8%B7%AF%E6%95%B4%E7%90%86/","excerpt":"","text":"flask思路整理一. 创建虚拟环境python3 -m venv venv. venv/bin/activate 二. init.py主要工作： 创建应用 添加配置 初始化应用 注册蓝图 创建应用: 123456import osfrom flask import Flaskdef create_app(test_config=None): ## instance_relative_config 使用True的话就会从instance文件夹作为配置文件的路径 app = Flask(__name__, instance_relative_config=True) 添加配置 12345678910111213##方式一:app.config.from_mapping( SECRET_KEY='dev' )##方式二:#silent 文件不存在是否静默报错app.config.from_pyfile('config.py', silent=True)#随机key的生成import osimport binasciibinascii.hexlify(os.urandom(16)) 初始化应用 123app.teardown_appcontext(db.close_db) ##在返回响应后进行清理调用的程序app.cli.add_command(db.init_db_command) ##添加命令行指令 注册蓝图 12from . import authapp.register_buleprint(auth.bp) 三、db.py 使用click创建命令 12345678import clickfrom flask.cli import with_appcontext@click.command('init-db')@with_appcontextdef init_db_command(): **** click.echo(\"输出内容\") 数据库连接使用g,不会每次请求都重新创建连接 1234567from flasj import current_app, gdef get_db(): if 'db' not in g: g.db = 创建连接 return g.db 关闭数据库 12345def close_db(e=None): db = g.pop('db', None) if db is not None: db.close() 四、具体逻辑 蓝图 123from flask import Blueprintbp &#x3D; Blueprint(&#39;auth&#39;, __name__, url_prefix&#x3D;&#39;&#x2F;auth&#39;) 密码相关 1from werkzeug.security import check_password_hash, generate_password_hash 保证请求之前知道当时的用户 12345678@bp.before app_requestdef load_logged_in_user(): user_id = session.get('user_id') if user_id is None: g.user = None else: g.user = get_db()..... 要求是在登录状态(使用装饰符) 123456789import functoolsdef login_required(view): @functools.wraps(view) def wrapped_view(**kwargs): if g.user is None: return redirect(url_for(\"auth.login\")) return view(**kwargs) return wrapped_view 五、项目可安装 setup.py 123456789101112from setuptools import find_packages, setupsetup( name='flaskr', version='1.0.0', packages=find_packages(), include_package_data=True, zip_safe=False, install_requires=[ 'flask', ],) MANIIFEST.in 1234include flaskr&#x2F;schema.sqlgraft flaskr&#x2F;staticgraft flaskr&#x2F;templatesglobal-exclude *.pyc 可以使用pip install -e .安装项目 六、部署产品 构建 123pip install wheelpython setup.py bdist_wheel 安装将生成的.whl文件发送到服务求 1pip install ***.whl 更改配置是在venv/var/***-instance/下 七、运行产品服务器12pip iinstall waitresswaitress-serve --call &#39;appname:create_app&#39; 启动应用 12345export FLASK_APP&#x3D;APP_NAMEexport FLASK_ENV&#x3D;developmentflask runflask init-db","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"flask","slug":"flask","permalink":"http://yoursite.com/tags/flask/"},{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"tmux——基本使用示例","slug":"tmux——基本使用示例","date":"2020-03-27T04:00:05.000Z","updated":"2020-03-27T04:09:32.564Z","comments":true,"path":"2020/03/27/tmux——基本使用示例/","link":"","permalink":"http://yoursite.com/2020/03/27/tmux%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B/","excerpt":"","text":"tmux——基本使用示例 使用tmux后台运行bash_history_notify项目 一、安装tmux12345#mac:brew install tmux#centos:yum install tmux 二、配置tmux 使用了github上的配置，然后做了修改tmux的默认快捷键前缀为 ctrl + b, 现在改为ctrl+a，下文中使用Prefix代替ctrl+a⚠️ :使用快捷键是先摁下快捷键前缀，然后抬手，在进行具体的快捷按键 1234cdgit clone https:&#x2F;&#x2F;github.com&#x2F;gpakosz&#x2F;.tmux.gitln -s -f .tmux&#x2F;.tmux.confcp .tmux&#x2F;.tmux.conf.local . 修改快捷键前缀 vim .tmux/.tmux.conf.local去掉290行附近的注释符 1234unbind C-aunbind C-bset -g prefix C-abind C-a send-prefix 三、使用tmux开启任务 bcc_00创建kafka_server的sessionn1tmux new -s kafka_server 使用Prefix+%分屏，使用Prefix+方向键切换鼠标所在窗格 在窗格1中启动zookeeper 12cd app&#x2F;kafka_2.12-2.4.0&#x2F;.&#x2F;bin&#x2F;zookeeper-server-start.sh .&#x2F;config&#x2F;zookeeper.properties 在窗格2中启动kafka 12cd app&#x2F;kafka_2.12-2.4.0&#x2F;.&#x2F;bin&#x2F;kafka-server-start.sh .&#x2F;config&#x2F;server.properties ) 使用Prefix+c 创建新窗口,并创建3个窗格,启动bash_history的消费端、生产端、实验窗口 在窗格1中启动消费端 12cd ~&#x2F;code&#x2F;python&#x2F;bash_history&#x2F;python bash_history_consumer.py 在窗格2中启动生产端 12cd ~&#x2F;code&#x2F;python&#x2F;bash_history&#x2F;python bash_history_producer.py ~&#x2F;.bash_history 在窗格3中随便输入命令，检查另外两个窗格是否有输出 bcc_01上开启生产端同上步骤，bcc_01上没有消费端 使用Prefix + d 退出session 使用tmux ls 查看session， 使用tmux at -t &lt;session_name&gt;连接session","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[]},{"title":"kafak示例-远程监控终端输入","slug":"kafak示例-远程监控终端输入","date":"2020-03-26T03:52:54.000Z","updated":"2020-03-26T03:52:54.112Z","comments":true,"path":"2020/03/26/kafak示例-远程监控终端输入/","link":"","permalink":"http://yoursite.com/2020/03/26/kafak%E7%A4%BA%E4%BE%8B-%E8%BF%9C%E7%A8%8B%E7%9B%91%E6%8E%A7%E7%BB%88%E7%AB%AF%E8%BE%93%E5%85%A5/","excerpt":"","text":"kafak示例-远程监控终端输入 使用pyinotify监听.bash_history文件，使用kafka生产消费 一、kafka安装配置 安装java环境 官网下载kafka安装包 1wget http:&#x2F;&#x2F;archive.apache.org&#x2F;dist&#x2F;kafka&#x2F;1.0.0&#x2F;kafka_2.12-1.0.0.tgz 解压压缩包 配置/config/server.properties 1advertised.listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;&#123;&#123;IP&#125;&#125;:9092 启动kafka 12345##zookeeper启动.&#x2F;bin&#x2F;zookeeper-server-start.sh .&#x2F;config&#x2F;zookeeper.properties ##server启动.&#x2F;bin&#x2F;kafka-server-start.sh .&#x2F;config&#x2F;server.properties 创建topic 1.&#x2F;bin&#x2F;kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic &#123;&#123;topic_name&#125;&#125; 二、安装python依赖12345#kafka-pythonpip install kafka-python#pyinotifypip install pyinotify 三、服务器端代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495# -*- coding: utf-8 -*-# @Description: 监听文件变更向kafka发送# @Author: kowhoy# @Last Modified by: kowhoy# @filename: bash_history_producer.pyimport pyinotifyimport timeimport osimport sysimport jsonfrom kafka import KafkaProducerimport datetimeimport socketbootstrap_servers = \"ip:9092\"topic = \"bash_history\"host_name = \"bcc_00\"host_ip = socket.gethostbyname(socket.getfqdn(socket.gethostname()))'''@name: [class]Monitor_file@date: 2020-03-26 09:41:13@desc: 监听文件@param: [str]filename @return: '''class Monitor_file: def notify_bash_history(self, filename): wm = pyinotify.WatchManager() notifier = pyinotify.Notifier(wm) wm.watch_transient_file(filename, pyinotify.IN_MODIFY, Process_transient_file) notifier.loop()'''@name: [class]Process_transient_file@date: 2020-03-26 09:44:29@desc: 文件发生变化触发@param: @return: '''class Process_transient_file(pyinotify.ProcessEvent): def process_IN_MODIFY(self, event): line = file.readlines() if line: if len(line) == 2: order_time = int(line[0][1:-1]) order_time_array = time.localtime(order_time) order_time_str = time.strftime(\"%Y-%m-%d %H:%M:%S\", order_time_array) order = line[1][:-1] msg = &#123; \"host_name\": host_name, \"host_ip\": host_ip, \"order_time\": order_time_str, \"order\": order &#125; Send_to_kafka().send_msg(msg)'''@name: [class]Send_to_kafka@date: 2020-03-26 09:47:50@desc: 向kafka发送msg@param:@return: '''class Send_to_kafka(): def __init__(self): self.producer = KafkaProducer(bootstrap_servers=bootstrap_servers) def send_msg(self, msg): msg = json.dumps(msg, ensure_ascii=False).encode(\"utf-8\") self.producer.send(topic, msg) print(\"-\"*10, \"数据已发送\", \"-\"*10) print(msg) self.producer.flush()if __name__ == \"__main__\": filename = sys.argv[1] if not os.path.exists(filename): raise FileExistsError file_stat = os.stat(filename) file_size = file_stat[6] file = open(filename, \"r\") file.seek(file_size) Monitor_file().notify_bash_history(filename) 四、客户端代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697# -*- coding: utf-8 -*-# @Description: # @Author: kowhoy# @Date: 2020-03-26 10:15:24# @Last Modified time: 2020-03-26 11:25:15from kafka import KafkaConsumerfrom kafka.structs import TopicPartitionimport pandas as pdfrom sqlalchemy import create_engineimport jsonimport sysimport osdbh_config = &#123; \"host\": \"localhost\", \"user\": \"root\", \"passwd\": \"passwd\", \"database\": \"bash_history\", \"table\": \"bash_history_log\"&#125;save_to_tb = True ## 是否存数据库topic = \"bash_history\"bootstrap_servers = [\"ip:9092\"]single_file_size = 1 #Mclass Bash_history_consumer: def __init__(self): self.topic = \"bash_history\" self.consumer = KafkaConsumer(bootstrap_servers=bootstrap_servers) self.log_file_path = \"./bash_history_log/\" log_files = os.listdir(self.log_file_path) max_log_count = max([int(filename.split(\"_\")[-1]) for filename in log_files]) if len(log_files) &gt; 0 else -1 self.count = max_log_count + 1 self.save_list = [] if save_to_tb: e = \"mysql+pymysql://%s:%s@%s/%s\"%(dbh_config[\"user\"], dbh_config[\"passwd\"], dbh_config[\"host\"], dbh_config[\"database\"]) self.dbh = create_engine(e) empty_sql = 'select * from information_schema.TABLES where TABLE_SCHEMA = \"&#123;db&#125;\" and TABLE_NAME = \"&#123;tb&#125;\"'.\\ format(db=dbh_config[\"database\"], tb=dbh_config[\"table\"]) empty_df = pd.read_sql_query(empty_sql, self.dbh) self.tb_empty = empty_df.empty def bash_consumer(self): self.consumer.assign([TopicPartition(topic=topic, partition=0)]) for msg in self.consumer: msg_offset = msg.offset msg_value = (msg.value).decode(\"utf-8\") msg_value = json.loads(msg_value) self.save_list.append(msg_value) save_data_size = sys.getsizeof(self.save_list) if save_data_size &gt;= single_file_size * 1024 * 1024: save_file = self.log_file_path + \"bash_history_log_\" + str(self.count) self.count += 1 with open(save_file, \"w+\") as f: for line in self.save_list: f.write(line.decode(\"utf-8\")+\"\\n\") self.save_list = [] print(\"*\"*10, \"写入文件\", \"*\"*10) if save_to_tb: if self.tb_empty: add_way = \"replace\" self.tb_empty = False else: add_way = \"append\" for k, v in msg_value.items(): msg_value[k] = [v] insert_df = pd.DataFrame.from_dict(msg_value) insert_df.to_sql(dbh_config[\"table\"], self.dbh, if_exists=add_way, index=False) print(\"当前size\", save_data_size, \"\\t\", msg_value)if __name__ == \"__main__\": Bash_history_consumer().bash_consumer() 五、开启任务 我是使用bcc_00作为唯一的消费者，bcc_00和bcc_01作为两个生产者，修改好代码中的配置数据 bcc_00开启消费者 1python bash_history_consumer.py bcc_00 和 bcc_01分别开启生产者 1python bash_history_producer.py ~&#x2F;.bash_history 新起窗口进行命令行操作，就可以在控制台和数据库看到相关bash 00_consumer: 00_producer: 01_producer: sql_table(两台bcc内网ip一样):","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/tags/kafka/"},{"name":"pyinotify","slug":"pyinotify","permalink":"http://yoursite.com/tags/pyinotify/"}]},{"title":"mac下chromedriver升级","slug":"mac下chromedriver升级","date":"2020-03-13T02:50:14.000Z","updated":"2020-03-13T02:50:14.554Z","comments":true,"path":"2020/03/13/mac下chromedriver升级/","link":"","permalink":"http://yoursite.com/2020/03/13/mac%E4%B8%8Bchromedriver%E5%8D%87%E7%BA%A7/","excerpt":"","text":"下载与本地chrome版本一致的chromedriver,下载地址[http://chromedriver.storage.googleapis.com/index.html] 解压到本地的 /usr/local/bin 检查版本 chromedirver -v","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/tags/%E5%B7%A5%E5%85%B7/"}]},{"title":"scala伴生类和伴生对象","slug":"scala伴生类和伴生对象","date":"2020-02-23T11:36:55.000Z","updated":"2020-02-23T11:36:55.191Z","comments":true,"path":"2020/02/23/scala伴生类和伴生对象/","link":"","permalink":"http://yoursite.com/2020/02/23/scala%E4%BC%B4%E7%94%9F%E7%B1%BB%E5%92%8C%E4%BC%B4%E7%94%9F%E5%AF%B9%E8%B1%A1/","excerpt":"","text":"Scala伴生类和伴生对象 定义： 在Scala中，类名和对象名一致时，互为伴生类和伴生对象。 例子:123456789// 伴生类class ApplyTest &#123;&#125;// 伴生对象object ApplyTest &#123;&#125; 使用: 通常在伴生对象中写apply()方法，然后在apply方法中实例伴生类，然后每次使用类名称()的时候，就已经实例化了类 例子:12345678910111213141516object ApplyApp &#123; def main(args:Array[String]):Unit = &#123; val c = ApplyTest() println(c) &#125;&#125;class ApplyTest &#123;&#125;object ApplyTest &#123; def apply() = &#123; new ApplyTest() &#125;&#125;","categories":[{"name":"Scala","slug":"Scala","permalink":"http://yoursite.com/categories/Scala/"}],"tags":[{"name":"伴生","slug":"伴生","permalink":"http://yoursite.com/tags/%E4%BC%B4%E7%94%9F/"}]}]}