{"meta":{"title":"onestarko","subtitle":"","description":"","author":"kowhoy","url":"http://yoursite.com","root":"/"},"pages":[{"title":"categories","date":"2020-02-15T00:58:48.000Z","updated":"2020-02-15T00:59:32.763Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2020-02-15T00:59:39.000Z","updated":"2020-02-15T01:57:12.301Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"MySQLé¢˜ï¼ˆ1ï¼‰","slug":"MySQLé¢˜ï¼ˆ1ï¼‰","date":"2020-07-28T06:11:56.000Z","updated":"2020-07-28T06:11:56.240Z","comments":true,"path":"2020/07/28/MySQLé¢˜ï¼ˆ1ï¼‰/","link":"","permalink":"http://yoursite.com/2020/07/28/MySQL%E9%A2%98%EF%BC%881%EF%BC%89/","excerpt":"","text":"MySQLé¢˜é—®é¢˜:1234567891011121314151617181920æœ‰ä¸¤ç±»åº”ç”¨ï¼Œæ¸¸æˆå’Œå·¥å…·ï¼Œæ¸¸æˆç±»çš„åº”ç”¨æœ‰ï¼šA1game1, Bg2ame2, Cga3me3, Dgam4e4, Egame5, Fgame6, ..., Tgame10å·¥å…·ç±»çš„åº”ç”¨æœ‰:A1tool1, Bt2ool2, Cto3ol3, Dtoo4l4, Etool5, Ftool6, Gtool7, Htool8æœ‰ä¸€å¼ ç”¨æˆ·å®‰è£…åº”ç”¨åˆ—è¡¨(è¡¨å: user_install_app)+---------+-----------------------------------------------------------+| user_id | app_name |+---------+-----------------------------------------------------------+| 1 | A1game1, Bg2ame2, Cga3me3, A1tool1, Cto3ol3, Gtool7 || 2 | A1game1, Fgame6, Cga3me3, Egame5, Bt2ool2, Ftool6, Gtool7 || 3 | Cga3me3 || 4 | Dtoo4l4 |+---------+-----------------------------------------------------------+1&gt; åŒæ—¶è£…æœ‰Cga3me3 å’Œ Bt2ool2çš„ç”¨æˆ·2&gt; åŒæ—¶è£…æœ‰æ¸¸æˆå’Œå·¥å…·ç±» (ä¸”æ²¡æœ‰å®‰è£…Cto3ol3) çš„ç”¨æˆ·åˆ†åˆ«å®‰è£…æ¸¸æˆå’Œå·¥å…·ç±»åº”ç”¨çš„ä¸ªæ•° æ€è·¯: åŸå§‹æ•°æ®æ˜¯æ¯ä¸ªç”¨æˆ·ä¸€è¡Œæ•°æ®ï¼Œåé¢çš„app_nameä½¿ç”¨â€™,â€™è¿›è¡Œäº†åˆ†éš”ï¼Œå¯ä»¥å…ˆå°†æ•°æ®è¿›è¡Œåˆ†æˆå¤šè¡Œæ•°æ®ï¼Œ æ–¹ä¾¿è¿›è¡Œç»Ÿè®¡ã€‚ è§£ç­”:1. å°†æ•°æ®åˆ†éš”æˆå¤šè¡Œ1234567891011create temporary table user_app (user_id int(11) not null, app_name varchar(50) not null);insert into user_app (SELECT a.user_id, trim( substring_index( SUBSTRING_INDEX( app_name, ',', help_topic_id + 1 ), ',', - 1 ) ) app_name FROM user_install_app a, mysql.help_topic b WHERE b.help_topic_id &lt; LENGTH(a.app_name) - length(REPLACE ( a.app_name, ',', '' )) + 1); 123456789101112131415161718192021select * from user_app;+---------+----------+| user_id | app_name |+---------+----------+| 1 | A1game1 || 1 | Bg2ame2 || 1 | Cga3me3 || 1 | A1tool1 || 1 | Cto3ol3 || 1 | Gtool7 || 2 | A1game1 || 2 | Fgame6 || 2 | Cga3me3 || 2 | Egame5 || 2 | Bt2ool2 || 2 | Ftool6 || 2 | Gtool7 || 3 | Cga3me3 || 4 | Dtoo4l4 |+---------+----------+ 2. é—®é¢˜ä¸€ï¼š åŒæ—¶è£…æœ‰Cga3me3 å’Œ Bt2ool2çš„ç”¨æˆ·1234567select user_id, count(*) num from user_app where app_name &#x3D; &#39;Cga3me3&#39; or app_name &#x3D; &#39;Bt2ool2&#39; group by user_id having num &gt; 1;+---------+-----+| user_id | num |+---------+-----+| 2 | 2 |+---------+-----+ 3. é—®é¢˜äºŒ: åŒæ—¶è£…æœ‰æ¸¸æˆå’Œå·¥å…·ç±» (ä¸”æ²¡æœ‰å®‰è£…Cto3ol3) çš„ç”¨æˆ·åˆ†åˆ«å®‰è£…æ¸¸æˆå’Œå·¥å…·ç±»åº”ç”¨çš„ä¸ªæ•°1234567891011121314151617181920212223242526272829303132333435363738391. å¢åŠ ä¸€åˆ—app_typecreate temporary table user_app_v2 (user_id int(11) not null, app_name varchar(50) not null, app_type varchar(20) not null);insert into user_app_v2 (select *, case when app_name like &#39;%g%&#39; then &#39;game&#39; else &#39;tool&#39; end as &#39;app_type&#39; from user_app);select * from user_app_v2;+---------+----------+----------+| user_id | app_name | app_type |+---------+----------+----------+| 1 | A1game1 | game || 1 | Bg2ame2 | game || 1 | Cga3me3 | game || 1 | A1tool1 | tool || 1 | Cto3ol3 | tool || 1 | Gtool7 | game || 2 | A1game1 | game || 2 | Fgame6 | game || 2 | Cga3me3 | game || 2 | Egame5 | game || 2 | Bt2ool2 | tool || 2 | Ftool6 | tool || 2 | Gtool7 | game || 3 | Cga3me3 | game || 4 | Dtoo4l4 | tool |+---------+----------+----------+2. ç»Ÿè®¡select user_id, count(distinct app_type) num from user_app_v2 where app_name !&#x3D; &#39;Cga3me3&#39; group by user_id having num &gt; 1;+---------+-----+| user_id | num |+---------+-----+| 1 | 2 || 2 | 2 |+---------+-----+","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"}]},{"title":"Pythonæ•°æ®ç»“æ„çš„æ€§èƒ½åˆ†æ ( Dict )","slug":"Pythonæ•°æ®ç»“æ„çš„æ€§èƒ½åˆ†æ-Dict","date":"2020-07-24T03:37:17.000Z","updated":"2020-07-24T03:37:17.518Z","comments":true,"path":"2020/07/24/Pythonæ•°æ®ç»“æ„çš„æ€§èƒ½åˆ†æ-Dict/","link":"","permalink":"http://yoursite.com/2020/07/24/Python%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%9A%84%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90-Dict/","excerpt":"","text":"Dictçš„æ€§èƒ½åˆ†æ æ“ä½œ æ—¶é—´æ•ˆç‡ å¤åˆ¶ O(n) è®¿é—® O(1) èµ‹å€¼ O(1) åˆ é™¤ O(1) åŒ…å«(in) O(1) è¿­ä»£ O(n) Dict å’Œ List æŸ¥æ‰¾æ•ˆç‡å¯¹æ¯” Listçš„æ—¶é—´æ•ˆç‡ä¸ºO(n), Dictçš„æ—¶é—´æ•ˆç‡ä¸ºO(1) éªŒè¯12345678910111213141516171819202122232425262728293031323334import timeitimport randomimport matplotlib.pyplot as pltfig, ax = plt.subplots()x_list = []ls_consume_list = []d_consume_list = []for i in range(10000, 100000001, 20000): x_list.append(i) t = timeit.Timer(\"random.randrange(%d) in x\" % i, \"from __main__ import random, x\") x = list(range(i)) ls_consume = t.timeit(1000) ls_consume_list.append(ls_consume) x = &#123;j: None for j in range(i)&#125; d_consume = t.timeit(1000) d_consume_list.append(d_consume) print(\"%d, %10.3f, %10.3f\" % (i, ls_consume, d_consume)) ax.cla() ax.plot(x_list, ls_consume_list, 'bo', 5) ax.plot(x_list, d_consume_list, 'r*', 5) plt.pause(0.1) ç»“è®º Listçš„æŸ¥è¯¢å…ƒç´ çš„å¤æ‚åº¦ä¸ºO(n)ï¼Œ DictæŸ¥è¯¢å…ƒç´ çš„å¤æ‚åº¦ä¸ºO(1)","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"æ•°æ®ç»“æ„","slug":"æ•°æ®ç»“æ„","permalink":"http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"Pythonæ•°æ®ç»“æ„çš„æ€§èƒ½åˆ†æ ( List )","slug":"Pythonæ•°æ®ç»“æ„çš„æ€§èƒ½åˆ†æ-List","date":"2020-07-24T02:41:39.000Z","updated":"2020-07-24T02:41:39.215Z","comments":true,"path":"2020/07/24/Pythonæ•°æ®ç»“æ„çš„æ€§èƒ½åˆ†æ-List/","link":"","permalink":"http://yoursite.com/2020/07/24/Python%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%9A%84%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90-List/","excerpt":"","text":"Listçš„æ€§èƒ½åˆ†æ å¯¹åˆ—è¡¨è¿›è¡Œæ‰©å……ï¼Œé€šå¸¸æœ‰ä¸¤ç§æ–¹å¼ã€‚1&gt; ä½¿ç”¨append 2&gt;ä½¿ç”¨ä¸²è”è¿ç®—ç¬¦+appendçš„å¤æ‚åº¦ä¸ºO(1), ä¸²è”è¿ç®—ç¬¦çš„å¤æ‚åº¦ä¸ºO(k), kä¸ºè¢«è¿æ¥åˆ—è¡¨çš„é•¿åº¦ ç”Ÿæˆç®€å•çš„åˆ—è¡¨æœ‰ä¸¤ç§æ–¹å¼ã€‚ 1&gt; [i for i in range(100)] 2&gt; list(range(100))list()æ–¹å¼çš„æ•ˆç‡æ›´é«˜ å¯¹æ•°æ®è¿›è¡Œpopæ“ä½œï¼Œ pop()çš„å¤æ‚åº¦ä¸ºO(1), pop(i) çš„å¤æ‚åº¦ä¸ºO(n) éªŒè¯ä¸€123456789101112131415161718192021222324252627282930313233import timeitfrom timeit import Timerdef test1(): l = [] for i in range(100): l = l + [i]def test2(): l = [] for i in range(100): l.append(i)def test3(): [i for i in range(100)]def test4(): list(range(100))t1 = Timer(\"test1()\", \"from __main__ import test1\")t2 = Timer(\"test2()\", \"from __main__ import test2\")t3 = Timer(\"test3()\", \"from __main__ import test3\")t4 = Timer(\"test4()\", \"from __main__ import test4\")print('test1: ', t1.timeit(10000))print('test2: ', t2.timeit(10000))print('test3: ', t3.timeit(10000))print('test4: ', t4.timeit(10000))# test1: 0.210552271# test2: 0.064808912# test3: 0.03384729600000003# test4: 0.01063408199999999 éªŒè¯äºŒ12345678910111213141516171819202122232425262728import timeitfrom timeit import Timerimport matplotlib.pyplot as pltpop_start = Timer(\"ls.pop(0)\", \"from __main__ import ls\")pop_end = Timer(\"ls.pop()\", \"from __main__ import ls\")fig, ax = plt.subplots()x_list = []pop_start_consume = []pop_end_consume = []for i in range(1000000, 100000001, 1000000): x_list.append(i) ls = list(range(i)) ps = pop_start.timeit(1000) ls = list(range(i)) pe = pop_end.timeit(1000) pop_start_consume.append(ps) pop_end_consume.append(pe) ax.cla() ax.plot(x_list, pop_start_consume, 'bo', 5) ax.plot(x_list, pop_end_consume, 'r*', 5) plt.pause(0.1) print(\"%15.5f, %15.5f\" % (ps, pe)) ç»“è®º åŒæ ·è¿›è¡Œ1wæ¬¡æ‰©å……æ“ä½œï¼Œappendçš„æ•ˆç‡æ¯”+å¿«äº†3ï½4å€ åŒæ ·è¿›è¡Œ1wæ¬¡åˆ›å»ºåˆ—è¡¨çš„æ“ä½œï¼Œ list()æ–¹å¼çš„é€Ÿåº¦å¿«äº†3å€ pop()çš„å¤æ‚åº¦ä¸ºO(1)ï¼Œ pop(i)çš„å¤æ‚åº¦ä¸ºO(n)","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"æ•°æ®ç»“æ„","slug":"æ•°æ®ç»“æ„","permalink":"http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"PageRank(1)","slug":"PageRank-1","date":"2020-07-17T10:01:31.000Z","updated":"2020-07-17T10:01:31.064Z","comments":true,"path":"2020/07/17/PageRank-1/","link":"","permalink":"http://yoursite.com/2020/07/17/PageRank-1/","excerpt":"","text":"1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192# -*- coding: utf-8 -*-# @Description:# @Name: pages_rank# @Author: kowhoy# @Date: 2020/7/17import numpy as npimport itertoolsclass PageRank: def __init__(self): self.nodes_dict = &#123;&#125; self.nodes_set = set() self.network_path = './pages.data' ''' * @Desc: æ ¹æ®æ–‡ä»¶è®¾ç½®å¯¹è±¡ * @Date: 5:51 ä¸‹åˆ 2020/7/17 ''' def _set_nodes(self): with open(self.network_path, 'r') as f: lines = f.readlines() for line in lines: line_arr = line.split(' -&gt; ') to_ch = line_arr[0] from_ch = line_arr[1].replace('\\n', '') self.nodes_set.add(from_ch) self.nodes_set.add(to_ch) if from_ch not in self.nodes_dict: self.nodes_dict[from_ch] = &#123;'from': 0, 'to': 0, 'from_chs': []&#125; if to_ch not in self.nodes_dict: self.nodes_dict[to_ch] = &#123;'from': 0, 'to': 0, 'from_chs': []&#125; self.nodes_dict[from_ch]['from'] += 1 self.nodes_dict[to_ch]['to'] += 1 self.nodes_dict[from_ch]['from_chs'].append(to_ch) ''' * @Desc: æ ¹æ®èŠ‚ç‚¹å¯¹è±¡è®¡ç®—æ¦‚ç‡çŸ©é˜µ * @Date: 5:52 ä¸‹åˆ 2020/7/17 ''' def _set_matrix(self): nodes_list = sorted(self.nodes_set) combines = itertools.product(nodes_list, nodes_list) matrix = np.zeros([len(nodes_list), len(nodes_list)]) for combine in combines: tc = combine[0] fc = combine[1] loc = nodes_list.index(tc), nodes_list.index(fc) v = 0 if fc in self.nodes_dict[tc]['from_chs']: v = 1 / self.nodes_dict[fc]['to'] matrix[loc[0], loc[1]] = v return matrix ''' * @Desc: è®¡ç®—PR * @Date: 5:53 ä¸‹åˆ 2020/7/17 ''' def _compute_page_rank(self, rate, matrix, threshold): nodes_len = len(self.nodes_set) r0 = np.zeros(nodes_len) r1 = np.ones(nodes_len) while np.sum(np.abs(r0 - r1)) &gt;= threshold: r0 = r1.copy() r1 = (1 - rate) / nodes_len + rate * np.dot(matrix, r1) print(r1) return r1 def main(self): self._set_nodes() matrix = self._set_matrix() result = self._compute_page_rank(0.5, matrix, .0001) print(\"result:\", result)if __name__ == '__main__': PageRank().main()","categories":[{"name":"ç®—æ³•","slug":"ç®—æ³•","permalink":"http://yoursite.com/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"PageRank","slug":"PageRank","permalink":"http://yoursite.com/tags/PageRank/"}]},{"title":"Pandasçš„DataFrameæ•°æ®æŒä¹…åŒ–æ–¹æ¡ˆæ¯”è¾ƒ","slug":"Pandasçš„DataFrameæ•°æ®æŒä¹…åŒ–æ–¹æ¡ˆæ¯”è¾ƒ","date":"2020-07-13T04:04:43.000Z","updated":"2020-07-13T05:32:39.537Z","comments":true,"path":"2020/07/13/Pandasçš„DataFrameæ•°æ®æŒä¹…åŒ–æ–¹æ¡ˆæ¯”è¾ƒ/","link":"","permalink":"http://yoursite.com/2020/07/13/Pandas%E7%9A%84DataFrame%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96%E6%96%B9%E6%A1%88%E6%AF%94%E8%BE%83/","excerpt":"","text":"Pandasçš„DataFrameæ•°æ®æŒä¹…åŒ–æ–¹æ¡ˆæ¯”è¾ƒ ç»Ÿä¸€ä½¿ç”¨30ä¸ªå­—æ®µï¼Œ44wè¡Œçš„è®¢å•è¡¨ ä¸€ã€ä½¿ç”¨CSVæ–‡ä»¶123456789101112131415import pandas as pdfrom sqlalchemy import create_engineimport oscache_file = \"./cache.csv\"if os.path.exists(cache_file): df = pd.read_csv(cache_file)else: dbh = create_engine(\"mysql+pymysql://user:passwd@ip/db\") df = pd.read_sql_query(\"select * from orders\", dbh) df.to_csv(cache_file, encoding=\"utf8\", index=False) print(df.head(10)) é¦–æ¬¡è€—æ—¶ï¼š37.85sè¯»å–è€—æ—¶ï¼š3.38s äºŒã€ä½¿ç”¨pickleå¯¹è±¡å­˜å‚¨123456789101112131415161718import pandas as pdimport picklefrom sqlalchemy import create_engineimport oscache_file = \"./cache\"if os.path.exists(cache_file): with open(cache_file, \"rb\") as f: df = pickle.load(df)else: dbh = create_engine(\"mysql+pymysql://user:passwd@ip/db\") df = pd.read_sql_query(\"select * from order_profit\", dbh) with open(cache_file, \"wb\") as f: pickle.dump(df, f) print(df.head(10)) é¦–æ¬¡è€—æ—¶ï¼š24.86sè¯»å–è€—æ—¶ï¼š1.11s ä¸‰ã€ä½¿ç”¨HDFStoreå­˜å‚¨12345678910111213141516import pandas as pdfrom sqlalchemy import create_enginecache_file = \"./orders.h5\"store = pd.HDFStore(cache_file)if \"orders\" in store: df = store[\"orders\"]else: dbh = create_engine(\"mysql+pymysql://user:passwd@ip/db\") df = pd.read_sql_query(\"select * from order_profit\", dbh) store[\"orders\"] = df print(df.head(10)) é¦–æ¬¡è€—æ—¶ï¼š24.85sè¯»å–è€—æ—¶: 1.78s å››ã€æ€»ç»“ å¯¹äºPandasçš„DataFrameè¿›è¡ŒæŒä¹…åŒ–ç¼“å­˜ï¼Œä½¿ç”¨äº†ä¸Šé¢ä¸‰ç§æ–¹å¼è¿›è¡Œæ¯”è¾ƒï¼Œé™¤æ­¤ä¹‹å¤–è¿˜å°è¯•äº†ä½¿ç”¨Alluxioçš„å­˜å‚¨æ–¹å¼ï¼Œå·®å¼‚ä¸å¤§ã€‚æ¯”è¾ƒå¾—å‡ºä½¿ç”¨å¯¹è±¡å­˜å‚¨é€Ÿåº¦æœ€å¿«ã€‚","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"Pandas","slug":"Pandas","permalink":"http://yoursite.com/tags/Pandas/"}]},{"title":"SparkSQL çª—å£å‡½æ•°","slug":"SparkSQL-çª—å£å‡½æ•°","date":"2020-07-10T08:43:05.000Z","updated":"2020-07-10T08:43:05.858Z","comments":true,"path":"2020/07/10/SparkSQL-çª—å£å‡½æ•°/","link":"","permalink":"http://yoursite.com/2020/07/10/SparkSQL-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0/","excerpt":"","text":"SparkSQL çª—å£å‡½æ•°ä¸€ã€çª—å£å‡½æ•°çš„å®šä¹‰å’Œä¸å…¶ä»–å‡½æ•°çš„åŒºåˆ« æ™®é€šå‡½æ•° : ä½œç”¨äºæ¯ä¸€æ¡è®°å½•ï¼Œä¾èµ–äºæœ¬æ¡è®°å½•æ¥è®¡ç®—å‡ºä¸€ä¸ªæ–°åˆ—ï¼Œè®°å½•æ•°ä¸å˜ èšåˆå‡½æ•° : ä½œç”¨äºä¸€ç»„è®°å½•ï¼Œè®¡ç®—å‡ºä¸€ä¸ªèšåˆå€¼ï¼Œè®°å½•æ•°å˜å°‘ çª—å£å‡½æ•° : ä½œç”¨äºæ¯ä¸€æ¡è®°å½•ï¼Œä¾èµ–äºçª—å£ä¸­çš„è®°å½•æ¥è®¡ç®—å‡ºä¸€ä¸ªæ–°åˆ—ï¼Œè®°å½•æ•°ä¸å˜ äºŒã€çª—å£å‡½æ•°è¯­æ³•ç»“æ„12345&lt;çª—å£å‡½æ•°&gt;([å‚æ•°]) over ([partition by &lt;åˆ†ç»„åˆ—&gt;][order by &lt;æ’åºåˆ—&gt;] [asc&#x2F;desc](rows | range) &lt;èŒƒå›´æ¡ä»¶&gt;) 1. çª—å£å‡½æ•° ranking functions: æ’åºç±»å‡½æ•°, row_number(), rank(), dense_rank(), percent_rank(), ntile(n) analytic functions: ç»Ÿè®¡æ¯”è¾ƒç±»å‡½æ•°, cume_dist, lag, lead, first_value, last_value aggregate functions: èšåˆç±»å‡½æ•°, avg, sum, min, max 2. over å…³é”®å­—ï¼Œè¯´æ˜è¿™æ˜¯ä¸€ä¸ªçª—å£å‡½æ•° 3. partition by åˆ†ç»„4. order by æ’åº5.rows | ranges æ§åˆ¶çª—å£è¾¹ç•Œ rows: è¡¨ç¤ºç‰©ç†çª—å£ï¼Œå°±æ˜¯æ’åºä¹‹åçš„index range: æ˜¯æ ¹æ®è®°å½•ä¸­å…·ä½“çš„å­—æ®µå€¼è¿›è¡Œæ¯”è¾ƒç¡®å®šè¾¹ç•Œ ä¸‰ã€Ranking Functions func guide row_number ç»„å†…åŠ index rank ç»„å†…æ’åï¼Œä¸ä¼šè·³è¿‡å¹¶åˆ—çš„æƒ…å†µï¼Œæ¯”å¦‚ä¸¤ä¸ªå¹¶åˆ—ç¬¬ä¸€ï¼Œåˆ™æ¥ä¸‹æ¥çš„rankä¸º3ï¼Œè€Œä¸æ˜¯2 dense_rank åŒrankï¼Œä¼šè·³è¿‡å¹¶åˆ—çš„æƒ…å†µï¼Œä¸¤ä¸ªå¹¶åˆ—ç¬¬ä¸€ï¼Œæ¥ä¸‹æ¥çš„rankä¸º2 percent_rank (ç»„å†…æ’å - 1) / (ç»„å†…è¡Œæ•° - 1) ntile(n) å°†ç»„å†…æ•°æ®åˆ†æˆnæ¡¶ï¼Œè¯¥æ¡è®°å½•æ‰€åœ¨çš„æ¡¶å·(ä»1è®¡æ•°) 12345678spark.sql( s\"\"\" |select name, department, salary, row_number() over (partition by department order by salary desc) as row_number |, rank() over (partition by department order by salary desc) as rank |, dense_rank() over (partition by department order by salary desc) as dense_rank |, ntile(2) over (partition by department order by salary desc) as ntile |from salary |\"\"\".stripMargin).show() 123456789101112131415+--------+----------+------+----------+----+----------+-----+| name|department|salary|row_number|rank|dense_rank|ntile|+--------+----------+------+----------+----+----------+-----+| Berni| Sales| 4700| 1| 1| 1| 1|| Tom| Sales| 4500| 2| 2| 2| 1||Guoxiang| Sales| 4200| 3| 3| 3| 1|| Georgi| Sales| 4200| 4| 3| 3| 2|| Kyoichi| Sales| 3000| 5| 5| 4| 2|| Berni| Sales| 0| 6| 6| 5| 2|| Sumant| Finance| 3900| 1| 1| 1| 1|| Anneke| Finance| 3300| 2| 2| 2| 1|| Parto| Finance| 2700| 3| 3| 3| 2|| Jeff| Marketing| 3100| 1| 1| 1| 1||Patricio| Marketing| 2500| 2| 2| 2| 2|+--------+----------+------+----------+----+----------+-----+ å››ã€Analytic Functions func guide cume_dist ç»„å†… &lt;= æœ¬æ¡æ•°å€¼çš„è¡Œæ•° / ç»„å†…æ€»è¡Œæ•° lag(col, [n, default]) å½“å‰çš„index &lt; n çš„è¯ï¼Œ è¿”å› default(é»˜è®¤ä¸ºnull), å¦åˆ™è¿”å› colçš„å€¼ lead ç±»ä¼¼lagï¼Œè¶…å‰ first_value å–åˆ†ç»„å†…æ’åºåï¼Œæˆªæ­¢åˆ°å½“å‰è¡Œï¼Œç¬¬ä¸€ä¸ªå€¼ last_value å–åˆ†ç»„å†…æ’åºå, æˆªæ­¢åˆ°å½“å‰è¡Œ, æœ€åä¸€ä¸ªå€¼ 12345678910spark.sql( \"\"\" |select name, department, salary, cume_dist() over (partition by department order by salary) as cume_dist |, lag(salary, 1) over (partition by department order by salary) as lag |, lag(salary, 2) over (partition by department order by salary) as lag_2 |, lead(salary, 1) over (partition by department order by salary) as lead_1 |, first_value(salary) over (partition by department order by salary) as first_value |, last_value(salary) over (partition by department order by salary) as last_value |from salary |\"\"\".stripMargin).show() 123456789101112131415+--------+----------+------+-------------------+----+-----+------+-----------+----------+| name|department|salary| cume_dist| lag|lag_2|lead_1|first_value|last_value|+--------+----------+------+-------------------+----+-----+------+-----------+----------+| Berni| Sales| 0|0.16666666666666666|null| null| 3000| 0| 0|| Kyoichi| Sales| 3000| 0.3333333333333333| 0| null| 4200| 0| 3000||Guoxiang| Sales| 4200| 0.6666666666666666|3000| 0| 4200| 0| 4200|| Georgi| Sales| 4200| 0.6666666666666666|4200| 3000| 4500| 0| 4200|| Tom| Sales| 4500| 0.8333333333333334|4200| 4200| 4700| 0| 4500|| Berni| Sales| 4700| 1.0|4500| 4200| null| 0| 4700|| Parto| Finance| 2700| 0.3333333333333333|null| null| 3300| 2700| 2700|| Anneke| Finance| 3300| 0.6666666666666666|2700| null| 3900| 2700| 3300|| Sumant| Finance| 3900| 1.0|3300| 2700| null| 2700| 3900||Patricio| Marketing| 2500| 0.5|null| null| 3100| 2500| 2500|| Jeff| Marketing| 3100| 1.0|2500| null| null| 2500| 3100|+--------+----------+------+-------------------+----+-----+------+-----------+----------+ äº”ã€Aggregate Functions func guide sum æ±‚å’Œï¼Œä¸æŒ‡å®šè¾¹ç•ŒæŒ‰å€¼è¿›è¡Œç´¯è®¡æ±‚å’Œ avg æ±‚å¹³å‡, åŒä¸Š max æ±‚æœ€å¤§, åŒä¸Š min æ±‚æœ€å°, åŒä¸Š 123456789spark.sql( \"\"\" |select |name, department, salary, sum(salary) over (partition by department order by salary) as sum, |avg(salary) over (partition by department order by salary) as avg, |min(salary) over (partition by department order by salary) as min, |max(salary) over (partition by department order by salary) as max |from salary |\"\"\".stripMargin).show() 123456789101112131415+--------+----------+------+-----+------------------+----+----+| name|department|salary| sum| avg| min| max|+--------+----------+------+-----+------------------+----+----+| Berni| Sales| 0| 0| 0.0| 0| 0|| Kyoichi| Sales| 3000| 3000| 1500.0| 0|3000||Guoxiang| Sales| 4200|11400| 2850.0| 0|4200|| Georgi| Sales| 4200|11400| 2850.0| 0|4200|| Tom| Sales| 4500|15900| 3180.0| 0|4500|| Berni| Sales| 4700|20600|3433.3333333333335| 0|4700|| Parto| Finance| 2700| 2700| 2700.0|2700|2700|| Anneke| Finance| 3300| 6000| 3000.0|2700|3300|| Sumant| Finance| 3900| 9900| 3300.0|2700|3900||Patricio| Marketing| 2500| 2500| 2500.0|2500|2500|| Jeff| Marketing| 3100| 5600| 2800.0|2500|3100|+--------+----------+------+-----+------------------+----+----+ å…­ã€çª—å£è¾¹ç•Œ ROWS ï½œ RANGE æ§åˆ¶çª—å£è¾¹ç•Œ ROWS: è¡¨ç¤ºç‰©ç†çª—å£ï¼Œå°±æ˜¯ä¾æ®æ’åºåçš„indexæ¥åˆ’åˆ† RANGES: è¡¨ç¤ºé€»è¾‘çª—å£ï¼Œæ˜¯æ ¹æ®è®°å½•å­—æ®µçš„å®é™…å€¼è¿›è¡Œæ¯”è¾ƒåˆ’åˆ†çš„ è¯­æ³•ï¼š over (partition by â€¦ order by â€¦ between {start} and {end}) äº”ç§è¾¹ç•Œ current row : å½“å‰è¡Œ unbounded preceding : åˆ†åŒºç¬¬ä¸€è¡Œ unbounded following : åˆ†åŒºæœ€åä¸€è¡Œ n preceding : å½“å‰è¡Œå‘å‰nè¡Œ n following : å½“å‰è¡Œå‘ånè¡Œ unbounded : èµ·ç‚¹ 1234567spark.sql( \"\"\" |select |name, department, salary, sum(salary) over (partition by department order by salary rows between 1 preceding and 2 following) as rows, |sum(salary) over (partition by department order by salary range between 1 preceding and 2 following) as range |from salary |\"\"\".stripMargin).show() 123456789101112131415+--------+----------+------+-----+-----+| name|department|salary| rows|range|+--------+----------+------+-----+-----+| Berni| Sales| 0| 7200| 0|| Kyoichi| Sales| 3000|11400| 3000||Guoxiang| Sales| 4200|15900| 8400|| Georgi| Sales| 4200|17600| 8400|| Tom| Sales| 4500|13400| 4500|| Berni| Sales| 4700| 9200| 4700|| Parto| Finance| 2700| 9900| 2700|| Anneke| Finance| 3300| 9900| 3300|| Sumant| Finance| 3900| 7200| 3900||Patricio| Marketing| 2500| 5600| 2500|| Jeff| Marketing| 3100| 5600| 3100|+--------+----------+------+-----+-----+","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"çª—å£å‡½æ•°","slug":"çª—å£å‡½æ•°","permalink":"http://yoursite.com/tags/%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0/"},{"name":"SparkSQL","slug":"SparkSQL","permalink":"http://yoursite.com/tags/SparkSQL/"}]},{"title":"SparkStreamingæµå¤„ç†åˆ†æè®¢å•æ—¥å¿—","slug":"SparkStreamingæµå¤„ç†åˆ†æè®¢å•æ—¥å¿—","date":"2020-07-09T03:06:17.000Z","updated":"2020-07-09T03:06:17.576Z","comments":true,"path":"2020/07/09/SparkStreamingæµå¤„ç†åˆ†æè®¢å•æ—¥å¿—/","link":"","permalink":"http://yoursite.com/2020/07/09/SparkStreaming%E6%B5%81%E5%A4%84%E7%90%86%E5%88%86%E6%9E%90%E8%AE%A2%E5%8D%95%E6%97%A5%E5%BF%97/","excerpt":"","text":"SparkStreamingæµå¤„ç†åˆ†æè®¢å•æ—¥å¿—ä¸€ã€é¡¹ç›®è¯´æ˜ è®¢å•æ•°æ®å‘é€åˆ°Kafkaä¸­ï¼ŒSparkStreamingæ¶ˆè´¹Kafkaæ•°æ®ï¼Œä»¥å¤©/æ—¶/åˆ†é’Ÿçš„ç»´åº¦ç»Ÿè®¡ä¸‹å•å•æ•°ã€æˆäº¤å•æ•°ã€æˆäº¤é‡‘é¢ï¼Œå°†ç»“æœå†™å…¥åˆ°Redisä¸­ äºŒã€æ¨¡å—åˆ†è§£ åŸºç¡€æ¨¡å—ï¼ˆé…ç½®é¡¹ç±»ã€Redisè¿æ¥æ± ï¼‰ mock è®¢å•JSONæ•°æ®å‘é€åˆ°Kafka æ¶ˆè´¹Kafkaè¿›è¡Œç»Ÿè®¡ï¼Œä½¿ç”¨Zookeeperå’ŒKafkaä¸¤ç§æ–¹å¼è¿›è¡ŒOffsetç®¡ç† å‚¨å­˜ç»Ÿè®¡ç»“æœåˆ°Redis ä¸‰ã€åŸºç¡€æ¨¡å—1. åŸºç¡€é¡¹ç±» ä½¿ç”¨com.typesafe:config å·¥å…·ç±» 123456789101112131415161718/*** * @Desc æ ¹æ®é”®å€¼è·å–é…ç½®é¡¹ * @Date 9:36 ä¸Šåˆ 2020/7/3 * @Param [key] * @Return java.lang.String **/ def getConfigFiled(key: String): String = &#123; val conf = ConfigFactory.load() var value: String = null; try &#123; value = conf.getString(key) &#125;catch &#123; case _:Exception =&gt; System.err.println(key + \" not exists\") &#125; value &#125; 2. Redisè¿æ¥æ± 1234567891011121314151617/*** * @Desc è·å–REDISè¿æ¥ * @Date 11:23 ä¸Šåˆ 2020/7/7 * @Param [] * @Return redis.clients.jedis.Jedis **/ def getRedis(): Jedis = &#123; val poolConfig = new GenericObjectPoolConfig() poolConfig.setMaxIdle(10) poolConfig.setMaxTotal(1000) lazy val jedisPool = new JedisPool(poolConfig, CommonUtil.getConfigFiled(\"REDIS_HOST\")) val jedis = jedisPool.getResource jedis.select(CommonUtil.getConfigFiled(\"REDIS_DB\").toInt) jedis &#125; å››ã€Mock JSONè®¢å•æ•°æ®åˆ°Kafka1. åˆ›å»ºKafkaProducer123456789101112131415val props = new Properties() props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\") props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\") props.put(\"bootstrap.servers\", CommonUtil.getConfigFiled(\"KAFKA_BROKER_LIST\")) props.put(\"request.required.acks\", \"1\")// request.required.acksåƒæ•¸èªªæ˜:// 0 è¡¨ç¤º producerä¸ç­‰å¾…brokeråŒæ­¥å®Œæˆç¢ºèªä¹‹å¾Œç™¼é€ä¸‹ä¸€æ¢æ•¸æ“š// 1 è¡¨ç¤º produceråœ¨leaderå·²æˆåŠŸæ”¶åˆ°æ•¸æ“šå®Œæˆç¢ºèªä¹‹å¾Œç™¼é€ä¸‹ä¸€æ¢æ•¸æ“š// -1 è¡¨ç¤º produceråœ¨followerå‰¯æœ¬ä¸­ç¢ºèªæ”¶åˆ°æ•¸æ“šå¾Œä¹‹å¾Œç™¼é€ä¸‹ä¸€æ¢æ•¸æ“š//// 0 -&gt; 1 -&gt; -1 æ€§èƒ½é™ä½ã€æ•¸æ“šå¥å£¯æ€§å¢å¼· val producer = new KafkaProducer[String, String](props) 2. éšæœºç”Ÿæˆè®¢å•æ•°æ®å¹¶å‘é€123456789101112131415161718192021222324val random = new Random()val dateFormat = FastDateFormat.getInstance(\"yyyy-MM-dd HH:mm:ss\")val topic = CommonUtil.getConfigFiled(\"KAFKA_TOPIC\")for (i &lt;- 0 to 9) &#123; val time = dateFormat.format(new Date()) val userId = random.nextInt(1000).toString val courseId = random.nextInt(500).toString val fee = random.nextInt(500).toString val result = Array(\"0\", \"1\") val flag = result(random.nextInt(2)) val orderId = UUID.randomUUID().toString val map = new util.HashMap[String, Object]() map.put(\"time\", time) map.put(\"userId\", userId) map.put(\"courseId\", courseId) map.put(\"fee\", fee) map.put(\"flag\", flag) map.put(\"orderId\", orderId) val json = new JSONObject(map) producer.send(new ProducerRecord[String, String](topic, json.toString())) 3. æ‰“åŒ…å¹¶å®šæ—¶æ‰§è¡Œ123456#!/bin/bash/Users/k/soft/spark-2.4.5-bin-hadoop2.7/bin/spark-submit \\--class com.kowhoy.Utils.KafkaProducerApp \\--jars $(echo /Users/k/IdeaProjects/StreamingAnalysis/out/artifacts/StreamingAnalysis_jar/*.jar | tr ' ', ',') \\--packages com.typesafe:config:1.4.0 \\ /Users/k/IdeaProjects/StreamingAnalysis/out/artifacts/StreamingAnalysis_jar/StreamingAnalysis.jar 4. å¯åœ¨ç»ˆç«¯æŸ¥çœ‹æ¶ˆè´¹æ•°æ®12345678 kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic logtopic&#123;&quot;flag&quot;:&quot;1&quot;,&quot;orderId&quot;:&quot;7085f2be-3d2e-4e23-b53d-2fe39d879c06&quot;,&quot;fee&quot;:&quot;84&quot;,&quot;time&quot;:&quot;2020-07-09 09:58:03&quot;,&quot;userId&quot;:&quot;135&quot;,&quot;courseId&quot;:&quot;178&quot;&#125;&#123;&quot;flag&quot;:&quot;1&quot;,&quot;orderId&quot;:&quot;fc0c55fd-1591-4b3f-bf5d-50a771996709&quot;,&quot;fee&quot;:&quot;21&quot;,&quot;time&quot;:&quot;2020-07-09 09:58:03&quot;,&quot;userId&quot;:&quot;190&quot;,&quot;courseId&quot;:&quot;437&quot;&#125;&#123;&quot;flag&quot;:&quot;0&quot;,&quot;orderId&quot;:&quot;54c28598-fc98-480f-b6e1-8e1b7ed750fc&quot;,&quot;fee&quot;:&quot;343&quot;,&quot;time&quot;:&quot;2020-07-09 09:58:03&quot;,&quot;userId&quot;:&quot;986&quot;,&quot;courseId&quot;:&quot;415&quot;&#125;&#123;&quot;flag&quot;:&quot;1&quot;,&quot;orderId&quot;:&quot;127a06d6-af95-413c-84fb-23cf7727e0a9&quot;,&quot;fee&quot;:&quot;218&quot;,&quot;time&quot;:&quot;2020-07-09 09:58:03&quot;,&quot;userId&quot;:&quot;53&quot;,&quot;courseId&quot;:&quot;291&quot;&#125;&#123;&quot;flag&quot;:&quot;1&quot;,&quot;orderId&quot;:&quot;91bb49aa-acf7-4cb6-ad6f-b298e33873ed&quot;,&quot;fee&quot;:&quot;421&quot;,&quot;time&quot;:&quot;2020-07-09 09:58:03&quot;,&quot;userId&quot;:&quot;313&quot;,&quot;courseId&quot;:&quot;22&quot;&#125;&#123;&quot;flag&quot;:&quot;1&quot;,&quot;orderId&quot;:&quot;579b7686-a07c-49de-8546-300dcf3fd3f8&quot;,&quot;fee&quot;:&quot;136&quot;,&quot;time&quot;:&quot;2020-07-09 09:58:03&quot;,&quot;userId&quot;:&quot;304&quot;,&quot;courseId&quot;:&quot;265&quot;&#125;&#123;&quot;flag&quot;:&quot;1&quot;,&quot;orderId&quot;:&quot;6bedec73-5c02-4808-829c-2504226ffab2&quot;,&quot;fee&quot;:&quot;242&quot;,&quot;time&quot;:&quot;2020-07-09 09:58:03&quot;,&quot;userId&quot;:&quot;203&quot;,&quot;courseId&quot;:&quot;171&quot;&#125; äº”ã€æ¶ˆè´¹Kafkaç»Ÿè®¡æ•°æ®1. Kafkaè®¾ç½®12345678val kafkaParams = Map[String, Object]( \"bootstrap.servers\" -&gt; CommonUtil.getConfigFiled(\"KAFKA_BROKER_LIST\"), \"key.deserializer\" -&gt; classOf[StringDeserializer], \"value.deserializer\" -&gt; classOf[StringDeserializer], \"group.id\" -&gt; groupId, \"auto.offset.reset\" -&gt; \"earliest\", \"enable.auto.commit\" -&gt; (false: java.lang.Boolean) // å…³é—­è‡ªåŠ¨æäº¤ ) 2. ä½¿ç”¨Zookeeperç®¡ç†Offset â€”- è®¾ç½®fromOffset123456789101112131415161718192021222324252627282930313233343536val topic = CommonUtil.getConfigFiled(\"KAFKA_TOPIC\")val topics = List(topic)val topicDirs = new ZKGroupTopicDirs(groupId, topic)val zkTopicPath = topicDirs.consumerOffsetDirval zkClient = new ZkClient(CommonUtil.getConfigFiled(\"ZK_HOST\"))val children = zkClient.countChildren(zkTopicPath)val streamRDD = if (children &gt; 0) &#123; println(\"å·²ç»æ¶ˆè´¹è¿‡...\") var fromOffsets = Map[TopicPartition, Long]() for (partitionId &lt;- 0 until children) &#123; val offset = zkClient.readData[String](zkTopicPath + \"/\" + partitionId) fromOffsets += (new TopicPartition(topic, partitionId) -&gt; offset.toLong) &#125; KafkaUtils.createDirectStream( ssc, LocationStrategies.PreferConsistent, ConsumerStrategies.Assign[String, String](fromOffsets.keys.toList, kafkaParams, fromOffsets) )&#125; else &#123; println(\"ç¬¬ä¸€æ¬¡è¿›è¡Œæ¶ˆè´¹...\") KafkaUtils.createDirectStream( ssc, LocationStrategies.PreferConsistent, ConsumerStrategies.Subscribe[String, String](topics, kafkaParams) )&#125; 3. ä½¿ç”¨Zookeeperç®¡ç†Offset â€”- ä¿å­˜untilOffset12345678910val dateFormat = FastDateFormat.getInstance(\"yyyy-MM-dd HH:mm:ss\") streamRDD.foreachRDD(rdd =&gt; &#123; val offsetRange = rdd.asInstanceOf[HasOffsetRanges].offsetRanges ...... for (o &lt;- offsetRange) &#123; // ğŸš©ğŸš©ğŸš© ZkUtils(zkClient, false).updatePersistentPath(zkTopicPath +\"/\"+o.partition, o.untilOffset.toString) &#125; &#125;) 4. ä½¿ç”¨Kafkaè‡ªå¸¦Offsetç®¡ç†ï¼Œéœ€è¦æ‰‹åŠ¨æäº¤Offset12345678val timeFormat = FastDateFormat.getInstance(\"yyyy-MM-dd HH:mm:ss\") streamRDD.foreachRDD(rdd =&gt; &#123; val offsetRange = rdd.asInstanceOf[HasOffsetRanges].offsetRanges ..... //ğŸš©ğŸš©ğŸš© streamRDD.asInstanceOf[CanCommitOffsets].commitAsync(offsetRange) &#125;) 5. ç»Ÿè®¡æ•°æ®å¹¶ä¿å­˜åˆ°Redis12345678910111213141516171819202122232425262728streamRDD.foreachRDD(rdd =&gt; &#123; val data = rdd.map(x =&gt; JSON.parseObject(x.value)) .map(log =&gt; &#123; val flag = log.getString(\"flag\") val fee = log.getLong(\"fee\") val time = log.getString(\"time\") val day = time.substring(0, 10) val hour = time.substring(11, 13) val minute = time.substring(14, 16) val success:(Long, Long) = if (flag == \"1\") (1, fee) else (0, 0) (day, hour, minute, List[Long](1, success._1, success._2)) &#125;) data.map(x =&gt; (x._1, x._4)).reduceByKey((a, b) =&gt; &#123; a.zip(b).map(x =&gt; x._1 + x._2) &#125;).foreachPartition(part =&gt; &#123; val jedis = CommonUtil.getRedis() part.foreach(x =&gt; &#123; jedis.hincrBy(\"n-ko-\"+x._1, \"total\", x._2(0)) jedis.hincrBy(\"n-ko-\"+x._1, \"success\", x._2(1)) jedis.hincrBy(\"n-ko-\"+x._1, \"fee\", x._2(2)) &#125;) println(\"saved at \" + timeFormat.format(new Date())) &#125;)&#125;) å…­ã€é¡¹ç›®å®Œæ•´ä»£ç  âš ï¸ éœ€è¦åœ¨/src/main ä¸‹åˆ›å»ºresources/application.conf 12345678HBASE_ROOT_DIR&#x3D;&quot;alluxio:&#x2F;&#x2F;ip:port&#x2F;hbase&quot;KAFKA_BROKER_LIST&#x3D;&quot;ip:port&quot;KAFKA_GROUP_ID&#x3D;&quot;xxx&quot;KAFKA_TOPIC&#x3D;&quot;xxx&quot;REDIS_HOST&#x3D;&quot;xxx&quot;REDIS_DB&#x3D;&quot;1&quot;ZK_HOST&#x3D;&quot;ip:port&quot; å®Œæ•´ä»£ç  ä¸ƒã€è¡¥å……SparkStreamingæ•´åˆKafkaï¼Œä¸åŒç‰ˆæœ¬çš„ä½¿ç”¨å·®å¼‚","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://yoursite.com/tags/Kafka/"},{"name":"SparkStreaming","slug":"SparkStreaming","permalink":"http://yoursite.com/tags/SparkStreaming/"},{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}]},{"title":"è¿è¡ŒäºAlluxioä¹‹ä¸Šçš„Sparkä»»åŠ¡","slug":"è¿è¡ŒäºAlluxioä¹‹ä¸Šçš„Sparkä»»åŠ¡","date":"2020-07-03T06:12:37.000Z","updated":"2020-07-03T06:12:37.287Z","comments":true,"path":"2020/07/03/è¿è¡ŒäºAlluxioä¹‹ä¸Šçš„Sparkä»»åŠ¡/","link":"","permalink":"http://yoursite.com/2020/07/03/%E8%BF%90%E8%A1%8C%E4%BA%8EAlluxio%E4%B9%8B%E4%B8%8A%E7%9A%84Spark%E4%BB%BB%E5%8A%A1/","excerpt":"","text":"è¿è¡ŒäºAlluxioä¹‹ä¸Šçš„Sparkä»»åŠ¡Sparké…ç½® ${SPARK_HOME}/conf/spark-defaults.conf å¢åŠ é…ç½®spark.driver.extraClassPath //client/alluxio-1.8.2-client.jarspark.executor.extraClassPath //client/alluxio-1.8.2-client.jar é…ç½®æ›´æ”¹ application.conf123HBASE_ROOT_DIR&#x3D;&quot;alluxio:&#x2F;&#x2F;localhost:19998&#x2F;hbase&quot;ZOOKEEPER_PATH&#x3D;&quot;localhost:2181&quot;OUTPUT_PATH&#x3D;&quot;alluxio:&#x2F;&#x2F;localhost:19998&#x2F;access_logs&#x2F;&quot; å¼‚å¸¸å¤„ç†ï¼šWrong FS: alluxio://localhost:19998, expected: hdfs://localhost:9000 æœ¬åœ°ä¼ªåˆ†å¸ƒå¼ä¼šå‡ºç°è¿™ç§é—®é¢˜ åŸå…ˆ123456val output = \"alluxio://localhost:19998/access_logs/data_log\" val outputPath = new Path(output) if (FileSystem.get(conf).exists(outputPath)) &#123; FileSystem.get(conf).delete(outputPath, true) &#125; ä¿®æ”¹ä¸º12345678val output = \"alluxio://localhost:19998/access_logs/data_log\"val outputPath = new Path(output)val fileSystem = outputPath.getFileSystem(conf)if (fileSystem.exists(outputPath)) &#123; fileSystem.delete(outputPath, true)&#125;","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"},{"name":"Alluxio","slug":"Alluxio","permalink":"http://yoursite.com/tags/Alluxio/"}]},{"title":"Sparkç»Ÿè®¡HBaseï¼Œè¾“å‡ºMysql(å­—æ®µè‡ªå®šä¹‰ç‰ˆæœ¬)","slug":"Sparkç»Ÿè®¡HBaseï¼Œè¾“å‡ºMysql-å­—æ®µè‡ªå®šä¹‰ç‰ˆæœ¬","date":"2020-07-02T09:17:03.000Z","updated":"2020-07-02T09:17:03.321Z","comments":true,"path":"2020/07/02/Sparkç»Ÿè®¡HBaseï¼Œè¾“å‡ºMysql-å­—æ®µè‡ªå®šä¹‰ç‰ˆæœ¬/","link":"","permalink":"http://yoursite.com/2020/07/02/Spark%E7%BB%9F%E8%AE%A1HBase%EF%BC%8C%E8%BE%93%E5%87%BAMysql-%E5%AD%97%E6%AE%B5%E8%87%AA%E5%AE%9A%E4%B9%89%E7%89%88%E6%9C%AC/","excerpt":"","text":"ä½¿ç”¨ä¸¤ç§æ–¹å¼ SparkCore, SparkSQL 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233package com.kowhoyimport java.sql.DriverManagerimport org.apache.hadoop.conf.Configurationimport org.apache.hadoop.hbase.client.&#123;Result, Scan&#125;import org.apache.hadoop.hbase.io.ImmutableBytesWritableimport org.apache.spark.sql.&#123;DataFrame, Row, SparkSession&#125;import org.apache.hadoop.hbase.mapreduce.&#123;TableInputFormat, TableMapReduceUtil&#125;import org.apache.hadoop.hbase.util.Bytesimport org.apache.spark.rdd.RDDimport org.apache.spark.sql.types.&#123;StringType, StructField, StructType&#125;import scala.collection.mutable.ListBufferimport scala.util.&#123;Failure, Success, Try&#125;/** * @DESC åœ°ç†ä½ç½®ç»Ÿè®¡åˆ†æ * @Date 2020/7/2 11:35 ä¸Šåˆ **/object RegionAnalysisApp &#123; def main(args: Array[String]): Unit = &#123; if (args.length &lt; 2) &#123; System.err.println(\"Usage: RegionAnalysisApp &lt;dimension&gt; &lt;date&gt;\") System.exit(1) &#125; val Array(dimension, date) = args val dimensionSupport = List(\"country\", \"province\", \"country&amp;province\", \"city\", \"country&amp;city\", \"operator\") if (! dimensionSupport.contains(dimension)) &#123; System.err.println(\"the dimension should in the list (\\\"country\\\", \\\"province\\\", \\\"country&amp;province\\\", \\\"city\\\", \\\"country&amp;city\\\", \\\"operator\\\") \") System.exit(1) &#125; val spark = SparkSession.builder().config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .appName(\"AnalysisApp\").master(\"local[2]\").getOrCreate() spark.sparkContext.setLogLevel(\"WARN\") // è¯»å–HBaseæ•°æ® // 1. åŸºç¡€é…ç½® val conf = new Configuration() conf.set(\"hbase.rootdir\", Utils.getConfigField(\"HBASE_ROOT_DIR\")) conf.set(\"hbase.zookeeper.quorum\", Utils.getConfigField(\"ZOOKEEPER_PATH\")) // 2. è®¾ç½®è¡¨ val tableName = \"access_log_\" + date conf.set(TableInputFormat.INPUT_TABLE, tableName) // 3. è®¾ç½®Scan val scan = new Scan() scan.addFamily(Bytes.toBytes(\"o\")) val dimensionArray =dimension.split(\"&amp;\") for (col &lt;- dimensionArray) &#123; scan.addColumn(Bytes.toBytes(\"o\"), Bytes.toBytes(col)) &#125; conf.set(TableInputFormat.SCAN, TableMapReduceUtil.convertScanToString(scan)) // 4. è¯»å–æ•°æ® val logRDD = spark.sparkContext.newAPIHadoopRDD( conf, classOf[TableInputFormat], classOf[ImmutableBytesWritable], classOf[Result] ) // è®¾ç½®ç¼“å­˜ logRDD.cache() // ç»Ÿè®¡ç»“æœ val analysisResultByCore = analysisByCore(logRDD, dimensionArray) val logDF = getLogDF(spark, logRDD, dimensionArray) val analysisBySparkSQLTableRDD = analysisBySparkSQLTable(spark, logDF, dimensionArray) analysisResultByCore.foreach(println) println(\"------------------------------------------\") analysisBySparkSQLTableRDD.foreach(println) // ä¿å­˜åˆ°ç»“æœåˆ°MySQL saveResultWithListBufferRDD(analysisResultByCore, dimensionArray) &#125; /*** * @Desc ä½¿ç”¨SparkCoreç»Ÿè®¡ * @Date 3:04 ä¸‹åˆ 2020/7/2 * @Param [logRDD, dimensionArray] * @Return org.apache.spark.rdd.RDD&lt;scala.Tuple2&lt;scala.collection.mutable.ListBuffer&lt;java.lang.String&gt;,java.lang.Object&gt;&gt; **/ def analysisByCore(logRDD: RDD[(ImmutableBytesWritable, Result)], dimensionArray: Array[String]): RDD[(ListBuffer[String], Int)] = &#123; logRDD.map&#123; log =&gt; &#123; val dimensionList: ListBuffer[String] = ListBuffer[String]() for (dimension &lt;- dimensionArray) &#123; val dimensionValue = Bytes.toString(log._2.getValue(\"o\".getBytes(), dimension.getBytes())) dimensionList.append(dimensionValue) &#125; (dimensionList, 1) &#125; &#125;.reduceByKey(_+_) &#125; /*** * @Des ç”ŸæˆDataSet * @Date 3:43 ä¸‹åˆ 2020/7/2 * @Param [spark, logRDD, dimensionArray] * @Return org.apache.spark.sql.Dataset&lt;org.apache.spark.sql.Row&gt; **/ def getLogDF(spark: SparkSession, logRDD: RDD[(ImmutableBytesWritable, Result)], dimensionArray: Array[String]): DataFrame = &#123; val schema = StructType( dimensionArray.map(dimension =&gt; &#123; StructField(dimension, StringType, true) &#125;) ) val rowRDD = logRDD.map&#123; x =&gt; &#123; var s: Seq[String] = Seq[String]() for (dimension &lt;- dimensionArray) &#123; val dimensionValue = Bytes.toString(x._2.getValue(\"o\".getBytes(), dimension.getBytes())) s = s:+ dimensionValue &#125; Row.fromSeq(s) &#125; &#125; spark.createDataFrame(rowRDD, schema) &#125; /*** * @Desc ä½¿ç”¨SparkSQLTableç»Ÿè®¡æ•°æ® * @Date 4:05 ä¸‹åˆ 2020/7/2 * @Param [spark, logDF, dimensionArray] * @Return void **/ def analysisBySparkSQLTable(spark: SparkSession, logDF: DataFrame, dimensionArray: Array[String]): RDD[Row] = &#123; logDF.createOrReplaceTempView(\"log\") var dimensionStr = \"\" if (dimensionArray.length == 1) &#123; dimensionStr = dimensionArray(0) + \",\" &#125; else &#123; for (dimension &lt;- dimensionArray) &#123; dimensionStr += dimension + \",\" &#125; &#125; val sql = \"select \" + dimensionStr + \" count(1) cnt from log group by \" + dimensionStr.substring(0, dimensionStr.length - 1) + \" order by cnt desc\" val resultDF = spark.sql(sql) resultDF.rdd &#125; /*** * @Desc ä¿å­˜ç»“æœ * @Date 4:39 ä¸‹åˆ 2020/7/2 * @Param [resultRDD, dimensionArray] * @Return void **/ def saveResultWithListBufferRDD(resultRDD: RDD[(ListBuffer[String], Int)], dimensionArray: Array[String]): Unit = &#123; val insertColumnsBuilder = new StringBuilder(\"(\") dimensionArray.foreach(x =&gt; insertColumnsBuilder.append(x+\",\")) insertColumnsBuilder.deleteCharAt(insertColumnsBuilder.length-1).append(\", cnt)\") val columns = insertColumnsBuilder.toString() val valArr = Array.fill(dimensionArray.length+1)(\"?\") val valStr = valArr.mkString(\",\") val mysqlTableName = dimensionArray.mkString(\"_\") + \"_stat\" resultRDD.coalesce(1).foreachPartition&#123; part =&gt; &#123; Try &#123; val connection = &#123; Class.forName(\"com.mysql.jdbc.Driver\") val url = \"jdbc:mysql://localhost:3306/spark2?characterEncoding=UTF-8\" val user = \"root\" val password = \"root \" DriverManager.getConnection(url, user, password) &#125; val preAutoCommit = connection.getAutoCommit connection.setAutoCommit(false) // å…³é—­è‡ªåŠ¨æäº¤ val sql = s\"insert into $mysqlTableName $columns values ($valStr)\" println(sql) val pstmt = connection.prepareStatement(sql) pstmt.addBatch(s\"truncate $mysqlTableName\") part.foreach(log =&gt; &#123; for (idx &lt;- 1 to dimensionArray.length) &#123; pstmt.setString(idx, log._1(idx-1)) &#125; pstmt.setInt(dimensionArray.length+1, log._2) pstmt.addBatch() &#125;) pstmt.executeBatch() connection.commit() (connection, preAutoCommit) &#125; match &#123; case Success((connection, preAutoCommit)) =&gt; &#123; connection.setAutoCommit(preAutoCommit) if (null != connection) connection.close() &#125; case Failure(exception) =&gt; throw exception &#125; &#125; &#125; &#125;&#125;","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://yoursite.com/tags/HBase/"},{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"},{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"}]},{"title":"åŸºäºHFileåŒæ­¥åˆ°HBaseçš„æ—¥å¿—ETL","slug":"åŸºäºHFileåŒæ­¥åˆ°HBaseçš„æ—¥å¿—ETL","date":"2020-07-01T08:44:08.000Z","updated":"2020-07-02T03:18:38.215Z","comments":true,"path":"2020/07/01/åŸºäºHFileåŒæ­¥åˆ°HBaseçš„æ—¥å¿—ETL/","link":"","permalink":"http://yoursite.com/2020/07/01/%E5%9F%BA%E4%BA%8EHFile%E5%90%8C%E6%AD%A5%E5%88%B0HBase%E7%9A%84%E6%97%A5%E5%BF%97ETL/","excerpt":"","text":"åŸºäºHFileåŒæ­¥åˆ°HBaseçš„æ—¥å¿—ETLä¸€ã€æ•°æ®æ ¼å¼1234110.85.18.234 - - [30&#x2F;Jan&#x2F;2019:00:00:21 +0800] &quot;GET &#x2F;course&#x2F;list?c&#x3D;cb HTTP&#x2F;1.1&quot; 200 12800 &quot;www.imooc.com&quot; &quot;https:&#x2F;&#x2F;www.imooc.com&#x2F;course&#x2F;list?c&#x3D;data&quot; - &quot;Mozilla&#x2F;5.0 (Windows NT 10.0; WOW64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;58.0.3029.110 Safari&#x2F;537.36 SE 2.X MetaSr 1.0&quot; &quot;-&quot; 10.100.16.243:80 200 0.172 0.172218.74.48.154 - - [30&#x2F;Jan&#x2F;2019:00:00:22 +0800] &quot;GET &#x2F;.well-known&#x2F;apple-app-site-association HTTP&#x2F;1.1&quot; 200 165 &quot;www.imooc.com&quot; &quot;-&quot; - &quot;swcd (unknown version) CFNetwork&#x2F;974.2.1 Darwin&#x2F;18.0.0&quot; &quot;-&quot; 10.100.135.47:80 200 0.001 0.001113.77.139.245 - - [30&#x2F;Jan&#x2F;2019:00:00:22 +0800] &quot;GET &#x2F;static&#x2F;img&#x2F;common&#x2F;new.png HTTP&#x2F;1.1&quot; 200 1020 &quot;www.imooc.com&quot; &quot;https:&#x2F;&#x2F;www.imooc.com&#x2F;&quot; - &quot;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;73.0.3642.0 Safari&#x2F;537.36&quot; &quot;-&quot; 10.100.16.241:80 200 0.001 0.001113.77.139.245 - - [30&#x2F;Jan&#x2F;2019:00:00:22 +0800] &quot;GET &#x2F;static&#x2F;img&#x2F;menu_icon.png HTTP&#x2F;1.1&quot; 200 4816 &quot;www.imooc.com&quot; &quot;https:&#x2F;&#x2F;www.imooc.com&#x2F;&quot; - &quot;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;73.0.3642.0 Safari&#x2F;537.36&quot; &quot;-&quot; 10.100.16.243:80 200 0.001 0.001 äºŒã€æ•´ä½“æ€è·¯ å› ä¸ºæ¯ç»„å­—æ®µä¸­ä¹Ÿå­˜åœ¨ç©ºæ ¼é”®ï¼Œæ‰€ä»¥ä½¿ç”¨åˆ†éš”ç¬¦è¿›è¡Œåˆ†éš”è§£æå‡ºå„ä¸ªå­—æ®µä¸å¯è¡Œï¼Œå› æ­¤ä½¿ç”¨åˆ†ç»„çš„æ­£åˆ™åŒ¹é…è¿›è¡Œè§£ææ—¥å¿— å¯¹IPå­—æ®µå¯ä»¥è§£æå‡º country\\province\\city\\operatorä¿¡æ¯ï¼Œä½¿ç”¨Javaå®ç°è§£æå·¥å…·ç±» å¯¹UserAgentå­—æ®µå¯ä»¥è§£æå‡ºbrowserName\\browserVersion\\osName\\osVersionä¿¡æ¯ï¼Œä½¿ç”¨Javaå®ç°è§£æå·¥å…·ç±» æ—¥å¿—ä¸­çš„æ—¶é—´å­—æ®µï¼Œä¸ä¾¿äºè§£æç»Ÿè®¡ï¼Œå°†æ—¶é—´å­—æ®µè¿›è¡Œæ ¼å¼åŒ– ä½¿ç”¨PUTçš„æ–¹æ³•æ•ˆç‡ä½ï¼Œä½¿ç”¨ç”ŸæˆHFileï¼Œloadåˆ°HBaseçš„æ–¹å¼è¿›è¡Œå­˜å‚¨ ä¸‰ã€ç›®å½•ç»“æ„1234567891011121314151617181920.â”œâ”€â”€ javaâ”‚ â””â”€â”€ comâ”‚ â””â”€â”€ kowhoyâ”‚ â”œâ”€â”€ domainâ”‚ â”‚ â”œâ”€â”€ IpInfo.javaâ”‚ â”‚ â””â”€â”€ UaInfo.javaâ”‚ â””â”€â”€ utilâ”‚ â”œâ”€â”€ IpParseUtil.javaâ”‚ â””â”€â”€ UaParseUtil.javaâ”œâ”€â”€ resourcesâ”‚ â”œâ”€â”€ application.confâ”‚ â”œâ”€â”€ core-site.xmlâ”‚ â””â”€â”€ hdfs-site.xmlâ””â”€â”€ scala â””â”€â”€ com â””â”€â”€ kowhoy â”œâ”€â”€ ETLApp.scala â””â”€â”€ commonUtil â””â”€â”€ Util.scala javaç›®å½•ä¸‹ï¼Œä¸»è¦æ˜¯è§£æå·¥å…·ç±» domainå­˜æ”¾ç»“æ„åŒ–ç±» utilå­˜æ”¾è§£ææ–¹æ³• scalaç›®å½•ä¸‹, ä¸»è¦æ˜¯ETLï¼Œå’Œå¸¸ç”¨å·¥å…·ç±» resources/application.conf ä¸ºé…ç½®æ–‡ä»¶ å››ã€IPè§£æIpIfo.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package com.kowhoy.domain;/** * @ClassName IpInfo * @DESC IpInfoç»“æ„ * @Date 2020/6/30 9:39 ä¸‹åˆ **/public class IpInfo &#123; private String country = null; private String province = null; private String city = null; private String operator = null; public String getCountry() &#123; return country; &#125; public String getProvince() &#123; return province; &#125; public String getCity() &#123; return city; &#125; public String getOperator() &#123; return operator; &#125; public void setCountry(String country) &#123; this.country = country; &#125; public void setProvince(String province) &#123; this.province = province; &#125; public void setCity(String city) &#123; this.city = city; &#125; public void setOperator(String operator) &#123; this.operator = operator; &#125; @Override public String toString() &#123; return \"IpInfo&#123;\" + \"country='\" + country + '\\'' + \", province='\" + province + '\\'' + \", city='\" + city + '\\'' + \", operator='\" + operator + '\\'' + '&#125;'; &#125;&#125; IpParseUtil.java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071package com.kowhoy.util;import com.kowhoy.domain.IpInfo;import org.lionsoul.ip2region.*;import java.io.FileNotFoundException;import java.io.IOException;import java.util.logging.Logger;/** * @ClassName IpParseUtil * @DESC Ipè§£æå·¥å…·ç±» * @Date 2020/6/30 9:39 ä¸‹åˆ **/public class IpParseUtil &#123; private static DbConfig dbConfig = null; private static DbSearcher dbSearcher = null; private static Logger logger = Logger.getLogger(IpParseUtil.class.getName()); static &#123; String dbFile = \"/Users/zhouke/tmp_data/ip2region.db\"; try &#123; dbConfig = new DbConfig(); dbSearcher = new DbSearcher(dbConfig, dbFile); &#125; catch (DbMakerConfigException e) &#123; logger.warning(\"ipParser config init exception\" + e.getMessage()); &#125; catch (FileNotFoundException e) &#123; logger.warning(\"ipParser file not found\" + e.getMessage()); &#125; &#125; /** * æ ¹æ®ipè§£æå‡ºipçš„ä½ç½®ä¿¡æ¯ * @param ip: String * @return IpInfo */ public static IpInfo getIpInfo(String ip) &#123; IpInfo info = null; DataBlock block = null; if (Util.isIpAddress(ip)) &#123; try &#123; block = dbSearcher.btreeSearch(ip); &#125; catch (IOException e) &#123; logger.warning(\"ipParser parse error io:\" + e.getMessage() + \"\\t\" + ip); &#125; catch (Exception e) &#123; logger.warning(\"ipParser parse error:\" + e.getMessage() + \"\\t\" + ip); &#125; &#125; if (null != block) &#123; info = new IpInfo(); String regionStr = block.getRegion(); String[] regionData = regionStr.replace(\"|\", \",\").split(\",\"); info.setCountry(regionData[0]); info.setProvince(regionData.length &gt; 2 ? regionData[2] : \"-\"); info.setCity(regionData.length &gt; 3 ? regionData[3] : \"-\"); info.setOperator(regionData.length &gt; 4 ? regionData[4] : \"-\"); &#125; return info; &#125;&#125; äº”ã€UserAgentè§£æUaInfo.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package com.kowhoy.domain;/** * @ClassName UaInfo * @DESC UserAgentç±» * @Date 2020/7/1 11:02 ä¸Šåˆ **/public class UaInfo &#123; private String browserName = null; private String browserVersion = null; private String osName = null; private String osVersion = null; public String getBrowserName() &#123; return browserName; &#125; public void setBrowserName(String browserName) &#123; this.browserName = browserName; &#125; public String getBrowserVersion() &#123; return browserVersion; &#125; public void setBrowserVersion(String browserVersion) &#123; this.browserVersion = browserVersion; &#125; public String getOsName() &#123; return osName; &#125; public void setOsName(String osName) &#123; this.osName = osName; &#125; public String getOsVersion() &#123; return osVersion; &#125; public void setOsVersion(String osVersion) &#123; this.osVersion = osVersion; &#125; @Override public String toString() &#123; return \"UaInfo&#123;\" + \"browserName='\" + browserName + '\\'' + \", browserVersion='\" + browserVersion + '\\'' + \", osName='\" + osName + '\\'' + \", osVersion='\" + osVersion + '\\'' + '&#125;'; &#125;&#125; UaParseUtil.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.kowhoy.util;import com.kowhoy.domain.UaInfo;import cz.mallat.uasparser.OnlineUpdater;import cz.mallat.uasparser.UASparser;import cz.mallat.uasparser.UserAgentInfo;import org.apache.commons.lang3.StringUtils;import java.io.IOException;/** * @ClassName UaParseUtil * @DESC UserAgentè§£æç±» * @Date 2020/7/1 11:02 ä¸Šåˆ **/public class UaParseUtil &#123; private static UASparser parser = null; static &#123; try &#123; parser = new UASparser(OnlineUpdater.getVendoredInputStream()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public static UaInfo getUaInfo(String ua) &#123; UaInfo info = null; try &#123; if (StringUtils.isNotEmpty(ua)) &#123; info = new UaInfo(); UserAgentInfo uaInfo = parser.parse(ua); if (null != uaInfo) &#123; info.setBrowserName(uaInfo.getUaFamily()); info.setBrowserVersion(uaInfo.getBrowserVersionInfo()); info.setOsName(uaInfo.getOsFamily()); info.setOsVersion(uaInfo.getOsName()); &#125; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return info; &#125;&#125; å…­ã€Scalaå·¥å…·ç±»Util.scala123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384package com.kowhoy.commonUtilimport java.util.zip.CRC32import com.typesafe.config.&#123;Config, ConfigFactory&#125;import org.apache.commons.lang3.StringUtilsimport org.apache.hadoop.conf.Configurationimport org.apache.hadoop.hbase.client.&#123;Admin, Connection, ConnectionFactory&#125;import org.apache.hadoop.hbase.util.Bytes/** * @DESC åŸºç¡€å·¥å…·ç±» * @Date 2020/7/1 1:37 ä¸‹åˆ **/object Util &#123; /*** * @Desc ç”ŸæˆrowKey * @Date 1:42 ä¸‹åˆ 2020/7/1 * @Param [date, info] * @Return java.lang.String **/ def generatingRowKey(date:String, info:String): String = &#123; val builder = new StringBuilder(date) builder.append(\"_\") val crc32 = new CRC32() crc32.reset() if (StringUtils.isNotEmpty(info)) &#123; crc32.update(Bytes.toBytes(info)) &#125; builder.append(crc32.getValue) builder.toString() &#125; /*** * @Desc è·å–é…ç½®å­—æ®µ * @Date 2:27 ä¸‹åˆ 2020/7/1 * @Param [key] * @Return java.lang.Object **/ def getConfigField(key: String): String = &#123; lazy val conf: Config = ConfigFactory.load() try &#123; conf.getString(key) &#125; catch &#123; case _ =&gt; System.err.println(key + \"ä¸å­˜åœ¨\") \"not exists\" &#125; &#125; /*** * @Desc HBaseçš„ç›¸å…³ * @Date 2:22 ä¸‹åˆ 2020/7/1 * @Param [] * @Return scala.Tuple3&lt;org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection,org.apache.hadoop.hbase.client.Admin&gt; **/ def getHBaseEngine(): (Configuration, Connection, Admin) = &#123; val conf = new Configuration() conf.set(\"hbase.rootdir\", getConfigField(\"HBASE_ROOT_DIR\")) conf.set(\"hbase.zookeeper.quorum\", getConfigField(\"ZOOKEEPER_PATH\")) var connection: Connection = null; var admin: Admin = null; try &#123; connection = ConnectionFactory.createConnection(conf) admin = connection.getAdmin &#125; catch &#123; case e: Exception =&gt; e.printStackTrace() &#125; (conf, connection, admin) &#125;&#125; ä¸ƒã€ETLAppETLApp.scala123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222package com.kowhoyimport java.util.Localeimport java.util.regex.&#123;Matcher, Pattern&#125;import com.kowhoy.domain.&#123;IpInfo, UaInfo&#125;import com.kowhoy.util.&#123;IpParseUtil, UaParseUtil&#125;import org.apache.commons.lang3.time.FastDateFormatimport org.apache.hadoop.fs.&#123;FileSystem, Path&#125;import org.apache.hadoop.hbase.&#123;KeyValue, TableName&#125;import org.apache.hadoop.hbase.client.&#123;Admin, ColumnFamilyDescriptorBuilder, TableDescriptorBuilder&#125;import org.apache.hadoop.hbase.io.ImmutableBytesWritableimport org.apache.hadoop.hbase.mapreduce.&#123;HFileOutputFormat2, LoadIncrementalHFiles, TableOutputFormat&#125;import org.apache.hadoop.hbase.util.Bytesimport org.apache.spark.sql.SparkSessionimport org.apache.hadoop.mapreduce.&#123;Job =&gt; NewAPIHadoopJob&#125;import org.apache.spark.internal.Loggingimport scala.collection.mutable.ListBuffer/** * @DESC ETL log through HFile saved to HBase * @Date 2020/6/30 5:49 ä¸‹åˆ **/object ETLApp extends Logging&#123; def main(args: Array[String]): Unit = &#123; if (args.length &lt; 1) &#123; System.err.println(\"Usage: ETLApp &lt;date&gt;\") System.exit(1) &#125; // ä¼ å…¥æ—¶é—´å˜é‡ val Array(date) = args val spark = SparkSession.builder().config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .appName(\"ETLApp\").master(\"local[2]\").getOrCreate() // æ—¥å¿—æ–‡ä»¶è·¯å¾„ val logPath = \"hdfs://localhost:9000/access_logs/2020-06-17.log\" val data = spark.sparkContext.textFile(logPath).take(10) val logRDD = spark.sparkContext.parallelize(data) // å°†æ—¥å¿—RDD ä½¿ç”¨åˆ†ç»„æ­£åˆ™åŒ¹é…è¿›è¡Œè§£æ val pattern: Pattern = Pattern.compile(\"([\\\\d\\\\.]&#123;7,&#125;) - - (\\\\[.&#123;26,&#125;\\\\]) (\\\".+?\\\"|\\\\-) (\\\\d&#123;3&#125;|\\\\-) (\\\\d+|\\\\-) (\\\".+?\\\"|\\\\-) (\\\".+?\\\"|\\\\-) - (\\\".+?\\\"|\\\\-) (\\\".+?\\\"|\\\\-) ([\\\\d\\\\.]&#123;7,&#125;:\\\\d+|\\\\-) (\\\\d+|\\\\-) (\\\\d+?\\\\.\\\\d+|\\\\-) (\\\\d+?\\\\.\\\\d+|\\\\-)\") val columnMatcherMap = Map(\"ip\"-&gt;1, \"time\"-&gt;2, \"requestData\"-&gt;3, \"status\"-&gt;4, \"bytesSent\"-&gt;5, \"host\"-&gt;6, \"referer\"-&gt;7, \"ua\"-&gt;8, \"hostIp\"-&gt;10, \"spendTime\"-&gt;12) val saveRDD = logRDD.mapPartitions(partition =&gt; &#123; partition.flatMap( log =&gt; &#123; val matcher: Matcher = pattern.matcher(log) val logMap = scala.collection.mutable.HashMap[String, String]() val rowKeyColValueList = new ListBuffer[((String, String), KeyValue)] if (matcher.matches()) &#123; for ((k, v) &lt;- columnMatcherMap) &#123; logMap.put(k, matcher.group(v)) &#125; val ip = logMap(\"ip\") val time = logMap(\"time\") val ua = logMap(\"ua\") // è§£æIP val ipInfo: IpInfo = IpParseUtil.getIpInfo(ip) var (country, province, city, operator) = (\"-\", \"-\", \"-\", \"-\") if (null != ipInfo) &#123; country = ipInfo.getCountry province = ipInfo.getProvince city = ipInfo.getCity operator = ipInfo.getOperator &#125; // è§£æUA val uaInfo: UaInfo = UaParseUtil.getUaInfo(ua) var (browserName, browserVersion, osName, osVersion) = (\"-\", \"-\", \"-\", \"-\") if (null != uaInfo) &#123; browserName = uaInfo.getBrowserName browserVersion = uaInfo.getBrowserVersion osName = uaInfo.getOsName osVersion = uaInfo.getOsVersion &#125; // åˆ†éš”RequestData val requestArr = logMap(\"requestData\").split(\" \") var(method, url, protocol) = (\"-\", \"-\", \"-\") if (requestArr.length &gt; 2) &#123; method = requestArr(0).replace(\"\\\"\", \"\") url = requestArr(1) protocol = requestArr(2).replace(\"\\\"\", \"\") &#125; // æ ¼å¼åŒ–æ—¶é—´ val timeStr = time.substring(time.indexOf(\"[\")+1, time.indexOf(\"]\")) val logTime = FastDateFormat.getInstance(\"dd/MMM/yyy:HH:mm:ss Z\", Locale.ENGLISH).parse(timeStr).getTime val formatTime = FastDateFormat.getInstance(\"yyyy-MM-dd HH:mm:ss\").format(logTime) logMap.put(\"browserName\", browserName) logMap.put(\"browserVersion\", browserVersion) logMap.put(\"osName\", osName) logMap.put(\"osVersion\", osVersion) logMap.put(\"country\", country) logMap.put(\"province\", province) logMap.put(\"city\", city) logMap.put(\"operator\", operator) logMap.put(\"formatTime\", formatTime) logMap.put(\"method\", method) logMap.put(\"url\", url) logMap.put(\"protocol\", protocol) // ç”ŸæˆrowKey val rowKey: String = commonUtil.Util.generatingRowKey(date, logMap(\"referer\")+url+ip+ua) val rowKeyBytes = Bytes.toBytes(rowKey) for ((col, value) &lt;- logMap) &#123; val keyValue = new KeyValue(rowKeyBytes, \"o\".getBytes, Bytes.toBytes(String.valueOf(col)), Bytes.toBytes(String.valueOf(value))) rowKeyColValueList.append(((rowKey, col), keyValue)) //åŠ å…¥colå­—æ®µä¸ºäº†æ’åºç”¨ &#125; &#125; rowKeyColValueList.toList &#125;) &#125;).sortByKey() // å†™å…¥HFileéœ€è¦æ˜¯æ’åºåçš„ .map(x =&gt; (new ImmutableBytesWritable(Bytes.toBytes(x._1._1)), x._2)) val (conf, connection, admin) = commonUtil.Util.getHBaseEngine() val tableName: String = \"access_log_\" + date val table: TableName = TableName.valueOf(tableName) createTable(tableName, admin) // è®¾ç½®è¾“å‡ºè¡¨ conf.set(TableOutputFormat.OUTPUT_TABLE, tableName) // è®¾ç½®HFileOutPutFormatéœ€è¦çš„ job / table.descriptor / regionLocator val job = NewAPIHadoopJob.getInstance(conf) // NewAPIHadoopJobæ‰‹åŠ¨å¯¼å…¥ val tableDescriptor = connection.getTable(table) val regionLocator = connection.getRegionLocator(table) HFileOutputFormat2.configureIncrementalLoad(job, tableDescriptor, regionLocator) val output = commonUtil.Util.getConfigField(\"OUTPUT_PATH\") + s\"-$date-logs\" val outputPath = new Path(output) if (FileSystem.get(conf).exists(outputPath)) &#123; FileSystem.get(conf).delete(outputPath, true) &#125; // ä¿å­˜HFile, å†™åˆ°outputçš„è·¯å¾„ä¸Š saveRDD.saveAsNewAPIHadoopFile( output, classOf[ImmutableBytesWritable], classOf[KeyValue], classOf[HFileOutputFormat2], job.getConfiguration ) // å¦‚æœoutputä¸Šæœ‰æ–‡ä»¶ï¼Œå°†æ–‡ä»¶loadåˆ°HBase,å¹¶åˆ é™¤HFile if (FileSystem.get(conf).exists(outputPath)) &#123; val load = new LoadIncrementalHFiles(conf) load.doBulkLoad(outputPath, admin, connection.getTable(table), regionLocator) logInfo(\"åˆ é™¤HFile\") FileSystem.get(conf).delete(outputPath, true) &#125; logInfo(\"å†™å…¥å®Œæˆ\") // å…³é—­èµ„æº if (null != connection) &#123; connection.close() &#125; if (null != admin) &#123; admin.close() &#125; spark.stop() &#125; /** * @Desc åˆ›å»ºæ•°æ®è¡¨ * @Date 2:39 ä¸‹åˆ 2020/7/1 * @Param [tableName, admin] * @Return void **/ def createTable(tableName: String, admin: Admin): Unit = &#123; val table = TableName.valueOf(tableName) if (admin.tableExists(table)) &#123; // è¡¨å·²å­˜åœ¨çš„è¯ å…ˆåˆ é™¤ admin.disableTable(table) admin.deleteTable(table) &#125; try &#123; val tableDescriptorBuilder = TableDescriptorBuilder.newBuilder(table) val columnFamilyDescriptor = ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(\"o\")).build() tableDescriptorBuilder.setColumnFamily(columnFamilyDescriptor) admin.createTable(tableDescriptorBuilder.build()) &#125; catch &#123; case e: Exception =&gt; e.printStackTrace() &#125; &#125;&#125; å…«ã€å¼‚å¸¸å¤„ç†java.io.IOException: Mkdirs failed to create å°†${HADOOP_HOME}/etc/hadoop/core-site.xml å’Œ hdfs-site.xmlæ‹·è´çš„src/main/resourcesä¸‹ ä¹ã€ä½¿ç”¨è„šæœ¬æäº¤ä½œä¸š123456789101112#!/bin/bashexport HADOOP_CONF_DIR=/Users/k/soft/hadoop-2.8.5/etc/hadoop/Users/k/soft/spark-2.4.5-bin-hadoop2.7/bin/spark-submit \\--class com.kowhoy.ETLApp \\--master yarn \\--deploy-mode client \\--name \"logETLApp\" \\--jars $(echo /Users/k/IdeaProjects/logETL/out/artifacts/logETL_jar/*.jar | tr ' ' ',') \\--conf spark.serializer=org.apache.spark.serializer.KryoSerializer \\/Users/k/IdeaProjects/logETL/out/artifacts/logETL_jar/logETL.jar \\$(date +\"%F\")","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://yoursite.com/tags/HBase/"},{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"},{"name":"ETL","slug":"ETL","permalink":"http://yoursite.com/tags/ETL/"},{"name":"HFile","slug":"HFile","permalink":"http://yoursite.com/tags/HFile/"}]},{"title":"Alluxioå®‰è£…","slug":"Alluxioå®‰è£…","date":"2020-07-01T02:33:39.000Z","updated":"2020-07-01T02:33:39.832Z","comments":true,"path":"2020/07/01/Alluxioå®‰è£…/","link":"","permalink":"http://yoursite.com/2020/07/01/Alluxio%E5%AE%89%E8%A3%85/","excerpt":"","text":"Alluxioå®‰è£…1. ä¸‹è½½å®‰è£…åŒ…å¯ä»¥ä¸‹è½½å¸¦æœ‰æ­£ç¡®Hadoopç‰ˆæœ¬çš„é¢„ç¼–è¯‘äºŒè¿›åˆ¶åŒ…ï¼Œä¹Ÿå¯æºç ç¼–è¯‘Alluxio2. é…ç½®ç¯å¢ƒå˜é‡3. é…ç½®æ–‡ä»¶ä¿®æ”¹1234567cp conf&#x2F;alluxio-site.properties.template conf&#x2F;alluxio-site.propertiesalluxio.underfs.address&#x3D;hdfs:&#x2F;&#x2F;&lt;NAMENODE&gt;:&lt;PORT&gt;alluxio.master.hostname&#x3D;localhost#&#96;workers&#96; å’Œ &#96;masters&#96; æ–‡ä»¶æ ¹æ®éœ€è¦é…ç½® 4. æ ¼å¼åŒ–1alluxio format ####5. å¯åŠ¨ 1alluxio-start.sh local SudoMount WebUI localhost:19999","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Alluxio","slug":"Alluxio","permalink":"http://yoursite.com/tags/Alluxio/"}]},{"title":"sparkç»Ÿè®¡HBaseæ•°æ®ï¼Œè¾“å‡ºåˆ°MySQL","slug":"sparkç»Ÿè®¡HBaseæ•°æ®ï¼Œè¾“å‡ºåˆ°MySQL","date":"2020-06-29T10:46:49.000Z","updated":"2020-06-29T10:46:49.258Z","comments":true,"path":"2020/06/29/sparkç»Ÿè®¡HBaseæ•°æ®ï¼Œè¾“å‡ºåˆ°MySQL/","link":"","permalink":"http://yoursite.com/2020/06/29/spark%E7%BB%9F%E8%AE%A1HBase%E6%95%B0%E6%8D%AE%EF%BC%8C%E8%BE%93%E5%87%BA%E5%88%B0MySQL/","excerpt":"","text":"12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394package com.kowhoyimport org.apache.spark.sql.SparkSessionimport org.apache.hadoop.conf.Configurationimport org.apache.hadoop.hbase.mapreduce.&#123;TableInputFormat, TableMapReduceUtil&#125;import org.apache.hadoop.hbase.client.&#123;Scan, Result&#125;import org.apache.hadoop.hbase.io.ImmutableBytesWritableimport org.apache.hadoop.hbase.util.Bytesimport scala.util.&#123;Try, Success, Failure&#125;import java.sql.DriverManagerobject BrowserAnalysis &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession.builder().config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .appName(\"BrowserAnalysis\").master(\"local[2]\").getOrCreate() spark.sparkContext.setLogLevel(\"WARN\") val date = \"20200629\" val tableName = \"access_\" + date val conf = new Configuration() conf.set(\"hbase.rootdir\", \"hdfs://localhost:9000/hbase\") conf.set(\"hbase.zookeeper.quorum\", \"localhost:2181\") conf.set(TableInputFormat.INPUT_TABLE, tableName) // é…ç½®Scan val scan = new Scan() scan.addFamily(\"o\".getBytes()) scan.addColumn(\"o\".getBytes(), \"browserName\".getBytes()) conf.set(TableInputFormat.SCAN, TableMapReduceUtil.convertScanToString(scan)) // è¯»å–æ•°æ® val hBaseRDD = spark.sparkContext.newAPIHadoopRDD( conf, classOf[TableInputFormat], classOf[ImmutableBytesWritable], classOf[Result] ) // ç»Ÿè®¡ val resultRDD = hBaseRDD.map(x =&gt; &#123; val browserName = Bytes.toString(x._2.getValue(\"o\".getBytes(), \"browserName\".getBytes())) (browserName, 1) &#125;).reduceByKey(_+_) resultRDD.foreach(println) // è¾“å‡ºåˆ°mysql resultRDD.coalesce(1).foreachPartition(part =&gt; &#123; Try &#123; val connection = &#123; Class.forName(\"com.mysql.jdbc.Driver\") val url = \"jdbc:mysql://localhost:3306/spark?characterEncoding=UTF-8\" val user = \"root\" val password = \"root\" DriverManager.getConnection(url, user, password) &#125; val preAutoCommit = connection.getAutoCommit connection.setAutoCommit(false) val sql = \"insert into browser_stat (date, browser, cnt) values (?, ?, ?)\" val pstmt = connection.prepareStatement(sql) pstmt.addBatch(s\"delete from browser_stat where date = $date\") part.foreach(x =&gt; &#123; pstmt.setString(1, date) pstmt.setString(2, x._1) pstmt.setString(3, x._2) pstmt.addBatch() &#125;) pstmt.executeBatch() connection.commit() (connection, preAutoCommit) &#125; match &#123; case Success((connection, preAutoCommit)) =&gt; &#123; connection.setAutoCommit(preAutoCommit) if (null != connection) conneciton.close() &#125; case Failure(e) =&gt; throw e &#125; &#125;) spark.stop() &#125;&#125;","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://yoursite.com/tags/HBase/"},{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"},{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"}]},{"title":"Sparkæœ¯è¯­åŠSpark-on-yarn","slug":"Sparkæœ¯è¯­åŠSpark-on-yarn","date":"2020-06-29T02:58:34.000Z","updated":"2020-06-29T02:58:34.912Z","comments":true,"path":"2020/06/29/Sparkæœ¯è¯­åŠSpark-on-yarn/","link":"","permalink":"http://yoursite.com/2020/06/29/Spark%E6%9C%AF%E8%AF%AD%E5%8F%8ASpark-on-yarn/","excerpt":"","text":"æœ¯è¯­æ¦‚å¿µæœºå™¨å±‚é¢ Master èŠ‚ç‚¹è´Ÿè´£ç®¡ç†WorkerèŠ‚ç‚¹ Worker èŠ‚ç‚¹ ä¸MasterèŠ‚ç‚¹é€šä¿¡ï¼Œç®¡ç†Executorè¿›ç¨‹ åº”ç”¨å±‚é¢ Application Spark åº”ç”¨ç¨‹åº Driver é©±åŠ¨ç¨‹åº Applicationä¸­çš„mainæ–¹æ³•åŠåˆ›å»ºSparkContext åˆ›å»ºSparkContextæ˜¯ç”¨æ¥å‡†å¤‡Applicationçš„è¿è¡Œç¯å¢ƒ SparkContextå‘ClusterManagerç”³è¯·èµ„æº æ‰§è¡Œå®Œæˆä¹‹åå…³é—­èµ„æº ClusterManager èµ„æºç®¡ç†å™¨ é›†ç¾¤ä¸Šè·å–èµ„æºçš„å¤–éƒ¨æœåŠ¡ åˆ†ç±»ï¼š Standalone SparkåŸç”Ÿçš„èµ„æºç®¡ç†å™¨ï¼Œç”±Masterè´Ÿè´£èµ„æºåˆ†é… Hadoop Yarn, ç”±Yarnä¸­çš„ResearchManagerè´Ÿè´£èµ„æºåˆ†é… Messos, ç”±Messosä¸­çš„Messos Masterè´Ÿè´£èµ„æºåˆ†é… executor æ‰§è¡Œå™¨ åœ¨WorkerèŠ‚ç‚¹ä¸Šï¼Œæœ‰å¤šä¸ªexectoræ‰§è¡Œå™¨ï¼Œæ¯ä¸ªæ‰§è¡Œå™¨æ˜¯ä¸€ä¸ªè¿›ç¨‹ï¼Œæ˜¯ç”¨æ¥æ‰§è¡ŒTaskå’Œå­˜å‚¨æ•°æ®åˆ°å†…å­˜æˆ–è€…ç£ç›˜ä¸Šï¼Œ æ¯ä¸ªæ‰§è¡Œå™¨é‡Œé¢æœ‰ä¸€ä¸ªçº¿ç¨‹æ± ï¼Œæ¯ä¸ªçº¿ç¨‹æ¥æ‰§è¡Œä¸€ä¸ªTask Job å¤šä¸ªStageç»„æˆJob Stage &lt;=&gt; TaskSet ä¸€ç»„Task, Stageåˆ†æˆä¸¤ç§ç±»å‹ShuffleMapStageã€ResultStage Task è¢«é€åˆ°æŸä¸ªExecutorä¸Šçš„å·¥ä½œä»»åŠ¡ï¼›å•ä¸ªåˆ†åŒºæ•°æ®é›†ä¸Šçš„æœ€å°å¤„ç†æµç¨‹å•å…ƒ DAG æœ‰å‘æ— ç¯å›¾ RDDä¹‹é—´çš„ä¾èµ–å…³ç³» DAGScheduler æœ‰å‘æ— ç¯å›¾è°ƒåº¦å™¨ å°†DAGåˆ’åˆ†å‡ºStage åˆ’åˆ†Stageï¼š ä»åå¾€å‰è¿›è¡Œé€†æ¨åˆ’åˆ†ï¼Œä»¥å®½ä¾èµ–ä½œä¸ºåˆ’åˆ†ä¾æ®ï¼Œé‡åˆ°å®½ä¾èµ–å°±ä½œä¸ºStageå‹å…¥æ ˆ å°†Stageè½¬åŒ–ä¸ºTaskSetï¼Œæäº¤åˆ°TaskScheduler è·Ÿè¸ªçŠ¶æ€ è·å–ç»“æœ TaskScheduler ä»»åŠ¡è°ƒåº¦å™¨ æ¥æ”¶åˆ°TaskSetï¼Œæäº¤ç»™Workerï¼Œ æ¯ä¸ªTaskSetä¼šè¢«é€åˆ°ä¸€ä¸ªTaskManagerï¼ŒTaskManageræ ¹æ®å°±è¿‘åŸåˆ™ç»™Taskåˆ†é…èµ„æºï¼ŒTaskManageræ ¹æ®è°ƒåº¦ç­–ç•¥å°†Taskåˆ†é…ç»™Executorè¿›è¡Œæ‰§è¡Œ è°ƒåº¦ç­–ç•¥ï¼š FIFO é˜Ÿåˆ—è§„åˆ™ FAIR å…¬å¹³è°ƒåº¦ Spark on Yarnçš„æ‰§è¡Œæµç¨‹åŠä¸¤ç§æ¨¡å¼1. Yarnç»„ä»¶ ResourceManager: è´Ÿè´£æ•´ä¸ªé›†ç¾¤çš„èµ„æºç®¡ç†å’Œåˆ†é… NodeManager: æ¯ä¸ªèŠ‚ç‚¹çš„èµ„æºã€ä»»åŠ¡ç®¡ç†ï¼Œè´Ÿè´£å¯åŠ¨å’Œåœæ­¢Container ApplicationMaster: æ¯ä¸ªApplicationæœ‰ä¸€ä¸ªApplicationMaster, è´Ÿè´£å‘ŠçŸ¥NodeManageråˆ†é…å’Œå¯åŠ¨Container Container: æŠ½è±¡èµ„æº 2. yarn-clusteræ¨¡å¼ ResourceManager â€”â€“&gt; åœ¨é›†ç¾¤ä¸­é€‰æ‹©ä¸€ä¸ªNodeManageråˆ†é…Containerï¼Œ åœ¨è¿™ä¸ªContainerä¸­å¯åŠ¨ ApplicationMaster è¿›ç¨‹ ApplicationMaster åˆå§‹åŒ– SparkContext ApplicationMaster å‘ResourceManager ç”³è¯·Container, å¹¶å‘ŠçŸ¥NodeManagerå¯åŠ¨Executorè¿›ç¨‹ SparkContext â€“&gt; DAGScheduler â€”&gt; TaskScheduler â€”&gt; åˆ†é… Task â€“&gt; Executoræ‰§è¡Œ â€”&gt; è¿è¡ŒçŠ¶æ€è¿”å›ç»™Driver/Application Master 3. yarn-client æ¨¡å¼ ResourceManager â€”-&gt; åœ¨é›†ç¾¤ä¸­é€‰æ‹©ä¸€ä¸ªNodeManageråˆ†é…Containerï¼Œåœ¨è¿™ä¸ªContainerä¸­å¯åŠ¨ApplicationMasterè¿›ç¨‹ Driver è¿è¡Œåœ¨Clientå®¢æˆ·ç«¯ä¸Š, åˆå§‹åŒ– SparkContext clientä¸Šçš„SparkContext ä¸ ApplicationMasterè¿›è¡Œé€šä¿¡ ApplicationMaster å‘ ResourceManager ç”³è¯·Containerï¼Œå¹¶å‘ŠçŸ¥NodeManagerå¯åŠ¨Executorè¿›ç¨‹ SparkContext â€”&gt; DAGScheduler â€”&gt; TaskScheduler â€”&gt; åˆ†é…Task â€”&gt; Executoræ‰§è¡Œ â€”&gt; è¿è¡ŒçŠ¶æ€è¿”å›ç»™ Driver/client 4. ä¸¤ç§æ¨¡å¼çš„åŒºåˆ«é€šè¿‡ --deploy-mode æŒ‡å®š SparkContextçš„åˆå§‹åŒ–ä½ç½®ä¸åŒï¼Œ clusteræ¨¡å¼Driveræ˜¯åœ¨é›†ç¾¤çš„ä¸€ä¸ªNodeä¸Šçš„ï¼Œ clientæ¨¡å¼Driveræ˜¯åœ¨å®¢æˆ·ç«¯ä¸Šï¼› Driveréœ€è¦ä¸Executorè¿›è¡Œé€šä¿¡ï¼Œæ‰€ä»¥clusteræ¨¡å¼æäº¤ä¹‹åå…³é—­clientä¸ä¼šå½±å“ä»»åŠ¡ï¼Œclientæ¨¡å¼å…³é—­æ‰clientï¼Œä»»åŠ¡å¤±è´¥ï¼› ç”Ÿäº§æ­£å¸¸ä½¿ç”¨clusteræ¨¡å¼ï¼Œè°ƒè¯•å¯ä»¥ä½¿ç”¨clientæ¨¡å¼","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"},{"name":"Yarn","slug":"Yarn","permalink":"http://yoursite.com/tags/Yarn/"}]},{"title":"è¯»å–HBaseç»Ÿè®¡åˆ†æ","slug":"è¯»å–HBaseç»Ÿè®¡åˆ†æ","date":"2020-06-28T07:18:11.000Z","updated":"2020-06-28T07:18:11.993Z","comments":true,"path":"2020/06/28/è¯»å–HBaseç»Ÿè®¡åˆ†æ/","link":"","permalink":"http://yoursite.com/2020/06/28/%E8%AF%BB%E5%8F%96HBase%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90/","excerpt":"","text":"1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package com.kowhoyimport org.apache.hadoop.conf.Configurationimport org.apache.hadoop.hbase.client.&#123;Result, Scan&#125;import org.apache.hadoop.hbase.io.ImmutableBytesWritableimport org.apache.hadoop.hbase.mapreduce.&#123;TableInputFormat, TableMapReduceUtil&#125;import org.apache.hadoop.hbase.util.Bytesimport org.apache.spark.sql.SparkSession/** * @DESC æ—¥å¿—æŒ‰å›½å®¶çœä»½ç»Ÿè®¡åˆ†æ * @Date 2020/6/28 2:49 ä¸‹åˆ **/object AnalysisAppV2 &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession.builder().config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") .appName(\"AnalysisAppV2\").master(\"local[2]\").getOrCreate() spark.sparkContext.setLogLevel(\"WARN\") // ä»HBaseè¯»å–æ•°æ® val date = \"2020-06-28\" val tableName = \"access_\" + date val conf = new Configuration() val scan = new Scan() scan.addFamily(Bytes.toBytes(\"o\")) //åˆ—æ— scan.addColumn(Bytes.toBytes(\"o\"), Bytes.toBytes(\"country\")) scan.addColumn(Bytes.toBytes(\"o\"), Bytes.toBytes(\"province\")) conf.set(\"hbase.rootdir\", \"hdfs://localhost:9000/hbase\") conf.set(\"hbase.zookeeper.quorum\", \"localhost:2181\") conf.set(TableInputFormat.INPUT_TABLE, tableName) conf.set(TableInputFormat.SCAN, TableMapReduceUtil.convertScanToString(scan)) val hBaseRDD = spark.sparkContext.newAPIHadoopRDD( conf, classOf[TableInputFormat], classOf[ImmutableBytesWritable], classOf[Result] ) hBaseRDD.cache() // æ–¹å¼1, ä½¿ç”¨SparkCore ç»Ÿè®¡ hBaseRDD.map(x =&gt; &#123; val country = Bytes.toString(x._2.getValue(\"o\".getBytes(), \"country\".getBytes())) val province = Bytes.toString(x._2.getValue(\"o\".getBytes(), \"province\".getBytes())) ((country, province), 1) &#125;).reduceByKey(_+_) .map(x =&gt; (x._2, x._1)).sortByKey(false) .map(x =&gt; (x._2, x._1)).take(10).foreach(println) println(\"---------------------------------------------------\") // æ–¹å¼äºŒ, ä½¿ç”¨SparkSql API import spark.implicits._ hBaseRDD.map(x =&gt; &#123; val country = Bytes.toString(x._2.getValue(\"o\".getBytes(), \"country\".getBytes())) val province = Bytes.toString(x._2.getValue(\"o\".getBytes(), \"province\".getBytes())) CountryProvince(country, province) &#125;).toDF.select(\"country\", \"province\") .groupBy(\"country\", \"province\").count().sort($\"count\".desc).show(10, false) println(\"---------------------------------------------------\") // æ–¹å¼ä¸‰, ä½¿ç”¨SparkSql ä¸´æ—¶è¡¨ hBaseRDD.map(x =&gt; &#123; val country = Bytes.toString(x._2.getValue(\"o\".getBytes(), \"country\".getBytes())) val province = Bytes.toString(x._2.getValue(\"o\".getBytes(), \"province\".getBytes())) CountryProvince(country, province) &#125;).toDF().createOrReplaceTempView(\"log\") spark.sql(\"select country, province, count(1) cnt from log group by country, province order by cnt desc limit 10\").show(false) hBaseRDD.unpersist(true) spark.stop() &#125; case class CountryProvince(country:String, province:String)&#125;","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://yoursite.com/tags/HBase/"},{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"}]},{"title":"æ—¥å¿—ETLåˆ°HBase","slug":"æ—¥å¿—ETLåˆ°HBase","date":"2020-06-28T05:50:57.000Z","updated":"2020-06-28T05:50:57.876Z","comments":true,"path":"2020/06/28/æ—¥å¿—ETLåˆ°HBase/","link":"","permalink":"http://yoursite.com/2020/06/28/%E6%97%A5%E5%BF%97ETL%E5%88%B0HBase/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282import java.util.Localeimport java.util.regex.&#123;Matcher, Pattern&#125;import java.util.zip.CRC32import com.kowhoy.domain.&#123;IpRegionInfo, LogSchema, UserAgentInfo&#125;import com.kowhoy.util.&#123;IpParseUtil, UserAgentUtil&#125;import org.apache.commons.lang3.StringUtilsimport org.apache.commons.lang3.time.FastDateFormatimport org.apache.hadoop.conf.Configurationimport org.apache.hadoop.hbase.TableNameimport org.apache.hadoop.hbase.client.&#123;Admin, ColumnFamilyDescriptorBuilder, Connection, ConnectionFactory, Put, TableDescriptorBuilder&#125;import org.apache.hadoop.hbase.io.ImmutableBytesWritableimport org.apache.hadoop.hbase.mapreduce.TableOutputFormatimport org.apache.hadoop.hbase.util.Bytesimport org.apache.spark.sql.SparkSession/** * @DESC æ—¥å¿—æ•°æ®ETL =&gt; HBase * @Date 2020/6/28 10:20 ä¸Šåˆ **/object LogETLAppV2 &#123; def main(args: Array[String]): Unit = &#123; if (args.length &lt; 1) &#123; System.err.println(\"Usage: LogETLAppV2 &lt;date&gt;\") System.exit(1) &#125; val Array(date) = args val spark = SparkSession.builder().appName(\"LogETLAppV2\").master(\"local[2]\").getOrCreate() // æ—¥å¿—æ–‡ä»¶åœ°å€ val logPath: String = \"hdfs://localhost:9000/access_logs/2020-06-17.log\" // æ—¥å¿—RDD val logRDD = spark.sparkContext.textFile(logPath) // æ­£åˆ™æ¨¡å¼ val pattern: Pattern = Pattern.compile(\"([\\\\d\\\\.]&#123;7,&#125;) - - (\\\\[.&#123;26,&#125;\\\\]) (\\\".+?\\\"|\\\\-) (\\\\d&#123;3&#125;|\\\\-) (\\\\d+|\\\\-) (\\\".+?\\\"|\\\\-) (\\\".+?\\\"|\\\\-) - (\\\".+?\\\"|\\\\-) (\\\".+?\\\"|\\\\-) ([\\\\d\\\\.]&#123;7,&#125;:\\\\d+|\\\\-) (\\\\d+|\\\\-) (\\\\d+?\\\\.\\\\d+|\\\\-) (\\\\d+?\\\\.\\\\d+|\\\\-)\") // æ—¥å¿—æ­£åˆ™è§£æå‡ºå„ä¸ªå­—æ®µ val transformRDD = logRDD.map&#123; line =&gt; &#123; val matcher: Matcher = pattern.matcher(line) if (matcher.matches()) &#123; val ip = matcher.group(1) val time = matcher.group(2) val requestData = matcher.group(3) val status = matcher.group(4) val bytesSent = matcher.group(5) val host = matcher.group(6) val referer = matcher.group(7) val ua = matcher.group(8) val hostIp = matcher.group(10) val spendTime = matcher.group(12) // è§£æIP val ipInfo:IpRegionInfo = IpParseUtil.getIpRegionInfo(ip) var country = \"unknown\" var province = \"unknown\" var city = \"unknown\" var operator = \"unknown\" if (null != ipInfo) &#123; country = ipInfo.getCountry province = ipInfo.getProvince city = ipInfo.getCity operator = ipInfo.getOperator &#125; // è§£æUserAgent val uaInfo:UserAgentInfo = UserAgentUtil.getUserAgentInfo(ua) var browserName = \"unknown\" var browserVersion= \"unknown\" var osName = \"unknown\" var osVersion = \"unknown\" if (null != uaInfo) &#123; browserName = uaInfo.getBrowserName browserVersion = uaInfo.getBrowserVersion osName = uaInfo.getOsName osVersion = uaInfo.getOsVersion &#125; // åˆ‡å‰²requestData val requestArr = requestData.split(\" \") var method = \"-\" var url = \"-\" var protocol = \"-\" if (requestArr.length &gt; 2) &#123; method = requestArr(0).replace(\"\\\"\", \"\") url = requestArr(1) protocol = requestArr(2).replace(\"\\\"\", \"\") &#125; LogSchema(ip, country, province, city, operator, time, method, url, protocol, status, bytesSent, host, referer, ua, browserName, browserVersion, osName, osVersion, hostIp, spendTime) &#125; else &#123; LogSchema(\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\") &#125; &#125; &#125;.take(1000000) // sparkSql udf æ ¼å¼åŒ–æ—¶é—´å­—æ®µ var logDF = spark.createDataFrame(transformRDD) import org.apache.spark.sql.functions.udf val formatTime = udf((time:String) =&gt; &#123; try &#123; val logTimeStr = time.substring(time.indexOf(\"[\")+1, time.lastIndexOf(\"]\")) val logTime = FastDateFormat.getInstance(\"dd/MMM/yyy:HH:mm:ss Z\", Locale.ENGLISH).parse(logTimeStr).getTime FastDateFormat.getInstance(\"yyyy-MM-dd HH:mm:ss\").format(logTime) &#125; catch &#123; case e: Exception =&gt; \"unknown\" &#125; &#125;) logDF = logDF.withColumn(\"formatTime\", formatTime(logDF(\"time\"))) // ä¿å­˜æ•°æ®åˆ°HBase val hBaseInfoRDD = logDF.rdd.map(x =&gt; &#123; //1. å°†æ•°æ®è½¬åŒ–æˆHashMapä¾¿äºéå† val ip = x.getAs[String](\"ip\") val country = x.getAs[String](\"country\") val province = x.getAs[String](\"province\") val city = x.getAs[String](\"city\") val operator = x.getAs[String](\"operator\") val time = x.getAs[String](\"time\") val formatTime = x.getAs[String](\"formatTime\") val method = x.getAs[String](\"method\") val url = x.getAs[String](\"url\") val protocol = x.getAs[String](\"protocol\") val status = x.getAs[String](\"status\") val bytesSent = x.getAs[String](\"bytesSent\") val host = x.getAs[String](\"host\") val referer = x.getAs[String](\"referer\") val ua = x.getAs[String](\"ua\") val browserName = x.getAs[String](\"browserName\") val browserVersion = x.getAs[String](\"browserVersion\") val osName = x.getAs[String](\"osName\") val osVersion = x.getAs[String](\"osVersion\") val hostIp = x.getAs[String](\"hostIp\") val spendTime = x.getAs[String](\"spendTime\") val columns = scala.collection.mutable.HashMap[String, String]() columns.put(\"ip\", ip) columns.put(\"country\", country) columns.put(\"province\", province) columns.put(\"city\", city) columns.put(\"operator\", operator) columns.put(\"time\", time) columns.put(\"formatTime\", formatTime) columns.put(\"method\", method) columns.put(\"url\", url) columns.put(\"protocol\", protocol) columns.put(\"status\", status) columns.put(\"bytesSent\", bytesSent) columns.put(\"host\", host) columns.put(\"referer\", referer) columns.put(\"ua\", ua) columns.put(\"browserName\", browserName) columns.put(\"browserVersion\", browserVersion) columns.put(\"osName\", osName) columns.put(\"osVersion\", osVersion) columns.put(\"hostIp\", hostIp) columns.put(\"spendTime\", spendTime) // 2. ç”ŸæˆRowKey val rowKey = getRowKey(date, referer+url+ip+ua) val put = new Put(Bytes.toBytes(rowKey)) for ((k, v) &lt;- columns) &#123; put.addColumn(Bytes.toBytes(\"o\"), Bytes.toBytes(String.valueOf(k)), Bytes.toBytes(String.valueOf(v))) &#125; (new ImmutableBytesWritable(rowKey.getBytes), put) &#125;) // 3. é…ç½®è¿æ¥ã€ä¿å­˜æ•°æ® val conf = new Configuration() conf.set(\"hbase.rootdir\", \"hdfs://localhost:9000/hbase\") conf.set(\"hbase.zookeeper.quorum\", \"localhost:2181\") val tableName = createTable(date, conf) conf.set(TableOutputFormat.OUTPUT_TABLE, tableName) hBaseInfoRDD.saveAsNewAPIHadoopFile( \"hdfs://localhost:9000/hbase\", classOf[ImmutableBytesWritable], classOf[Put], classOf[TableOutputFormat[ImmutableBytesWritable]], conf ) println(\"å†™å…¥å®Œæˆ\") spark.stop() &#125; /** * @Desc ç”ŸæˆRowKey * @Date 11:16 ä¸Šåˆ 2020/6/28 * @Param [date, info] * @Return java.lang.String **/ def getRowKey(date: String, info: String): String = &#123; val builder = new StringBuilder(date) builder.append(\"_\") val crc32 = new CRC32() crc32.reset() if (StringUtils.isNotEmpty(info)) &#123; crc32.update(Bytes.toBytes(info)) &#125; builder.append(crc32.getValue) builder.toString() &#125; /** * @Desc åˆ›å»ºè¡¨ * @Date 11:28 ä¸Šåˆ 2020/6/28 * @Param [date, conf] * @Return java.lang.String **/ def createTable(date: String, conf: Configuration): String = &#123; val table = \"access_\" + date var connection: Connection = null var admin: Admin = null try &#123; connection = ConnectionFactory.createConnection(conf) admin = connection.getAdmin val tableName = TableName.valueOf(table) if (admin.tableExists(tableName)) &#123; admin.disableTable(tableName) admin.deleteTable(tableName) println(\"è¡¨åˆ é™¤å®Œæˆ\") &#125; val tableDescriptorBuilder = TableDescriptorBuilder.newBuilder(tableName) val columnDescriptor = ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(\"o\")).build() tableDescriptorBuilder.setColumnFamily(columnDescriptor) admin.createTable(tableDescriptorBuilder.build) &#125; catch &#123; case e:Exception =&gt; e.printStackTrace() &#125; finally &#123; if (null != admin) &#123; admin.close() &#125; if (null != connection) &#123; connection.close() &#125; &#125; table &#125;&#125;","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"},{"name":"HBase","slug":"HBase","permalink":"http://yoursite.com/tags/HBase/"},{"name":"æ—¥å¿—","slug":"æ—¥å¿—","permalink":"http://yoursite.com/tags/%E6%97%A5%E5%BF%97/"},{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"},{"name":"ETL","slug":"ETL","permalink":"http://yoursite.com/tags/ETL/"}]},{"title":"IPè§£æå·¥å…·","slug":"IPè§£æå·¥å…·","date":"2020-06-28T05:48:14.000Z","updated":"2020-06-28T05:48:14.412Z","comments":true,"path":"2020/06/28/IPè§£æå·¥å…·/","link":"","permalink":"http://yoursite.com/2020/06/28/IP%E8%A7%A3%E6%9E%90%E5%B7%A5%E5%85%B7/","excerpt":"","text":"IpRegionInfo12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package com.kowhoy.domain;/** * @ClassName IpRegionInfo * @DESC è‡ªå®šä¹‰IPè§£æåœ°ç†ä½ç½®ç±» _åŸå¸‚Id|å›½å®¶|åŒºåŸŸ|çœä»½|åŸå¸‚|ISP_ 995|ä¸­å›½|0|ä¸Šæµ·|ä¸Šæµ·å¸‚|è”é€š|126445 * @Date 2020/6/16 3:38 ä¸‹åˆ **/public class IpRegionInfo &#123; private String country; private String province; private String city; private String operator; public void setCountry(String country) &#123; this.country = country; &#125; public void setProvince(String province) &#123; this.province = province; &#125; public void setCity(String city) &#123; this.city = city; &#125; public void setOperator(String operator) &#123; this.operator = operator; &#125; public String getCountry() &#123; return country; &#125; public String getProvince() &#123; return province; &#125; public String getCity() &#123; return city; &#125; public String getOperator() &#123; return operator; &#125; @Override public String toString() &#123; return \"IpRegionInfo&#123;\" + \"country='\" + country + '\\'' + \", province='\" + province + '\\'' + \", city='\" + city + '\\'' + \", operator='\" + operator + '\\'' + '&#125;'; &#125;&#125; UserAgentUtil123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package com.kowhoy.util;import com.kowhoy.domain.UserAgentInfo;import cz.mallat.uasparser.OnlineUpdater;import cz.mallat.uasparser.UASparser;import org.apache.commons.lang3.StringUtils;import java.io.IOException;/** * @ClassName UserAgentUtil * @DESC UserAgentè§£æå·¥å…· * @Date 2020/6/16 2:58 ä¸‹åˆ **/public class UserAgentUtil &#123; private static UASparser parser = null; static &#123; try &#123; parser = new UASparser(OnlineUpdater.getVendoredInputStream()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; /** * è·å–UserAgentè§£æ * @param ua:String * @return UserAgentInfo */ public static UserAgentInfo getUserAgentInfo(String ua) &#123; UserAgentInfo info = null; try &#123; if (StringUtils.isNotEmpty(ua)) &#123; info = new UserAgentInfo(); cz.mallat.uasparser.UserAgentInfo uaInfo = parser.parse(ua); if (null != uaInfo) &#123; info.setBrowserName(uaInfo.getUaFamily()); info.setBrowserVersion(uaInfo.getBrowserVersionInfo()); info.setOsName(uaInfo.getOsFamily()); info.setOsVersion(uaInfo.getOsName()); &#125; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return info; &#125;&#125;","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}]},{"title":"UserAgentè§£æå·¥å…·","slug":"UserAgentè§£æå·¥å…·","date":"2020-06-28T05:46:58.000Z","updated":"2020-06-28T05:46:59.018Z","comments":true,"path":"2020/06/28/UserAgentè§£æå·¥å…·/","link":"","permalink":"http://yoursite.com/2020/06/28/UserAgent%E8%A7%A3%E6%9E%90%E5%B7%A5%E5%85%B7/","excerpt":"","text":"UserAgentInfo12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package com.kowhoy.domain;/** * @ClassName UserAgentInfo * @DESC è‡ªå®šä¹‰UserAgentå·¥å…·ç±» * @Date 2020/6/16 2:56 ä¸‹åˆ **/public class UserAgentInfo &#123; private String browserName; private String browserVersion; private String osName; private String osVersion; public void setBrowserName(String browserName) &#123; this.browserName = browserName; &#125; public void setBrowserVersion(String browserVersion) &#123; this.browserVersion = browserVersion; &#125; public void setOsName(String osName) &#123; this.osName = osName; &#125; public void setOsVersion(String osVersion) &#123; this.osVersion = osVersion; &#125; public String getBrowserName() &#123; return browserName; &#125; public String getBrowserVersion() &#123; return browserVersion; &#125; public String getOsName() &#123; return osName; &#125; public String getOsVersion() &#123; return osVersion; &#125; @Override public String toString() &#123; return \"UserAgentInfo&#123;\" + \"browserName='\" + browserName + '\\'' + \", browserVersion='\" + browserVersion + '\\'' + \", osName='\" + osName + '\\'' + \", osVersion='\" + osVersion + '\\'' + '&#125;'; &#125;&#125; UserAgentUtil123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package com.kowhoy.util;import com.kowhoy.domain.UserAgentInfo;import cz.mallat.uasparser.OnlineUpdater;import cz.mallat.uasparser.UASparser;import org.apache.commons.lang3.StringUtils;import java.io.IOException;/** * @ClassName UserAgentUtil * @DESC UserAgentè§£æå·¥å…· * @Date 2020/6/16 2:58 ä¸‹åˆ **/public class UserAgentUtil &#123; private static UASparser parser = null; static &#123; try &#123; parser = new UASparser(OnlineUpdater.getVendoredInputStream()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; /** * è·å–UserAgentè§£æ * @param ua:String * @return UserAgentInfo */ public static UserAgentInfo getUserAgentInfo(String ua) &#123; UserAgentInfo info = null; try &#123; if (StringUtils.isNotEmpty(ua)) &#123; info = new UserAgentInfo(); cz.mallat.uasparser.UserAgentInfo uaInfo = parser.parse(ua); if (null != uaInfo) &#123; info.setBrowserName(uaInfo.getUaFamily()); info.setBrowserVersion(uaInfo.getBrowserVersionInfo()); info.setOsName(uaInfo.getOsFamily()); info.setOsVersion(uaInfo.getOsName()); &#125; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return info; &#125;&#125;","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}]},{"title":"HBase DML API","slug":"HBase-DML-API","date":"2020-06-15T07:26:18.000Z","updated":"2020-06-15T07:26:18.528Z","comments":true,"path":"2020/06/15/HBase-DML-API/","link":"","permalink":"http://yoursite.com/2020/06/15/HBase-DML-API/","excerpt":"","text":"HBase DML APIä¸€ã€å†™æ•°æ®1. å•æ¡å†™12345678910111213141516@Testpublic void addSingle() throws Exception &#123; String tableName = \"sku_info\"; TableName table = TableName.valueOf(tableName); String rowKey = \"ok\"; Put put = new Put(Bytes.toBytes(rowKey)); put.addColumn(Bytes.toBytes(\"basic\"), Bytes.toBytes(\"price\"), Bytes.toBytes(\"12.8\")); put.addColumn(Bytes.toBytes(\"basic\"), Bytes.toBytes(\"code\"), Bytes.toBytes(\"O_1\")); put.addColumn(Bytes.toBytes(\"area\"), Bytes.toBytes(\"province\"), Bytes.toBytes(\"SH\")); put.addColumn(Bytes.toBytes(\"area\"), Bytes.toBytes(\"city\"), Bytes.toBytes(\"SH\")); connection.getTable(table).put(put);&#125; 2. æ‰¹é‡å†™123456789101112131415161718192021222324@Testpublic void addBatch() throws Exception &#123; String tableName = \"sku_info\"; TableName table = TableName.valueOf(tableName); List&lt;Put&gt; puts = new ArrayList&lt;Put&gt;(); Put put1 = new Put(Bytes.toBytes(\"GA\")); put1.addColumn(Bytes.toBytes(\"basic\"), Bytes.toBytes(\"price\"), Bytes.toBytes(\"13.5\")); put1.addColumn(Bytes.toBytes(\"basic\"), Bytes.toBytes(\"code\"), Bytes.toBytes(\"G_1\")); put1.addColumn(Bytes.toBytes(\"area\"), Bytes.toBytes(\"province\"), Bytes.toBytes(\"JS\")); put1.addColumn(Bytes.toBytes(\"area\"), Bytes.toBytes(\"city\"), Bytes.toBytes(\"YC\")); Put put2 = new Put(Bytes.toBytes(\"GB\")); put2.addColumn(Bytes.toBytes(\"basic\"), Bytes.toBytes(\"price\"), Bytes.toBytes(\"11.14\")); put2.addColumn(Bytes.toBytes(\"basic\"), Bytes.toBytes(\"code\"), Bytes.toBytes(\"G_2\")); put2.addColumn(Bytes.toBytes(\"area\"), Bytes.toBytes(\"province\"), Bytes.toBytes(\"JS\")); put2.addColumn(Bytes.toBytes(\"area\"), Bytes.toBytes(\"city\"), Bytes.toBytes(\"Wx\")); puts.add(put1); puts.add(put2); connection.getTable(table).put(puts);&#125; äºŒã€ä¿®æ”¹æ•°æ®12345678910@Testpublic void update() throws Exception &#123; String tableName = \"sku_info\"; TableName table = TableName.valueOf(tableName); Put put = new Put(\"GA\".getBytes()); put.addColumn(Bytes.toBytes(\"basic\"), Bytes.toBytes(\"price\"), Bytes.toBytes(\"10.9\")); connection.getTable(table).put(put);&#125; ä¸‰ã€è·å–æ•°æ®1. æ‰“å°ç»“æœæ–¹æ³•12345678910111213141516/** * è¾“å‡ºç»“æœ * @param result: Result */private void printResult(Result result) &#123; Cell[] cells = result.rawCells(); for (Cell cell : cells) &#123; System.out.println(Bytes.toString(result.getRow()) + \"\\t\" + Bytes.toString(CellUtil.cloneFamily(cell)) + \"\\t\" + Bytes.toString(CellUtil.cloneQualifier(cell)) + \"\\t\" + Bytes.toString(CellUtil.cloneValue(cell)) + \"\\t\" + cell.getTimestamp() ); &#125;&#125; 2. è·å–æŒ‡å®šRowKeyæ•°æ®1234567891011@Testpublic void getData() throws Exception &#123; String tableName = \"sku_info\"; TableName table = TableName.valueOf(tableName); Get get = new Get(\"ok\".getBytes()); Result result = connection.getTable(table).get(get); printResult(result);&#125; 3. è·å–æ‰§è¡ŒRowKey,æŒ‡å®šåˆ—æ•°æ®12345678910111213@Testpublic void getDataWithPartColumns() throws Exception &#123; String tableName = \"sku_info\"; TableName table = TableName.valueOf(tableName); Get get = new Get(\"ok\".getBytes()); get.addColumn(Bytes.toBytes(\"area\"), Bytes.toBytes(\"province\")); Result result = connection.getTable(table).get(get); printResult(result);&#125; 4. æ‰«æå…¨è¡¨123456789101112131415@Testpublic void scanData() throws Exception &#123; String tableName = \"sku_info\"; TableName table = TableName.valueOf(tableName); Scan scan = new Scan(); ResultScanner resultScanner = connection.getTable(table).getScanner(scan); for (Result result: resultScanner) &#123; printResult(result); System.out.println(\"--------------\"); &#125;&#125; 5. æ‰«ææŒ‡å®šåˆ—12345678910111213141516171819@Testpublic void scanColumns() throws Exception &#123; String tableName = \"sku_info\"; TableName table = TableName.valueOf(tableName); Scan scan = new Scan(); scan.addColumn(Bytes.toBytes(\"area\"), Bytes.toBytes(\"city\")); scan.addColumn(Bytes.toBytes(\"basic\"), Bytes.toBytes(\"code\")); ResultScanner resultScanner = connection.getTable(table).getScanner(scan); for (Result result : resultScanner) &#123; printResult(result); System.out.println(\"-------------\"); &#125;&#125; 6. RowKeyæ¡ä»¶Scan12345678910111213141516@Testpublic void scanWithRowKeyCondition() throws Exception &#123; String tableName = \"sku_info\"; TableName table = TableName.valueOf(tableName); Scan scan = new Scan().withStartRow(Bytes.toBytes(\"GB\")).withStopRow(Bytes.toBytes(\"ok\")); ResultScanner resultScanner = connection.getTable(table).getScanner(scan); for (Result result : resultScanner) &#123; printResult(result); System.out.println(\"--------------\"); &#125;&#125; 7. å•ä¸ªFilter1234567891011121314151617181920@Testpublic void singleFilter() throws Exception &#123; String tableName = \"sku_info\"; TableName table = TableName.valueOf(tableName); Scan scan = new Scan(); String reg = \"^*k\"; Filter filter = new RowFilter(CompareOperator.EQUAL, new RegexStringComparator(\"reg\")); scan.setFilter(filter); ResultScanner resultScanner = connection.getTable(table).getScanner(scan); for (Result result: resultScanner) &#123; printResult(result); System.out.println(\"-------------\"); &#125;&#125; 8.å¤šä¸ªFilter1234567891011121314151617181920212223242526@Testpublic void muchFilter() throws Exception &#123; String tableName = \"sku_info\"; TableName table = TableName.valueOf(tableName); Scan scan = new Scan(); FilterList filters = new FilterList(FilterList.Operator.MUST_PASS_ONE); Filter filter1 = new PrefixFilter(\"o\".getBytes()); Filter filter2 = new RowFilter(CompareOperator.EQUAL, new RegexStringComparator(\"^*B\")); filters.addFilter(filter1); filters.addFilter(filter2); scan.setFilter(filters); ResultScanner resultScanner = connection.getTable(table).getScanner(scan); for (Result result: resultScanner) &#123; printResult(result); System.out.println(\"-----------\"); &#125;&#125;","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://yoursite.com/tags/HBase/"}]},{"title":"HBase DDL API","slug":"HBase-DDL-API","date":"2020-06-12T06:19:55.000Z","updated":"2020-06-15T02:41:25.456Z","comments":true,"path":"2020/06/12/HBase-DDL-API/","link":"","permalink":"http://yoursite.com/2020/06/12/HBase-DDL-API/","excerpt":"","text":"HBase DDL APIGAV12345&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;$&#123;hbase.version&#125;&lt;/version&gt;&lt;/dependency&gt; ä¸»è¦ä¾èµ–1234567891011121314import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HColumnDescriptor;import org.apache.hadoop.hbase.HTableDescriptor;import org.apache.hadoop.hbase.TableName;import org.apache.hadoop.hbase.client.Admin;import org.apache.hadoop.hbase.client.Connection;import org.apache.hadoop.hbase.client.ConnectionFactory;import org.apache.hadoop.hbase.util.Bytes;import org.junit.After;import org.junit.Assert;import org.junit.Before;import org.junit.Test;import java.io.IOException; Create Connection and Admin ConnectionFactory.createConnectionConfiguration 1234567891011121314@Beforepublic void setUp() &#123; Configuration configuration = new Configuration(); configuration.set(\"hbase.rootdir\", \"hdfs://localhost:9000/hbase\"); configuration.set(\"hbase.zookeeper.quorum\", \"localhost:2181\"); try &#123; connection = ConnectionFactory.createConnection(configuration); admin = connection.getAdmin(); &#125; catch (IOExpection e)&#123; e.printStackTrace(); &#125; Assert assertNotNull(admin); &#125; create table è€ç‰ˆæœ¬ since 2.0 version and will be removed in 3.0 version åˆ›å»ºè¡¨éœ€è¦æœ‰è¡¨çš„æè¿° HTableDescriptor è¡¨çš„æè¿°åŒ…å«äº†åˆ—æ—çš„æè¿° HColumnDescriptor 1234567891011121314151617@Testpublic void createTable() throws Exception &#123; String tableName = \"demo_table\"; TableName table = TableName.valueof(tableName); if admin.tableExists(table) &#123; System.out.println(tableName + \"è¡¨å·²ç»å­˜åœ¨...\"); &#125; else &#123; HTableDescriptor tableDescriptor = new HTableDescriptor(table); tableDescriptor.addFamily(new HColumnDescriptor(\"info\")); tableDescriptor.addFamily(new HColumnDescriptor(\"address\")); admin.createTable(tableDescriptor); System.out.println(tableName + \"è¡¨åˆ›å»ºå®Œæˆ...\"); &#125;&#125; æ–°ç‰ˆæœ¬ 12345678910111213141516171819202122@Testpublic void createTableNew() throws Exception &#123; String tableName = \"sku2\"; TableName table = TableName.valueOf(tableName); if (admin.tableExists(table)) &#123; System.out.println(tableName + \"å·²å­˜åœ¨...\"); &#125; else &#123; TableDescriptorBuilder tableDescriptorBuilder = TableDescriptorBuilder.newBuilder(table); ColumnFamilyDescriptor columnFamilyDescriptor1 = ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(\"basic\")).build(); ColumnFamilyDescriptor columnFamilyDescriptor2 = ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(\"area\")).build(); tableDescriptorBuilder.setColumnFamily(columnFamilyDescriptor1); tableDescriptorBuilder.setColumnFamily(columnFamilyDescriptor2); admin.createTable(tableDescriptorBuilder.build()); System.out.println(tableName + \"åˆ›å»ºå®Œæˆ...\\n\"); this.descTableInfos(); &#125;&#125; desc table 2 ç‰ˆæœ¬listTables() since 2.0 version and will be removed in 3.0 version. Use listTableDescriptors().getColumnFamilies() 12345678910111213@Testpublic void descTablesInfos() throws Exception &#123; HTableDescriptor[] tables = admin.listTables(); for (HTableDescriptor table : tables) &#123; System.out.println(table.getNameAsString()); HColumnDescriptor[] columns = table.getColumnFamilies(); for (HColumnDescriptor column : columns) &#123; System.out.println(\"\\t\" + column.getNameAsString()); &#125; &#125;&#125; 3.0 ç‰ˆæœ¬listTableDescriptorsgetColumnFamilies 12345678910111213141516@Testpublic void descTableInfos() throws Exception &#123; List&lt;TableDescriptor&gt; tableDescriptors = admin.listTableDescriptors(); for (TableDescriptor tableDescriptor : tableDescriptors) &#123; TableName table = tableDescriptor.getTableName(); String tableName = table.getNameAsString(); ColumnFamilyDescriptor[] columnFamilyDescriptors = tableDescriptor.getColumnFamilies(); for (ColumnFamilyDescriptor columnFamilyDescriptor: columnFamilyDescriptors) &#123; String columnName = columnFamilyDescriptor.getNameAsString(); System.out.println(tableName + \"\\n\\t\" + columnName); &#125; &#125;&#125; delete Column Family1234567891011@Testpublic void deleteColumnFamily() throws Exception &#123; String tableName = \"member\"; TableName table = TableName.valueOf(tableName); byte[] column = Bytes.toBytes(\"info\"); admin.deleteColumnFamily(table, column);&#125; drop table12345678910111213141516171819@Testpublic void deleteTable() &#123; String tableName = \"java_api\"; TableName table = TableName.valueOf(tableName); try &#123; if (admin.tableExists(table)) &#123; admin.disableTable(table); admin.deleteTable(table); System.out.println(tableName + \"åˆ é™¤å®Œæˆ...\"); &#125; else &#123; System.out.println(tableName + \"ä¸å­˜åœ¨...\"); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125;","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"http://yoursite.com/tags/HBase/"}]},{"title":"HBASE DML","slug":"HBASE-DML","date":"2020-06-10T14:05:39.000Z","updated":"2020-06-10T14:05:39.737Z","comments":true,"path":"2020/06/10/HBASE-DML/","link":"","permalink":"http://yoursite.com/2020/06/10/HBASE-DML/","excerpt":"","text":"HBASE DML åˆ›å»ºæ•°æ® 1put è¡¨å,RowKey,&#39;CF:COL&#39;, VALUE æ‰«æè¡¨ 1scan è¡¨å åˆ é™¤æ•°æ® 1delete è¡¨å, RowKey, &#39;CF:COL&#39; ä¿®æ”¹æ•°æ® 1put è¡¨å, RowKey, &#39;CF:COL&#39;, VALUE åˆ é™¤æ•´è¡Œ 1deleteall è¡¨å, RowKey è¡Œæ•° 1count è¡¨å æŸ¥çœ‹æ•°æ® 1get è¡¨å, RowKey [, &#39;CF:COL&#39;] æ¸…ç©ºè¡¨ 1truncate è¡¨å","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HBASE","slug":"HBASE","permalink":"http://yoursite.com/tags/HBASE/"}]},{"title":"HBASE DDL","slug":"HBASE-DDL","date":"2020-06-10T13:39:17.000Z","updated":"2020-06-10T13:39:17.704Z","comments":true,"path":"2020/06/10/HBASE-DDL/","link":"","permalink":"http://yoursite.com/2020/06/10/HBASE-DDL/","excerpt":"","text":"HBASE DDL å¯åŠ¨HBASE SHELL 1hbase shell æŸ¥çœ‹ç‰ˆæœ¬ã€çŠ¶æ€ 1234567hbase(main):037:0&gt; version2.2.2, re6513a76c91cceda95dad7af246ac81d46fa2589, Sat Oct 19 10:10:12 UTC 2019hbase(main):036:0&gt; status1 active master, 0 backup masters, 1 servers, 1 dead, 3.0000 average loadTook 0.0105 seconds åˆ›å»ºè¡¨ 1234567hbase(main):042:0* create &#39;member&#39;, &#39;member_id&#39;, &#39;info&#39;, &#39;address&#39;Created table memberTook 2.2730 seconds&#x3D;&gt; Hbase::Table - member## create &#39;è¡¨å&#39;, &#39;åˆ—æ—1&#x2F;cf&#39;, &#39;åˆ—æ—2&#x2F;cf&#39;... DESCè¡¨ 123456789101112131415161718192021hbase(main):043:0&gt; desc &#39;member&#39;Table member is ENABLEDmemberCOLUMN FAMILIES DESCRIPTION&#123;NAME &#x3D;&gt; &#39;address&#39;, VERSIONS &#x3D;&gt; &#39;1&#39;, EVICT_BLOCKS_ON_CLOSE &#x3D;&gt; &#39;false&#39;, NEW_VERSION_BEHAVIOR &#x3D;&gt; &#39;false&#39;, KEEP_DELETED_CELLS &#x3D;&gt; &#39;FALSE&#39;, CACHE_DATA_ON_WRITE &#x3D;&gt; &#39;false&#39;, DATA_BLOCK_ENCODING &#x3D;&gt; &#39;NONE&#39;, TTL &#x3D;&gt; &#39;FOREVER&#39;, MIN_VERSIONS &#x3D;&gt; &#39;0&#39;, REPLICATION_SCOPE &#x3D;&gt; &#39;0&#39;, BLOOMFILTER &#x3D;&gt; &#39;ROW&#39;, CACHE_INDEX_ON_WRITE &#x3D;&gt; &#39;false&#39;, IN_MEMORY &#x3D;&gt; &#39;false&#39;, CACHE_BLOOMS_ON_WRITE &#x3D;&gt; &#39;false&#39;, PREFETCH_BLOCKS_ON_OPEN &#x3D;&gt; &#39;false&#39;, COMPRESSION &#x3D;&gt; &#39;NONE&#39;, BLOCKCACHE &#x3D;&gt; &#39;true&#39;, BLOCKSIZE &#x3D;&gt; &#39;65536&#39;&#125;&#123;NAME &#x3D;&gt; &#39;info&#39;, VERSIONS &#x3D;&gt; &#39;1&#39;, EVICT_BLOCKS_ON_CLOSE &#x3D;&gt; &#39;false&#39;, NEW_VERSION_BEHAVIOR &#x3D;&gt; &#39;false&#39;, KEEP_DELETED_CELLS &#x3D;&gt; &#39;FALSE&#39;, CACHE_DATA_ON_WRITE &#x3D;&gt; &#39;false&#39;, DATA_BLOCK_ENCODING &#x3D;&gt; &#39;NONE&#39;, TTL &#x3D;&gt; &#39;FOREVER&#39;, MIN_VERSIONS &#x3D;&gt; &#39;0&#39;, REPLICATION_SCOPE &#x3D;&gt; &#39;0&#39;, BLOOMFILTER &#x3D;&gt; &#39;ROW&#39;, CACHE_INDEX_ON_WRITE &#x3D;&gt; &#39;false&#39;, IN_MEMORY &#x3D;&gt; &#39;false&#39;, CACHE_BLOOMS_ON_WRITE &#x3D;&gt; &#39;false&#39;, PREFETCH_BLOCKS_ON_OPEN &#x3D;&gt; &#39;false&#39;, COMPRESSION &#x3D;&gt; &#39;NONE&#39;, BLOCKCACHE &#x3D;&gt; &#39;true&#39;, BLOCKSIZE &#x3D;&gt; &#39;65536&#39;&#125;&#123;NAME &#x3D;&gt; &#39;member_id&#39;, VERSIONS &#x3D;&gt; &#39;1&#39;, EVICT_BLOCKS_ON_CLOSE &#x3D;&gt; &#39;false&#39;, NEW_VERSION_BEHAVIOR &#x3D;&gt; &#39;false&#39;, KEEP_DELETED_CELLS &#x3D;&gt; &#39;FALSE&#39;, CACHE_DATA_ON_WRITE &#x3D;&gt; &#39;false&#39;, DATA_BLOCK_ENCODING &#x3D;&gt; &#39;NONE&#39;, TTL &#x3D;&gt; &#39;FOREVER&#39;, MIN_VERSIONS &#x3D;&gt; &#39;0&#39;, REPLICATION_SCOPE &#x3D;&gt; &#39;0&#39;, BLOOMFILTER &#x3D;&gt; &#39;ROW&#39;, CACHE_INDEX_ON_WRITE &#x3D;&gt; &#39;false&#39;, IN_MEMORY &#x3D;&gt; &#39;false&#39;, CACHE_BLOOMS_ON_WRITE &#x3D;&gt; &#39;false&#39;, PREFETCH_BLOCKS_ON_OPEN &#x3D;&gt; &#39;false&#39;, COMPRESSION &#x3D;&gt; &#39;NONE&#39;, BLOCKCACHE &#x3D;&gt; &#39;true&#39;, BLOCKSIZE &#x3D;&gt; &#39;65536&#39;&#125;3 row(s)QUOTAS0 row(s)Took 0.0436 seconds æŸ¥çœ‹è¡¨ 1234567hbase(main):044:0&gt; listTABLEmemberuser2 row(s)Took 0.0048 seconds&#x3D;&gt; [&quot;member&quot;, &quot;user&quot;] åˆ é™¤åˆ—æ—/CF 12345678910111213141516171819202122hbase(main):071:0&gt; alter &#39;member&#39;, &#39;delete&#39;&#x3D;&gt;&#39;member_id&#39;Updating all regions with the new schema...1&#x2F;1 regions updated.Done.Took 2.0179 secondshbase(main):072:0&gt; desc &#39;member&#39;Table member is ENABLEDmemberCOLUMN FAMILIES DESCRIPTION&#123;NAME &#x3D;&gt; &#39;address&#39;, VERSIONS &#x3D;&gt; &#39;1&#39;, EVICT_BLOCKS_ON_CLOSE &#x3D;&gt; &#39;false&#39;, NEW_VERSION_BEHAVIOR &#x3D;&gt; &#39;false&#39;, KEEP_DELETED_CELLS &#x3D;&gt; &#39;FALSE&#39;, CACHE_DATA_ON_WRITE &#x3D;&gt; &#39;false&#39;, DATA_BLOCK_ENCODING &#x3D;&gt; &#39;NONE&#39;, TTL &#x3D;&gt; &#39;FOREVER&#39;, MIN_VERSIONS &#x3D;&gt; &#39;0&#39;, REPLICATION_SCOPE &#x3D;&gt; &#39;0&#39;, BLOOMFILTER &#x3D;&gt; &#39;ROW&#39;, CACHE_INDEX_ON_WRITE &#x3D;&gt; &#39;false&#39;, IN_MEMORY &#x3D;&gt; &#39;false&#39;, CACHE_BLOOMS_ON_WRITE &#x3D;&gt; &#39;false&#39;, PREFETCH_BLOCKS_ON_OPEN &#x3D;&gt; &#39;false&#39;, COMPRESSION &#x3D;&gt; &#39;NONE&#39;, BLOCKCACHE &#x3D;&gt; &#39;true&#39;, BLOCKSIZE &#x3D;&gt; &#39;65536&#39;&#125;&#123;NAME &#x3D;&gt; &#39;info&#39;, VERSIONS &#x3D;&gt; &#39;1&#39;, EVICT_BLOCKS_ON_CLOSE &#x3D;&gt; &#39;false&#39;, NEW_VERSION_BEHAVIOR &#x3D;&gt; &#39;false&#39;, KEEP_DELETED_CELLS &#x3D;&gt; &#39;FALSE&#39;, CACHE_DATA_ON_WRITE &#x3D;&gt; &#39;false&#39;, DATA_BLOCK_ENCODING &#x3D;&gt; &#39;NONE&#39;, TTL &#x3D;&gt; &#39;FOREVER&#39;, MIN_VERSIONS &#x3D;&gt; &#39;0&#39;, REPLICATION_SCOPE &#x3D;&gt; &#39;0&#39;, BLOOMFILTER &#x3D;&gt; &#39;ROW&#39;, CACHE_INDEX_ON_WRITE &#x3D;&gt; &#39;false&#39;, IN_MEMORY &#x3D;&gt; &#39;false&#39;, CACHE_BLOOMS_ON_WRITE &#x3D;&gt; &#39;false&#39;, PREFETCH_BLOCKS_ON_OPEN &#x3D;&gt; &#39;false&#39;, COMPRESSION &#x3D;&gt; &#39;NONE&#39;, BLOCKCACHE &#x3D;&gt; &#39;true&#39;, BLOCKSIZE &#x3D;&gt; &#39;65536&#39;&#125;2 row(s)QUOTAS0 row(s)Took 0.0503 seconds åˆ é™¤è¡¨ disable tb drop tb1234567891011121314151617181920212223242526272829303132333435hbase(main):078:0* disable &#39;member&#39;Took 0.7507 secondshbase(main):079:0&gt;hbase(main):080:0* desc &#39;member&#39;Table member is DISABLEDmemberCOLUMN FAMILIES DESCRIPTION&#123;NAME &#x3D;&gt; &#39;address&#39;, VERSIONS &#x3D;&gt; &#39;1&#39;, EVICT_BLOCKS_ON_CLOSE &#x3D;&gt; &#39;false&#39;, NEW_VERSION_BEHAVIOR &#x3D;&gt; &#39;false&#39;, KEEP_DELETED_CELLS &#x3D;&gt; &#39;FALSE&#39;, CACHE_DATA_ON_WRITE &#x3D;&gt; &#39;false&#39;, DATA_BLOCK_ENCODING &#x3D;&gt; &#39;NONE&#39;, TTL &#x3D;&gt; &#39;FOREVER&#39;, MIN_VERSIONS &#x3D;&gt; &#39;0&#39;, REPLICATION_SCOPE &#x3D;&gt; &#39;0&#39;, BLOOMFILTER &#x3D;&gt; &#39;ROW&#39;, CACHE_INDEX_ON_WRITE &#x3D;&gt; &#39;false&#39;, IN_MEMORY &#x3D;&gt; &#39;false&#39;, CACHE_BLOOMS_ON_WRITE &#x3D;&gt; &#39;false&#39;, PREFETCH_BLOCKS_ON_OPEN &#x3D;&gt; &#39;false&#39;, COMPRESSION &#x3D;&gt; &#39;NONE&#39;, BLOCKCACHE &#x3D;&gt; &#39;true&#39;, BLOCKSIZE &#x3D;&gt; &#39;65536&#39;&#125;&#123;NAME &#x3D;&gt; &#39;info&#39;, VERSIONS &#x3D;&gt; &#39;1&#39;, EVICT_BLOCKS_ON_CLOSE &#x3D;&gt; &#39;false&#39;, NEW_VERSION_BEHAVIOR &#x3D;&gt; &#39;false&#39;, KEEP_DELETED_CELLS &#x3D;&gt; &#39;FALSE&#39;, CACHE_DATA_ON_WRITE &#x3D;&gt; &#39;false&#39;, DATA_BLOCK_ENCODING &#x3D;&gt; &#39;NONE&#39;, TTL &#x3D;&gt; &#39;FOREVER&#39;, MIN_VERSIONS &#x3D;&gt; &#39;0&#39;, REPLICATION_SCOPE &#x3D;&gt; &#39;0&#39;, BLOOMFILTER &#x3D;&gt; &#39;ROW&#39;, CACHE_INDEX_ON_WRITE &#x3D;&gt; &#39;false&#39;, IN_MEMORY &#x3D;&gt; &#39;false&#39;, CACHE_BLOOMS_ON_WRITE &#x3D;&gt; &#39;false&#39;, PREFETCH_BLOCKS_ON_OPEN &#x3D;&gt; &#39;false&#39;, COMPRESSION &#x3D;&gt; &#39;NONE&#39;, BLOCKCACHE &#x3D;&gt; &#39;true&#39;, BLOCKSIZE &#x3D;&gt; &#39;65536&#39;&#125;2 row(s)QUOTAS0 row(s)Took 0.0326 secondshbase(main):081:0&gt; drop &#39;member&#39;Took 0.2391 secondshbase(main):082:0&gt; desc &#39;member&#39;ERROR: Table member does not exist.For usage try &#39;help &quot;describe&quot;&#39;Took 0.0035 secondshbase(main):083:0&gt; listTABLEuser1 row(s)Took 0.0030 seconds&#x3D;&gt; [&quot;user&quot;]","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HBASE","slug":"HBASE","permalink":"http://yoursite.com/tags/HBASE/"}]},{"title":"å®‰è£…é…ç½®HBASE","slug":"å®‰è£…é…ç½®HBASE","date":"2020-06-10T13:27:13.000Z","updated":"2020-06-10T13:27:13.824Z","comments":true,"path":"2020/06/10/å®‰è£…é…ç½®HBASE/","link":"","permalink":"http://yoursite.com/2020/06/10/%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AEHBASE/","excerpt":"","text":"å®‰è£…é…ç½®HBASEä¸€ã€å®‰è£… å®‰è£…åŒ…1wget https://mirrors.huaweicloud.com/apache/hbase/2.2.2/hbase-2.2.2-bin.tar.gz è§£å‹é…ç½®.bash_profile1234567tar -zxvf hbase-2.2.2-bin.tar.gz -C ..&#x2F;vim ~&#x2F;.bash_profileexport HBASH_HOME&#x3D;..export $PATH&#x3D;$HBASH_HOME&#x2F;bin:$PATHsource ~&#x2F;.bash_profile äºŒã€é…ç½® vim $HBASE_HOME/conf/hbase_env.sh 1234export JAVA_HOME&#x3D;...export HBASE_MANAGES_ZK&#x3D;false &#x2F;&#x2F;ä¸ä½¿ç”¨HBASEä¸­çš„zk vim $HBASE_HOME/conf/hbase_site.xml 1234567891011121314&lt;property&gt; &lt;name&gt;hbase.rootdir&lt;&#x2F;name&gt; &lt;value&gt;hdfs:&#x2F;&#x2F;localhost:9000&#x2F;hbase&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;&#x2F;name&gt; &lt;value&gt;true&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;&#x2F;name&gt; &lt;value&gt;localhost:2181&lt;&#x2F;value&gt;&lt;&#x2F;property&gt; ä¸‰ã€å¯åŠ¨ å…ˆå¯åŠ¨HDFS, ZK start-hbase.sh jps12HMasterHRegionServer","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HBASE","slug":"HBASE","permalink":"http://yoursite.com/tags/HBASE/"}]},{"title":"ç½‘ç‚¹æ’åˆ—ç»„åˆè®¡ç®—è·ç¦»","slug":"ç½‘ç‚¹æ’åˆ—ç»„åˆè®¡ç®—è·ç¦»","date":"2020-06-08T10:32:02.000Z","updated":"2020-06-08T10:32:02.313Z","comments":true,"path":"2020/06/08/ç½‘ç‚¹æ’åˆ—ç»„åˆè®¡ç®—è·ç¦»/","link":"","permalink":"http://yoursite.com/2020/06/08/%E7%BD%91%E7%82%B9%E6%8E%92%E5%88%97%E7%BB%84%E5%90%88%E8%AE%A1%E7%AE%97%E8%B7%9D%E7%A6%BB/","excerpt":"","text":"ç½‘ç‚¹æ’åˆ—ç»„åˆè®¡ç®—è·ç¦» æ ¹æ®oss_id, lon, latä¸‰ä¸ªå­—æ®µæ¥è®¡ç®—æ¯ä¸¤ä¸ªç½‘ç‚¹ä¹‹é—´çš„ç©ºé—´è·ç¦» 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148package OssDistanceimport org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;import org.apache.spark.sql.functions.colimport org.apache.spark.sql.types.&#123;DoubleType, IntegerType&#125;import scala.collection.mutable.ListBufferimport Math.&#123;PI, pow, cos, sin, asin, sqrt&#125;object OssDistance &#123; def main(args: Array[String]): Unit = &#123; if (args.length &lt; 2) &#123; System.err.println(\"Usage OssDistance &lt;ossLimitNum&gt; &lt;savePath&gt;\") System.exit(1) &#125; val Array(ossLimitNum, savePath) = args val spark = SparkSession.builder().getOrCreate() val ossDF = _getOssDF(spark, ossLimitNum.toInt) val (ossIds, ossLocation) = _getOssData(ossDF) val combinations = _getOssIdsCombinations(ossIds) val combinationsRDD = spark.sparkContext.parallelize(combinations) val ossLocationBroadcast = spark.sparkContext.broadcast(ossLocation) val ossDistance = combinationsRDD.map&#123; ossCombination =&gt; &#123; val ossAId = ossCombination._1 val ossBId = ossCombination._2 val ossLocationData = ossLocationBroadcast.value val ossALon = ossLocationData(ossAId)(\"lon\") val ossALat = ossLocationData(ossAId)(\"lat\") val ossBLon = ossLocationData(ossBId)(\"lon\") val ossBLat = ossLocationData(ossBId)(\"lat\") val distance = _computeDistance(ossALon, ossALat, ossBLon, ossBLat) ossDistanceR(ossAId, ossBId, distance) &#125; &#125; val ossDistanceDF = spark.createDataFrame(ossDistance) ossDistanceDF.write.format(\"json\").save(savePath) ossDistanceDF.show(10) &#125; /** * @Desc ç½‘ç‚¹æ•°æ® * @Date 2:23 ä¸‹åˆ 2020/6/8 * @Param [spark] * @Return DataSet **/ def _getOssDF(spark: SparkSession, ossLimitNum:Int): DataFrame = &#123; spark.read.format(\"jdbc\") .option(\"url\", \"jdbc:mysql://\"+Tools.getConfig(\"host\")+\"/dw_2_basic\") .option(\"driver\", \"com.mysql.jdbc.Driver\") .option(\"dbtable\", Tools.getConfig(\"defaultDb\")) .option(\"user\", Tools.getConfig(\"USER\")) .option(\"password\", Tools.getConfig(\"password\")) .load().select( col(\"omsç½‘ç‚¹ID\").cast(IntegerType).as(\"oss_id\"), col(\"ç»åº¦\").cast(DoubleType).as(\"lon\"), col(\"çº¬åº¦\").cast(DoubleType).as(\"lat\") ).limit(ossLimitNum) &#125; /** * @Desc æ ¹æ®ç½‘ç‚¹DF è¾“å‡ºç½‘ç‚¹IDåˆ—è¡¨å’Œç½‘ç‚¹ä½ç½®Map * @Date 2:22 ä¸‹åˆ 2020/6/8 * @Param [ossDF] * @Return (ListBuffer[Int], Map[Int, Map[String, Double]]) **/ def _getOssData(ossDF:DataFrame): (ListBuffer[Int], Map[Int, Map[String, Double]]) = &#123; val ossList = ossDF.collectAsList() val ossCount = ossDF.count().toInt var ossLocation: Map[Int, Map[String, Double]] = Map() val ossIds: ListBuffer[Int] = ListBuffer() for (i &lt;- 0 to ossCount-1) &#123; val ossRow = ossList.get(i) val ossId:Int = ossRow(0).asInstanceOf[Int] val lon:Double = ossRow(1).asInstanceOf[Double] val lat:Double = ossRow(2).asInstanceOf[Double] ossLocation += (ossId -&gt; Map(\"lon\" -&gt; lon, \"lat\" -&gt; lat)) ossIds.append(ossId) &#125; (ossIds, ossLocation) &#125; /** * @Desc ç½‘ç‚¹IDæ’åˆ—ç»„åˆ * @Date 2:34 ä¸‹åˆ 2020/6/8 * @Param [ossIds] * @Return scala.collection.mutable.ListBuffer&lt;scala.Tuple2&lt;java.lang.Object,java.lang.Object&gt;&gt; **/ def _getOssIdsCombinations(ossIds: ListBuffer[Int]): ListBuffer[(Int, Int)] = &#123; val ossIdsLength = ossIds.length val combinations: ListBuffer[(Int, Int)] = ListBuffer() for (i &lt;- 0 to ossIdsLength - 1) &#123; val ossId = ossIds(i) val leftOss = ossIds.slice(i+1, ossIdsLength) for (id &lt;- leftOss) &#123; combinations.append((ossId, id)) &#125; &#125; combinations &#125; /** * @Desc ç½‘ç‚¹IDä¹‹é—´çš„ç©ºé—´è·ç¦» * @Date 2:39 ä¸‹åˆ 2020/6/8 * @Param [lon1, lat1, lon2, lat2] * @Return double **/ def _computeDistance(lonA:Double, latA:Double, lonB:Double, latB:Double): Double = &#123; val Array(lon1, lat1, lon2, lat2) = Array(lonA, latA, lonB, latB).map(_ * PI / 180) val dLon = lon2 - lon1 val dLat = lat2 - lat1 val a = pow(sin(dLat/2), 2) + cos(lat1) * cos(lat2) * pow(sin(dLon/2), 2) val c = 2 * asin(sqrt(a)) val r = 6371 c * r &#125; case class ossDistanceR(oss1: Int, oss2: Int, distance: Double)&#125;","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkStreamingæ¶ˆè´¹Kakfaè¾“å‡ºæ—¥å¿—","slug":"SparkStreamingæ¶ˆè´¹Kakfaè¾“å‡ºæ—¥å¿—","date":"2020-06-04T08:49:12.000Z","updated":"2020-06-04T08:49:12.265Z","comments":true,"path":"2020/06/04/SparkStreamingæ¶ˆè´¹Kakfaè¾“å‡ºæ—¥å¿—/","link":"","permalink":"http://yoursite.com/2020/06/04/SparkStreaming%E6%B6%88%E8%B4%B9Kakfa%E8%BE%93%E5%87%BA%E6%97%A5%E5%BF%97/","excerpt":"","text":"SparkStreamingæ¶ˆè´¹Kakfaè¾“å‡ºæ—¥å¿—SparkStreamingä»£ç 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273package workuseimport kafka.serializer.StringDecoderimport org.apache.spark.SparkConfimport org.apache.spark.streaming.kafka.KafkaUtilsimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object StreamingConsumeKafka &#123; def main(args: Array[String]): Unit = &#123; if (args.length &lt; 3) &#123; System.err.println(\"Usage: StreamingConsumeKafka &lt;bootstrap.servers&gt; &lt;topics&gt; &lt;filterType&gt;\") System.exit(1) &#125; val Array(bootstrapServers, topicsStr, filterType) = args val topics:Set[String] = topicsStr.split(\",\").toSet val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(\"StreamingConsumeKafka\") val ssc = new StreamingContext(sparkConf, Seconds(60)) ssc.sparkContext.setLogLevel(\"WARN\") val kafkaParams:Map[String, String] = Map(\"bootstrap.servers\"-&gt;bootstrapServers) val streamData = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topics) val resData = streamData.map(_._2).map&#123; logStr =&gt; &#123; val logData = logStr.split(\"\\t\") val ip = logData(0) val time = logData(1) val path = logData(2) val reqMethod = path.split(\" \")(0) val pagePath = path.split(\" \")(1) val coursePage = pagePath.split(\"/\")(2) val courseId = coursePage.substring(0, coursePage.indexOf(\".\")).toInt val statusCode = logData(3).toInt val queryPath = logData(4) val searchEngine = queryPath.split('.')(1) val keyword = queryPath.split(\"=\")(1) WebVisitLog(ip, time, reqMethod, path, pagePath, courseId, statusCode, queryPath, searchEngine, keyword) &#125; &#125;.filter&#123; visitLog =&gt; &#123; val pagePath = visitLog.pagePath pagePath.split(\"/\")(1) == filterType &#125; &#125; resData.print() ssc.start() ssc.awaitTermination() &#125; case class WebVisitLog( ip: String, time: String, reqMethod: String, path: String, pagePath: String, courseId: Int, statusCode: Int, queryPath: String, searchEngine: String, keyWord: String )&#125; ä¾èµ–jaråŒ… spark-streaming-kafka-0-8_2.11-2.4.5.jar metrics-core-2.2.0.jar kafka_2.11-0.8.2.1.jar å¯ä»¥æ·»åŠ åˆ°${SPARK_HOME}/jars submitå‘½ä»¤1spark-submit --class workuse.StreamingConsumeKafka --name StreamingConsumeKafka --master \"local[2]\" --jars ~/jars/spark-streaming-kafka-0-8_2.11-2.4.5.jar ~/wk_jars/workuse-1.0.jar localhost:9092 flume_log class","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"sparkStreaming","slug":"sparkStreaming","permalink":"http://yoursite.com/tags/sparkStreaming/"}]},{"title":"Python+Flume+Kafka æ—¥å¿—ç”Ÿæˆé‡‡é›†","slug":"Python-Flume-Kafka-æ—¥å¿—ç”Ÿæˆé‡‡é›†","date":"2020-06-03T07:28:28.000Z","updated":"2020-06-03T07:28:28.513Z","comments":true,"path":"2020/06/03/Python-Flume-Kafka-æ—¥å¿—ç”Ÿæˆé‡‡é›†/","link":"","permalink":"http://yoursite.com/2020/06/03/Python-Flume-Kafka-%E6%97%A5%E5%BF%97%E7%94%9F%E6%88%90%E9%87%87%E9%9B%86/","excerpt":"","text":"Python+Flume+Kafka æ—¥å¿—ç”Ÿæˆé‡‡é›†ç”Ÿæˆæ—¥å¿—123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869#!/usr/bin/env python# coding=utf-8import loggingimport osimport randomimport datetimelog_path = os.getcwd()ip_list = [23, 12, 103, 88, 120, 48, 22, 101, 96, 130, 104, 74, 53, 13, 41]path_list = ['/class/12.html', '/class/123.html', '/class/142.html', '/class/90.html', '/class/241.html', '/class/88.html', '/learn/123.html', '/learn/140.html', '/buy/131.html', '/buy/103.html']status_code = [200, 404, 500, 502, 303]source_list = [\"https://www.baidu.com/s?wd=\", \"https://www.sogou.com/web?query=\", \"https://cn.bing.com/search?q=\"]keywords = [\"kafka\", \"spark\", \"flink\", \"flume\", \"hadoop\"]class Logger: def __init__(self, loggername): self.logger = logging.getLogger(loggername) self.logger.setLevel(logging.INFO) log_path = os.getcwd() logname = log_path + \"/access.log\" fh = logging.FileHandler(logname, encoding=\"utf-8\") fh.setLevel(logging.INFO) ch = logging.StreamHandler() ch.setLevel(logging.INFO) #formatter = logging.Formatter('&#123;ip&#125;\\t%(asctime)s\\t&#123;path&#125;'.format(ip=\"192.168.214.59\", path=\"GET /class/192.html HTTP/1.1\")) formatter = logging.Formatter('%(message)s') fh.setFormatter(formatter) #ch.setFormatter(formatter) self.logger.addHandler(fh) #self.logger.addHandler(ch) def get_log(self): return self.logger def add_log(self, log_num=100): message = \"&#123;ip&#125;\\t&#123;log_time&#125;\\t&#123;path&#125;\\t&#123;code&#125;\\t&#123;source&#125;\" for i in range(100): ips = random.sample(ip_list, 4) ip = \".\".join([str(n) for n in ips]) log_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") path = \"GET &#123;p&#125; HTTP/1.1\".format(p=random.sample(path_list, 2)[0]) code = random.sample(status_code, 1)[0] source = random.sample(source_list, 1)[0] + random.sample(keywords, 1)[0] msg = message.format(ip=ip, log_time=log_time, path=path, code=code, source=source) self.logger.info(msg)if __name__ == \"__main__\": Logger(\"lg\").add_log() crontabå®šæ—¶æ‰§è¡Œ Flumeæ”¶é›†æ—¥å¿—åˆ°Kafkaè¿œç¨‹é€šè¿‡ipè®¿é—®ï¼Œé…ç½®ä¿®æ”¹server.propertiesadvertised.listeners=PLAINTEXT://IP:9092 exec-memory-kafka.conf123456789101112131415exec-kafka-agent.sources &#x3D; exec-kafka-sourceexec-kafka-agent.sinks &#x3D; exec-kafka-sinkexec-kafka-agent.channels &#x3D; exec-kafka-channelexec-kafka-agent.sources.exec-kafka-source.type &#x3D; execexec-kafka-agent.sources.exec-kafka-source.command &#x3D; tail -F &#x2F;home&#x2F;kowhoy&#x2F;code&#x2F;log&#x2F;access.logexec-kafka-agent.sinks.exec-kafka-sink.type &#x3D; org.apache.flume.sink.kafka.KafkaSinkexec-kafka-agent.sinks.exec-kafka-sink.kafka.bootstrap.servers &#x3D; localhost:9092exec-kafka-agent.sinks.exec-kafka-sink.kafka.topic &#x3D; flume_logexec-kafka-agent.channels.exec-kafka-channel.type &#x3D; memoryexec-kafka-agent.sources.exec-kafka-source.channels &#x3D; exec-kafka-channelexec-kafka-agent.sinks.exec-kafka-sink.channel &#x3D; exec-kafka-channel åå°è¿è¡Œ1flume-ng agent --conf $FLUME_HOME/conf --name exec-kafka-agent --conf-file exec-memory-kafka.conf -Dflume.root.logger=INFO,console &amp; æ¶ˆè´¹1kafka-console-consumer.sh --bootstrap-server ecs01:9092 --topic flume_log --from-beginning","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"http://yoursite.com/tags/Flume/"},{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"Kafka","slug":"Kafka","permalink":"http://yoursite.com/tags/Kafka/"}]},{"title":"SparkStreamingæ•´åˆKafka","slug":"SparkStreamingæ•´åˆKafka","date":"2020-06-03T02:39:01.000Z","updated":"2020-07-08T02:22:38.346Z","comments":true,"path":"2020/06/03/SparkStreamingæ•´åˆKafka/","link":"","permalink":"http://yoursite.com/2020/06/03/SparkStreaming%E6%95%B4%E5%90%88Kafka/","excerpt":"","text":"SparkStreamingæ•´åˆKafka Spark2.3ç‰ˆæœ¬ä¹‹å‰å¯ä»¥æ”¯æŒbroker0-8å’Œ0-10ä¸¤ä¸ªstreaming-kafkaç‰ˆæœ¬ï¼Œ2.3ç‰ˆæœ¬ä¹‹åä¸å†æ”¯æŒ0-8 0-8ç‰ˆæœ¬å¯ä»¥ä½¿ç”¨ Receviers-based å’Œ Direct ä¸¤ç§æ–¹å¼æ¥æ”¶Kafkaæ•°æ® 0-10ç‰ˆæœ¬ä½¿ç”¨Kafka010æ“ä½œ spark-streaming-kafka-0-8 å¯ä»¥ä½¿ç”¨ Receviers-based å’Œ Direct ä¸¤ç§æ–¹å¼ Receivers-basedæ˜¯åŸºäºreceiverså’Œé«˜çº§APIè¿›è¡Œæ¥æ”¶æ•°æ® ç‰¹ç‚¹ è¿™ç§æ–¹å¼ï¼Œä½¿ç”¨æ¥æ”¶å™¨ï¼Œæ¥æ”¶å™¨ä¸€ç›´å¤„äºç­‰å¾…æ¥æ”¶çš„çŠ¶æ€ï¼Œå°±ä¼šæµªè´¹èµ„æºï¼› åœ¨é»˜è®¤é…ç½®æƒ…å†µä¸‹ï¼Œå¦‚æœå‘ç”Ÿæ•…éšœå¯èƒ½ä¼šä¸¢å¤±æ•°æ®ï¼Œä¸è¿‡å¯ä»¥é€šè¿‡SparkStreamingä¸­å¯åŠ¨write Ahead Logsï¼Œå°†kafkaçš„æ•°æ®å†™å…¥åˆ°æ—¥å¿—ä¸­(å¦‚hdfs)ï¼Œå‘ç”Ÿæ•…éšœæ—¶å€™å¯ä»¥é€šè¿‡æ—¥å¿—æ¢å¤ GAV123groupId &#x3D; org.apache.spark artifactId &#x3D; spark-streaming-kafka-0-8_2.11 version &#x3D; 2.2.2 æ–¹æ³•1234import org.apache.spark.streaming.kafka._ val kafkaStream &#x3D; KafkaUtils.createStream(streamingContext, [ZK quorum], [consumer group id], [per-topic number of Kafka partitions to consume]) ç¤ºä¾‹123456789101112131415161718192021import org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import org.apache.spark.streaming.kafka.KafkaUtilsobject KafkaReceiverApp &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(\"KafkaReceiver\") val ssc = new StreamingContext(sparkConf, Seconds(5)) ssc.sparkContext.setLogLevel(\"ERROR\") val topicMap = Map[String, Int](\"kafkaspark\"-&gt;1) val streamData = KafkaUtils.createStream(ssc, \"localhost:2181\", \"groupId\", topicMap) streamData.print() ssc.start() ssc.awaitTermination() &#125;&#125; æ³¨æ„ Kafkaä¸­çš„topicåˆ†åŒºå’ŒSparkStreamingä¸­ç”Ÿæˆçš„RDDçš„åˆ†åŒºä¸ç›¸å…³ï¼Œå¢åŠ KafkaUtils.createStream()ä¸­ç‰¹å®štopicåˆ†åŒºçš„æ•°é‡åªä¼šå¢åŠ åœ¨å•ä¸ªæ¥æ”¶å™¨ä¸­ä½¿ç”¨topicçš„çº¿ç¨‹æ•°é‡ï¼Œä¸ä¼šåœ¨å¤„ç†æ•°æ®çš„æ—¶å€™å¢åŠ sparkçš„å¹¶è¡Œæ€§ å¯ä»¥ä½¿ç”¨ä¸åŒçš„ç»„å’Œtopicæ¥åˆ›å»ºä¸åŒçš„recevieræ¥æ¥æ”¶æ•°æ® å¦‚æœè¦å¼€å¯write ahead logsï¼Œä½¿ç”¨KafkaUtils.createStream(â€¦, StorageLevel.MEMORY_AND_DISK_SER) Direct Approachæ— æ¥æ”¶å™¨ï¼Œå®šæœŸæŸ¥è¯¢çš„æ–¹å¼ ç‰¹ç‚¹ ç¡®ä¿äº†æ›´å¼ºçš„ç«¯åˆ°ç«¯ã€‚è¿™ç§æ–¹å¼ä¸æ˜¯ä½¿ç”¨æ¥æ”¶å™¨æ¥æ¥æ”¶æ•°æ®ï¼Œæ˜¯å®šæœŸå‘kafkaæŸ¥è¯¢æ¯ä¸ªä¸»é¢˜+åˆ†åŒºçš„æœ€æ–°å˜åŒ–é‡ ç®€åŒ–äº†å¹¶è¡Œæ€§ï¼šä¸éœ€è¦åˆ›å»ºå¤šä¸ªè¾“å…¥Kafkaæµå¹¶åˆå¹¶ï¼Œä½¿ç”¨directStreamï¼Œ SparkStreaming å°†åˆ›å»ºä¸è¦ä½¿ç”¨Kafkaåˆ†åŒºä¸€æ ·å¤šçš„RDDåˆ†åŒºï¼Œæ‰€ä»¥è¿™äº›åˆ†åŒºæ˜¯å¯ä»¥å¹¶è¡ŒåŒ–è¯»å–çš„ æ•ˆç‡æ›´å¥½ï¼Œç¬¬ä¸€ç§æ–¹å¼å¦‚æœè¦ç¡®ä¿æ•…éšœæ•°æ®å¯æ¢å¤ï¼Œå°±å¾—å¼€å¯write ahead logsï¼Œè¿™ç§æ–¹å¼å°±ä¼šæ•°æ®å¤åˆ¶è¿‡æ¥è¿˜å¾—å†å­˜ä¸€éæ—¥å¿—ã€‚directæ–¹å¼ï¼Œåªè¦æ•°æ®åœç•™åœ¨kafkaé‡Œé¢å°±å¥½äº† GAV123groupId &#x3D; org.apache.sparkartifactId &#x3D; spark-streaming-kafka-0-8_2.12version &#x3D; 2.4.5 æ–¹æ³•12345import org.apache.spark.streaming.kafka._ val directKafkaStream &#x3D; KafkaUtils.createDirectStream[ [key class], [value class], [key decoder class], [value decoder class] ]( streamingContext, [map of Kafka parameters], [set of topics to consume]) åœ¨kafkaå‚æ•°ä¸­ï¼Œå¿…é¡»æŒ‡å®šmetadata.broker.list æˆ–è€… bootstrap.servers é»˜è®¤æƒ…å†µä¸‹ï¼Œä¼šæ€»æœ€æ–°çš„åç§»é‡è¿›è¡Œæ¶ˆè€—ï¼Œå¦‚æœè¦ä»å¼€å§‹ä½ç½®æ¶ˆè´¹ï¼Œéœ€è¦å°†auto.offset.resetè®¾ç½®ä¸ºsmallest ç¤ºä¾‹123456789101112131415161718192021import kafka.serializer.StringDecoderimport org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import org.apache.spark.streaming.kafka.KafkaUtilsobject KafkaReceiverApp &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(\"KafkaReceiver\") val ssc = new StreamingContext(sparkConf, Seconds(5)) ssc.sparkContext.setLogLevel(\"ERROR\") val streamData = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, Map(\"metadata.broker.list\"-&gt;\"localhost:9092\", \"auto.offset.reset\"-&gt;\"smallest\"), Set(\"kafkaspark\")) streamData.print() ssc.start() ssc.awaitTermination() &#125;&#125; spark-streaming-kafka-0-10 åªæ”¯æŒDirectæ¨¡å¼ GAV123groupId &#x3D; org.apache.sparkartifactId &#x3D; spark-streaming-kafka-0-10_2.12 # 2.12 scalaçš„ç‰ˆæœ¬version &#x3D; $&#123;spark-version&#125; âš ï¸æ³¨æ„ï¼š ä¸éœ€è¦å†æ‰‹åŠ¨æ·»åŠ  org.apache.kafkaç»„å»ºçš„ç›¸å…³ä¾èµ–äº†ï¼Œæ¯”å¦‚ï¼škafka-client, å› ä¸ºspark-straming-kafka-0-10å·²ç»å’Œç›¸å…³çš„ä¾èµ–äº† åˆ›å»ºDirect Stream1234567891011121314151617181920212223import org.apache.kafka.clients.consumer.ConsumerRecordimport org.apache.kafka.common.serialization.StringDeserializerimport org.apache.spark.streaming.kafka010._import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistentimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribeval kafkaParams = Map[String, Object]( \"bootstrap.servers\" -&gt; \"localhost:9092,anotherhost:9092\", \"key.deserializer\" -&gt; classOf[StringDeserializer], // keyçš„ååºåˆ—åŒ– \"value.deserializer\" -&gt; classOf[StringDeserializer], // valueçš„ååºåˆ—åŒ– \"group.id\" -&gt; \"use_a_separate_group_id_for_each_stream\", \"auto.offset.reset\" -&gt; \"latest\", // offsetè®¾ç½® [latest, none, earilest] \"enable.auto.commit\" -&gt; (false: java.lang.Boolean) // æ˜¯å¦è‡ªåŠ¨æäº¤ï¼Œå¦‚æœæœ‰streamä¼šäº§ç”Ÿæ“ä½œçš„è¯ï¼Œå¯ä»¥ä¸ä½¿ç”¨è‡ªåŠ¨æäº¤offsetï¼Œè€Œæ˜¯å¤„ç†å®Œæ•°æ®ä¹‹åï¼Œæ‰‹åŠ¨æäº¤offset é˜²æ­¢æ•°æ®é‡å¤æ¶ˆè´¹)val topics = Array(\"topicA\", \"topicB\")val stream = KafkaUtils.createDirectStream[String, String]( streamingContext, PreferConsistent, //åˆ†åŒºç­–ç•¥ [PreferConsistent, PreferBrokers, PreferFixed] Subscribe[String, String](topics, kafkaParams) //æ¶ˆè´¹ç­–ç•¥)stream.map(record =&gt; (record.key, record.value)) è·å–åˆ°offset1234567stream.foreachRDD &#123; rdd =&gt; val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges rdd.foreachPartition &#123; iter =&gt; val o: OffsetRange = offsetRanges(TaskContext.get.partitionId) println(s\"$&#123;o.topic&#125; $&#123;o.partition&#125; $&#123;o.fromOffset&#125; $&#123;o.untilOffset&#125;\") &#125;&#125; ä¿å­˜offset æœ‰ä¸‰ç§æ–¹å¼ CheckPoints (ä¸æ¨èä½¿ç”¨) Kafkaè‡ªèº« å¤–éƒ¨å­˜å‚¨ åœ¨kafka-0-8ä¸­ï¼Œå¯ä»¥ä½¿ç”¨zookeeperé‡Œç®¡ç†offset ç›®å½•ç»“æ„æ˜¯/consumers/&lt;group.id&gt;/offsets/ &lt;topic&gt;/&lt;partitionId&gt; ä½†æ˜¯ç”±äº zookeeper çš„å†™å…¥èƒ½åŠ›å¹¶ä¸ä¼šéšç€ zookeeper èŠ‚ç‚¹æ•°é‡çš„å¢åŠ è€Œæ‰©å¤§ï¼Œå› è€Œï¼Œå½“å­˜åœ¨é¢‘ç¹çš„ Offset æ›´æ–°æ—¶ï¼ŒZOOKEEPER é›†ç¾¤æœ¬èº«å¯èƒ½æˆä¸ºç“¶é¢ˆã€‚å› è€Œï¼Œä¸æ¨èé‡‡ç”¨è¿™ç§æ–¹å¼ã€‚ åœ¨0-10ä¸­ï¼Œkafkaè‡ªèº«çš„ä¸€ä¸ªç‰¹æ®Š Topicï¼ˆ__consumer_offsetsï¼‰ä¸­ï¼šè¿™ç§æ–¹å¼æ”¯æŒå¤§ååé‡çš„Offset æ›´æ–°ï¼Œåˆä¸éœ€è¦æ‰‹åŠ¨ç¼–å†™ Offset ç®¡ç†ç¨‹åºæˆ–è€…ç»´æŠ¤ä¸€å¥—é¢å¤–çš„é›†ç¾¤ï¼Œå› è€Œæ˜¯åˆé€‚çš„å®ç°æ–¹å¼ã€‚","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/tags/kafka/"},{"name":"sparkStreaming","slug":"sparkStreaming","permalink":"http://yoursite.com/tags/sparkStreaming/"}]},{"title":"SparkStreamingæ•´åˆFlume ","slug":"SparkStreamingæ•´åˆFlume","date":"2020-06-01T08:31:17.000Z","updated":"2020-06-01T08:31:17.492Z","comments":true,"path":"2020/06/01/SparkStreamingæ•´åˆFlume/","link":"","permalink":"http://yoursite.com/2020/06/01/SparkStreaming%E6%95%B4%E5%90%88Flume/","excerpt":"","text":"SparkStreamingæ•´åˆFlume å¯ä»¥é€šè¿‡Push å’Œ Pullä¸¤ç§æ–¹å¼æ•´åˆPush: æ˜¯Flume Pushåˆ°Spark, ç¼ºç‚¹æ˜¯åªèƒ½ä¸€ä¸ªexecutorè¿›è¡Œå¤„ç†ï¼Œé€ æˆå‹åŠ›Pull: æ˜¯Sparkä¸»åŠ¨å»Pullæ¨èä½¿ç”¨Pullæ–¹å¼ Pushæ–¹å¼ï¼Œå…ˆæ‰§è¡ŒSpark,å†æ‰§è¡ŒFlume Pullæ–¹å¼ï¼Œå…ˆæ‰§è¡ŒFlume,å†æ‰§è¡ŒSpark Pushæ–¹å¼Flume.confnetcat-memory-avro.conf 12345678910111213141516netcat-avro-agent.sources &#x3D; netcat-avro-sourcenetcat-avro-agent.sinks &#x3D; netcat-avro-sinknetcat-avro-agent.channels &#x3D; netcat-avro-channelnetcat-avro-agent.sources.netcat-avro-source.type &#x3D; netcatnetcat-avro-agent.sources.netcat-avro-source.bind &#x3D; localhostnetcat-avro-agent.sources.netcat-avro-source.port &#x3D; 44444netcat-avro-agent.sinks.netcat-avro-sink.type &#x3D; avronetcat-avro-agent.sinks.netcat-avro-sink.hostname &#x3D; localhostnetcat-avro-agent.sinks.netcat-avro-sink.port &#x3D; 55555netcat-avro-agent.channels.netcat-avro-channel.type &#x3D; memorynetcat-avro-agent.sources.netcat-avro-source.channels &#x3D; netcat-avro-channelnetcat-avro-agent.sinks.netcat-avro-sink.channel &#x3D; netcat-avro-channel GAV123groupId &#x3D; org.apache.sparkartifactId &#x3D; spark-streaming-flume_2.12version &#x3D; 2.4.5 FlumePushStreaming.scala1234567891011121314151617181920212223package sk_demoimport org.apache.spark.SparkConfimport org.apache.spark.streaming.flume.FlumeUtilsimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object FlumePushStreaming &#123; def main(args: Array[String]): Unit &#x3D; &#123; val sparkConf &#x3D; new SparkConf().setAppName(&quot;FlumePushStreaming&quot;).setMaster(&quot;local[2]&quot;) val ssc &#x3D; new StreamingContext(sparkConf, Seconds(5)) ssc.sparkContext.setLogLevel(&quot;WARN&quot;) val streamData &#x3D; FlumeUtils.createStream(ssc, &quot;localhost&quot;, 55555) val res &#x3D; streamData.flatMap(x &#x3D;&gt; new String(x.event.getBody.array()).toString.split(&quot; &quot;)).map((_, 1)).reduceByKey(_+_) res.print() ssc.start() ssc.awaitTermination() &#125;&#125; Pullæ–¹å¼Flume.confnetcat-memory-spark.conf 12345678910111213141516netcat-spark-agent.sources &#x3D; netcat-spark-sourcenetcat-spark-agent.sinks &#x3D; netcat-spark-sinknetcat-spark-agent.channels &#x3D; netcat-spark-channelnetcat-spark-agent.sources.netcat-spark-source.type &#x3D; netcatnetcat-spark-agent.sources.netcat-spark-source.bind &#x3D; localhostnetcat-spark-agent.sources.netcat-spark-source.port &#x3D; 44444netcat-spark-agent.sinks.netcat-spark-sink.type &#x3D; org.apache.spark.streaming.flume.sink.SparkSinknetcat-spark-agent.sinks.netcat-spark-sink.hostname &#x3D; localhostnetcat-spark-agent.sinks.netcat-spark-sink.port &#x3D; 55555netcat-spark-agent.channels.netcat-spark-channel.type &#x3D; memorynetcat-spark-agent.sources.netcat-spark-source.channels &#x3D; netcat-spark-channelnetcat-spark-agent.sinks.netcat-spark-sink.channel &#x3D; netcat-spark-channel GAV1234567groupId &#x3D; org.apache.sparkartifactId &#x3D; spark-streaming-flume-sink_2.12version &#x3D; 2.4.5groupId &#x3D; org.apache.commonsartifactId &#x3D; commons-lang3version &#x3D; 3.5 FlumePullStreaming.scala123456789101112131415161718192021222324package sk_demoimport org.apache.spark.SparkConfimport org.apache.spark.streaming.flume.FlumeUtilsimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object FlumePullStreaming &#123; def main(args: Array[String]): Unit &#x3D; &#123; val sparkConf &#x3D; new SparkConf().setAppName(&quot;FlumePullStreaming&quot;).setMaster(&quot;local[2]&quot;) val ssc &#x3D; new StreamingContext(sparkConf, Seconds(5)) ssc.sparkContext.setLogLevel(&quot;WARN&quot;) val streamData &#x3D; FlumeUtils.createPollingStream(ssc, &quot;localhost&quot;, 55555) val res &#x3D; streamData.flatMap(x &#x3D;&gt; new String(x.event.getBody.array()).toString.split(&quot; &quot;)).map((_, 1)).reduceByKey(_+_) res.print() ssc.start() ssc.awaitTermination() &#125;&#125;","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"sparkStreaming","slug":"sparkStreaming","permalink":"http://yoursite.com/tags/sparkStreaming/"}]},{"title":"WEBæ—¥å¿—é‡‡é›†","slug":"WEBæ—¥å¿—é‡‡é›†","date":"2020-05-29T13:11:42.000Z","updated":"2020-05-29T13:11:42.045Z","comments":true,"path":"2020/05/29/WEBæ—¥å¿—é‡‡é›†/","link":"","permalink":"http://yoursite.com/2020/05/29/WEB%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86/","excerpt":"","text":"WEBæ—¥å¿—é‡‡é›†ä¸€ã€æµè§ˆå™¨æ—¥å¿—é‡‡é›† é¡µé¢æµè§ˆæ—¥å¿— è§£å†³PV(Page View)ã€UV(Unique Visitors) é¡µé¢æµé‡ã€é¡µé¢æ¥æºçš„é—®é¢˜ é¡µé¢äº¤äº’æ—¥å¿— ç”¨æˆ·å…´è¶£ç‚¹ã€ä½“éªŒä¼˜åŒ–ç‚¹ äºŒã€é¡µé¢æµè§ˆæ—¥å¿—1. é¡µé¢HTTPè¯·æ±‚è¿‡ç¨‹æµè§ˆå™¨å‘é€è¯·æ±‚ â€“1âƒ£ï¸â€“&gt; æœåŠ¡å™¨æ¥å—è¯·æ±‚,è¿”å›HTML â€“2âƒ£ï¸â€“&gt; æµè§ˆå™¨æ¥å—å“åº”ã€è§£ææ¸²æŸ“ è¿‡ç¨‹1âƒ£ï¸ä¸­åŒ…å«: - è¯·æ±‚è¡Œ (è¯·æ±‚æ–¹æ³•/URL/HTTPç‰ˆæœ¬å·) - è¯·æ±‚æŠ¥å¤´ (é™„åŠ ä¿¡æ¯ï¼Œå¦‚Cookie) - è¯·æ±‚æ­£æ–‡ è¿‡ç¨‹2âƒ£ï¸ä¸­åŒ…å«: - çŠ¶æ€è¡Œ (200/ 404/..) - å“åº”æŠ¥å¤´ ï¼ˆCookie..ï¼‰ - å“åº”æ­£æ–‡ 2. é¡µé¢æµè§ˆæ—¥å¿—çš„é‡‡é›†è¿‡ç¨‹æµè§ˆå™¨å‘é€è¯·æ±‚ â€”&gt;æœåŠ¡å™¨æ¥å—è¯·æ±‚,è¿”å›HTML + å†…éƒ¨å¤„ç†(HTMLåµŒå…¥æ—¥å¿—é‡‡é›†ä»£ç ) â€”&gt;æµè§ˆå™¨æ¥å—å“åº”ã€è§£ææ¸²æŸ“ â€”&gt;æ‰§è¡Œæ—¥å¿—é‡‡é›†ä»£ç å‘æ—¥å¿—æœåŠ¡å™¨å‘é€æ—¥å¿—è¯·æ±‚(ç«‹å³è¿”å›è¯·æ±‚æˆåŠŸå“åº”ï¼Œé¿å…å½±å“æ­£å¸¸åŠ è½½) â€”&gt;æ—¥å¿—æœåŠ¡å™¨è§£æ/é¢„å¤„ç†/å­˜å‚¨ 3. é¡µé¢äº¤äº’æ—¥å¿—çš„é‡‡é›†è¿‡ç¨‹å¼€å‘/ç”Ÿæˆ é‡‡é›†ä»£ç (ç»‘å®šç›‘æµ‹çš„äº¤äº’è¡Œä¸º) â€”&gt;å½“é¡µé¢äº§ç”ŸæŒ‡å®šè¡Œä¸ºï¼Œè§¦å‘é‡‡é›†ä»£ç  â€”&gt;é‡‡é›†ä»£ç é‡‡é›† + å‘é€è½¬å‚¨ 4. é¡µé¢æ—¥å¿—æ¸…æ´—/é¢„å¤„ç† è¯†åˆ«æµé‡æ”»å‡»ã€ç½‘ç»œçˆ¬è™«ã€æµé‡ä½œå¼Šç­‰ï¼Œéæ­£å¸¸é¡µé¢æ—¥å¿—å¯¼è‡´ç»Ÿè®¡åå·®å¤§ï¼Œ\bæ ¡éªŒè¯†åˆ«å‡ºéæ­£å¸¸å¹¶æ¸…æ´— æ•°æ®ç¼ºé¡¹è¡¥æ­£ï¼Œæ¯”å¦‚ç”¨æˆ·ç™»å½•ä¹‹åï¼Œè¡¥å……ç™»å½•ä¹‹å‰çš„ç”¨æˆ·ä¿¡æ¯ æ— æ•ˆæ•°æ®å‰”é™¤ï¼Œé…ç½®é”™è¯¯/è¿‡æ—¶çš„æ—¥å¿—å¾—æ¸…é™¤ï¼Œé¿å…èµ„æº(è®¡ç®—/å­˜å‚¨)æµªè´¹ æ—¥å¿—åˆ†ç¦»ï¼Œæ•æ„Ÿæ•°æ®ç­‰è¿›è¡Œç»†åˆ†éš”ç¦»åˆ†å‘","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"æ—¥å¿—é‡‡é›†","slug":"æ—¥å¿—é‡‡é›†","permalink":"http://yoursite.com/tags/%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86/"}]},{"title":"FlumePushStream","slug":"FlumePushStream","date":"2020-05-28T09:43:12.000Z","updated":"2020-05-28T09:43:12.317Z","comments":true,"path":"2020/05/28/FlumePushStream/","link":"","permalink":"http://yoursite.com/2020/05/28/FlumePushStream/","excerpt":"","text":"FlumePushStreamFlumeStreamingApp.scala1234567891011121314151617181920212223242526272829303132package sk_demoimport org.apache.spark.SparkConfimport org.apache.spark.streaming.flume.FlumeUtilsimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object FlumeStreamingApp &#123; def main(args:Array[String]): Unit = &#123; if (args.length &lt; 2) &#123; System.err.println(\"Usage: FlumeStreamingApp &lt;hostname&gt; &lt;port&gt;\") System.exit(1) &#125; val Array(hostname, port) = args val sparkConf = new SparkConf() val ssc = new StreamingContext(sparkConf, Seconds(5)) ssc.sparkContext.setLogLevel(\"WARN\") val flumeStream = FlumeUtils.createStream(ssc, hostname, port.toInt) val res = flumeStream.flatMap(x =&gt; new String(x.event.getBody().array())).split(\" \").map((_, 1)).reduceByKey(_+_) res.print() ssc.start() ssc.awaitTermination() &#125;&#125; flume-streaming.conf12345678910111213141516flume_streaming_agent.sources &#x3D; flume_streaming_sourceflume_streaming_agent.sinks &#x3D; flume_streaming_sinkflume_streaming_agent.channels &#x3D; flume_streaming_channelflume_streaming_agent.sources.flume_streaming_source.type &#x3D; netcatflume_streaming_agent.sources.flume_streaming_source.bind &#x3D; localhostflume_streaming_agent.sources.flume_streaming_source.port &#x3D; 44444flume_streaming_agent.sinks.flume_streaming_sink.type &#x3D; avroflume_streaming_agent.sinks.flume_streaming_sink.hostname &#x3D; localhostflume_streaming_agent.sinks.flume_streaming_sink.port &#x3D; 55555flume_streaming_agent.channels.flume_streaming_channel.type &#x3D; memoryflume_streaming_agent.sources.flume_streaming_source.channels &#x3D; flume_streaming_channelflume_streaming_agent.sinks.flume_streaming_sink.channel &#x3D; flume_streaming_channel ./spark-submit --packages org.apache.spark:spark-streaming-flume_2.11:2.4.5 --class workuse.FlumeStreamingApp --name FlumeStreaming --master &quot;local[2]&quot; /home/kowhoy/jars/workuse.jar localhost 55555","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"FlumePushStream","slug":"FlumePushStream","permalink":"http://yoursite.com/tags/FlumePushStream/"}]},{"title":"Streaming2Sql","slug":"Streaming2Sql","date":"2020-05-28T07:35:59.000Z","updated":"2020-05-28T07:35:59.561Z","comments":true,"path":"2020/05/28/Streaming2Sql/","link":"","permalink":"http://yoursite.com/2020/05/28/Streaming2Sql/","excerpt":"","text":"Streaming2Sql12345678910111213141516171819202122232425262728293031323334353637package sk_demoimport java.sql.DriverManagerimport org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object Streaming2Sql &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(\"Streaming2Sql\") val ssc = new StreamingContext(sparkConf, Seconds(5)) val wordStream = ssc.socketTextStream(\"localhost\", 9999) val wordCount = wordStream.flatMap(_.split(\" \")).map((_, 1)).reduceByKey(_+_) wordCount.foreachRDD(rdd =&gt; rdd.foreachPartition(line =&gt; &#123; Class.forName(\"com.mysql.jdbc.Driver\") val conn = DriverManager.getConnection(\"jdbc:mysql://localhost:3306/test\", \"root\", \"wojiushiwo \") try &#123; for (row &lt;- line) &#123; val sql = \"insert into streaming_save (word, count, save_time) values ('\" + row._1 + \"','\" + row._2 +\"', now())\" conn.prepareStatement(sql).executeUpdate() &#125; &#125; finally &#123; conn.close() &#125; &#125;)) ssc.start() ssc.awaitTermination() &#125;&#125;","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Streaming2Sql","slug":"Streaming2Sql","permalink":"http://yoursite.com/tags/Streaming2Sql/"}]},{"title":"SparkStreaming Transform","slug":"SparkStreaming-Transform","date":"2020-05-28T05:37:01.000Z","updated":"2020-05-28T05:37:01.920Z","comments":true,"path":"2020/05/28/SparkStreaming-Transform/","link":"","permalink":"http://yoursite.com/2020/05/28/SparkStreaming-Transform/","excerpt":"","text":"SparkStreaming Transform é»‘åå•è¿‡æ»¤ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package sk_demoimport org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object FilterBlackListApp &#123; def main(args: Array[String]): Unit = &#123; if (args.length &lt; 3) &#123; System.err.println(\"Usage: FilterBlackListApp &lt;hostname&gt; &lt;port&gt; &lt;hdfsPath&gt;\") System.exit(1) &#125; val Array(hostname, port, hdfsPath) = args val sparkConf = new SparkConf() val ssc = new StreamingContext(sparkConf, Seconds(5)) ssc.sparkContext.setLogLevel(\"WARN\") val blackListRDD = ssc.sparkContext.textFile(hdfsPath).flatMap(_.split(\" \")).map((_, true)) val logStream = ssc.socketTextStream(hostname, port.toInt) val logInfo = logStream.map &#123; log =&gt; &#123; val logAction = log.split(\",\") (logAction(0), UserLog(logAction(0), logAction(1))) &#125; &#125; val filteredLog = logInfo.transform( logRDD =&gt; &#123; val joinedRDD = logRDD.leftOuterJoin(blackListRDD) val filteredRDD = joinedRDD.filter &#123; log =&gt; &#123; if (log._2._2.getOrElse(flase)) &#123; false &#125; else &#123; true &#125; &#125; &#125; filteredRDD.map(line =&gt; line._2._1) &#125; ) filteredLog.print() ssc.start() ssc.awaitTermination() &#125; case class UserLog(username:String, actionLog:String)&#125;","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"transform","slug":"transform","permalink":"http://yoursite.com/tags/transform/"}]},{"title":"mapWithState","slug":"mapWithState","date":"2020-05-27T10:14:14.000Z","updated":"2020-05-27T10:14:14.383Z","comments":true,"path":"2020/05/27/mapWithState/","link":"","permalink":"http://yoursite.com/2020/05/27/mapWithState/","excerpt":"","text":"mapWithState åªæ˜¾ç¤ºæ›´æ–°éƒ¨åˆ†çš„ç´¯è®¡ 12345678910111213141516171819202122232425262728293031323334353637package sk_demoimport org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, State, StateSpec, StreamingContext&#125;object MapWithStateApp &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(\"MapWithStateApp\") val ssc = new StreamingContext(sparkConf, Seconds(2)) ssc.sparkContext.setLogLevel(\"WARN\") ssc.checkpoint(\"/tmp_data/tmp_checkpoint\") val socketLines = ssc.socketTextStream(\"localhost\", 9999) val batchStat = socketLines.flatMap(_.split(\" \")).map((_, 1)).reduceByKey(_+_) val mappingFunc = (word:String, one:Option[Int], state:State[Int]) =&gt; &#123; val sum = one.getOrElse(0) + state.getOption.getOrElse(0) val output = (word, sum) state.update(sum) output &#125; val result = batchStat.mapWithState(StateSpec.function(mappingFunc)) result.print() ssc.start() ssc.awaitTermination() &#125;&#125;","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"mapWithState","slug":"mapWithState","permalink":"http://yoursite.com/tags/mapWithState/"}]},{"title":"updateStateByKey","slug":"updateStateByKey","date":"2020-05-27T10:11:57.000Z","updated":"2020-05-27T10:11:58.002Z","comments":true,"path":"2020/05/27/updateStateByKey/","link":"","permalink":"http://yoursite.com/2020/05/27/updateStateByKey/","excerpt":"","text":"updateStateByKey å¯¹äºçŠ¶æ€çš„ç´¯è®¡æ›´æ–° 123456789101112131415161718192021222324252627282930313233343536373839package sk_demoimport org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object UpdateStateByKeyApp &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setAppName(\"UpdateStateByKeyApp\").setMaster(\"local[2]\") val ssc = new StreamingContext(sparkConf, Seconds(2)) ssc.checkpoint(\"/tmp_data/tmp_checkpoint\") val socketLines = ssc.socketTextStream(\"localhost\", 9999) val result = socketLines.flatMap(_.split(\" \")).map((_, 1)).updateStateByKey(updateStateFunc) result.print() ssc.start() ssc.awaitTermination() &#125; /** * @Param currValues: å½“å‰keyå¯¹åº”çš„æ‰€æœ‰å€¼, preValue: å½“å‰keyçš„å†å²çŠ¶æ€ * @Return Option[Int] */ def updateStateFunc(currValues:Seq[Int], preValue:Option[Int]): Option[Int] = &#123; var currValuesSum = 0 for (currVal &lt;- currValues) &#123; currValuesSum += currVal &#125; val allSum = preValue.getOrElse(0) + currValuesSum Option(allSum) &#125;&#125;","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"updateStateByKey","slug":"updateStateByKey","permalink":"http://yoursite.com/tags/updateStateByKey/"}]},{"title":"SparkCore ç”Ÿäº§å®ä¾‹","slug":"SparkCore-ç”Ÿäº§å®ä¾‹","date":"2020-05-22T09:15:39.000Z","updated":"2020-05-22T09:15:39.839Z","comments":true,"path":"2020/05/22/SparkCore-ç”Ÿäº§å®ä¾‹/","link":"","permalink":"http://yoursite.com/2020/05/22/SparkCore-%E7%94%9F%E4%BA%A7%E5%AE%9E%E4%BE%8B/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258package workuseimport org.apache.spark.rdd.RDDimport org.apache.spark.sql.SparkSessionimport scala.collection.mutable.ListBufferobject core_work &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession.builder().appName(\"core_work\").getOrCreate() val sc = spark.sparkContext sc.setLogLevel(\"ERROR\") val logRDD = sc.textFile(\"/Users/zhouke/tmp_data/user_visit_action.txt\") // top10 å•†å“, æ ¹æ®ç‚¹å‡»ã€è®¢å•ã€æ”¯ä»˜ ç»Ÿè®¡ ç‚¹å‡»æ•°*0.3 + ä¸‹å•æ•° * 0.4 + æ”¯ä»˜æ•° * 0.4 val top10Sku = getTop10Sku(logRDD)// sc.parallelize(top10Sku).saveAsTextFile(\"hdfs://localhost:9000/demo/top10Res\")// val top10DF = spark.createDataFrame(top10Sku)//// top10DF.selectExpr(\"round(score, 2) as sc\", \"*\").drop(\"score\")// .write// .format(\"jdbc\")// .option(\"url\", \"jdbc:mysql://localhost:3306\")// .option(\"dbtable\", \"test.top10sku\")// .option(\"driver\", \"com.mysql.jdbc.Driver\")// .option(\"user\", \"root\")// .option(\"password\", \"abc \")// .save() // top10å•†å“çš„ æ¯ä¸ªå“ç±»ç‚¹å‡»top10çš„session val top10session = getTop10Session(spark, logRDD) top10Sku.foreach(println) top10session.foreach(println) &#125; /** * @Desc çƒ­åº¦å‰10çš„å•†å“ * @Date 10:38 ä¸Šåˆ 2020/5/21 * @Param [logRDD] * @Return workuse.core_work.Top10Rs[] **/ def getTop10Sku(logRDD: RDD[String]): Array[Top10Rs] = &#123; val actionRDD = logRDD.map&#123; line =&gt; &#123; val log = line.split(\"_\") UserAction( log(0), log(1).toLong, log(2), log(3).toLong, log(4), log(5), log(6).toLong, log(7).toLong, log(8), log(9), log(10), log(11), log(12).toLong ) &#125; &#125; val infoRDD = actionRDD.flatMap &#123; action =&gt; &#123; if (action.click_category_id != -1) &#123; List(CategoryCountInfo(action.click_category_id+\"\", 1, 0, 0)) &#125; else if (action.order_category_ids != \"null\") &#123; val ids = action.order_category_ids.split(\",\") val actionList = ListBuffer[CategoryCountInfo]() for (id &lt;- ids) &#123; actionList += CategoryCountInfo(id, 0, 1, 0) &#125; actionList &#125; else if (action.pay_category_ids != \"null\") &#123; val ids = action.pay_category_ids.split(\",\") val actionList = ListBuffer[CategoryCountInfo]() for (id &lt;- ids) &#123; actionList += CategoryCountInfo(id, 0, 0, 1) &#125; actionList &#125;else &#123; Nil &#125; &#125; &#125; val categoryRDD = infoRDD.groupBy(_.categoryId) val categoryCombineRDD = categoryRDD.mapValues &#123; logs =&gt; &#123; logs.reduce &#123; (x, y) =&gt; &#123; CategoryCountInfo(x.categoryId, x.clickCount+y.clickCount, x.orderCount+y.orderCount, x.payCount+y.payCount) &#125; &#125; &#125; &#125; val top10Res = categoryCombineRDD.map&#123; sumLog =&gt; &#123; val lg = sumLog._2 Top10Rs(lg.categoryId, lg.clickCount, lg.orderCount, lg.payCount, lg.clickCount*0.3+lg.orderCount*0.4+lg.payCount*0.4) &#125; &#125;.sortBy(_.score, false).take(10) top10Res &#125; def getTop10Session(spark: SparkSession, logRDD: RDD[String]): RDD[(String, List[(String, Int)])] = &#123; val actionRDD = logRDD.map&#123; line =&gt; &#123; val log = line.split(\"_\") UserAction( log(0), log(1).toLong, log(2), log(3).toLong, log(4), log(5), log(6).toLong, log(7).toLong, log(8), log(9), log(10), log(11), log(12).toLong ) &#125; &#125; val countRDD = actionRDD.flatMap &#123; action =&gt; &#123; if (action.click_category_id != -1) &#123; List(CategoryCountInfo(action.click_category_id+\"\", 1, 0, 0)) &#125; else if (action.order_category_ids != \"null\") &#123; val ids = action.order_category_ids.split(\",\") val countList = ListBuffer[CategoryCountInfo]() for (id &lt;- ids) &#123; countList += CategoryCountInfo(id, 0, 1, 0) &#125; countList &#125; else if (action.pay_category_ids != \"null\") &#123; val ids = action.pay_category_ids.split(\",\") val countList = ListBuffer[CategoryCountInfo]() for (id &lt;- ids) &#123; countList += CategoryCountInfo(id, 0, 0, 1) &#125; countList &#125; else &#123; Nil &#125; &#125; &#125; val groupByCategoryRDD = countRDD.groupBy(_.categoryId) val categorySum = groupByCategoryRDD.mapValues&#123; logs =&gt; &#123; logs.reduce &#123; (x, y) =&gt; CategoryCountInfo(x.categoryId, x.clickCount+y.clickCount, x.orderCount+y.orderCount, x.payCount+y.payCount) &#125; &#125; &#125; val top10 = categorySum.map(_._2).sortBy&#123; line =&gt; &#123; line.clickCount * 0.3 + line.orderCount * 0.4 + line.payCount * 0.4 &#125; &#125;.take(10) val top10Ids = top10.map(_.categoryId) val broadcastIds = spark.sparkContext.broadcast(top10Ids) val top10Logs = actionRDD.filter &#123; line =&gt; &#123; if (line.click_category_id != -1) &#123; broadcastIds.value.contains(line.click_category_id.toString) &#125; else &#123; false &#125; &#125; &#125; val categoryAndSession = top10Logs.map&#123; action =&gt; &#123; (action.click_category_id+\"-\"+action.session_id, 1) &#125; &#125; val sumRDD = categoryAndSession.reduceByKey(_+_) val catRDD = sumRDD.map&#123; line =&gt; &#123; (line._1.split(\"-\")(0), (line._1.split(\"-\")(1), line._2)) &#125; &#125; val groupRDD = catRDD.groupByKey() val resRDD = groupRDD.mapValues &#123; datas =&gt; &#123; datas.toList.sortWith &#123; case(x, y) =&gt; &#123; x._2 &gt; y._2 &#125; &#125;.take(10) &#125; &#125; resRDD &#125; case class UserAction( date: String, user_id: Long, session_id: String, page_id: Long, action_time: String, search_keyword: String, click_category_id: Long, click_product_id: Long, order_category_ids: String, order_product_ids: String, pay_category_ids: String, pay_product_ids: String, city_id: Long ) case class CategoryCountInfo( categoryId: String, clickCount: Long, orderCount: Long, payCount: Long ) case class Top10Rs( categoryId:String, clickCount: Long, orderCount: Long, payCount: Long, score: Double )&#125;","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"SparkCore","slug":"SparkCore","permalink":"http://yoursite.com/tags/SparkCore/"}]},{"title":"flumeè·¨æœºå™¨ä¼ è¾“","slug":"flumeè·¨æœºå™¨ä¼ è¾“","date":"2020-05-22T02:52:52.000Z","updated":"2020-05-22T02:52:52.678Z","comments":true,"path":"2020/05/22/flumeè·¨æœºå™¨ä¼ è¾“/","link":"","permalink":"http://yoursite.com/2020/05/22/flume%E8%B7%A8%E6%9C%BA%E5%99%A8%E4%BC%A0%E8%BE%93/","excerpt":"","text":"Flumeè·¨æœºå™¨ä¼ è¾“ ecs00:å…¬ç½‘ip: 102.102.102.102å†…ç½‘ip: 172.100.100.100 ecs01:å…¬ç½‘ip: 103.103.103.103å†…ç½‘ip: 172.100.100.101 ç›®æ ‡ecs00çš„44444ç«¯å£çš„netcat ==&gt; ecs01çš„logger ecs00-netcat-memory-avro.conf12345678910111213141516netcat-avro-agent.sources &#x3D; netcat-avro-sourcenetcat-avro-agent.sinks &#x3D; netcat-avro-sinknetcat-avro-agent.channels &#x3D; netcat-avro-channelnetcat-avro-agent.sources.netcat-avro-source.type &#x3D; netcatnetcat-avro-agent.sources.netcat-avro-source.bind &#x3D; localhostnetcat-avro-agent.sources.netcat-avro-source.port &#x3D; 44444netcat-avro-agent.sinks.netcat-avro-sink.type &#x3D; avronetcat-avro-agent.sinks.netcat-avro-sink.hostname &#x3D; 103.103.103.103netcat-avro-agent.sinks.netcat-avro-sink.port &#x3D; 44444netcat-avro-agent.channels.netcat-avro-channel.type &#x3D; memorynetcat-avro-agent.sources.netcat-avro-source.channels &#x3D; netcat-avro-channelnetcat-avro-agent.sinks.netcat-avro-sink.channel &#x3D; netcat-avro-channel ecs01-avro-memory-logger.conf1234567891011121314avro-logger-agent.sources &#x3D; avro-logger-sourceavro-logger-agent.sinks &#x3D; avro-logger-sinkavro-logger-agent.channels &#x3D; avro-logger-channelavro-logger-agent.sources.avro-logger-source.type &#x3D; avroavro-logger-agent.sources.avro-logger-source.bind &#x3D; 172.100.100.101avro-logger-agent.sources.avro-logger-source.port &#x3D; 44444avro-logger-agent.sinks.avro-logger-sink.type &#x3D; loggeravro-logger-agent.channels.avro-logger-channel.type &#x3D; memoryavro-logger-agent.sources.avro-logger-source.channels &#x3D; avro-logger-channelavro-logger-agent.sinks.avro-logger-sink.channel &#x3D; avro-logger-channel å¯åŠ¨: ecs01 ä¸Š 1flume-ng agent --conf $&#123;FLUME_HOME&#125;/conf --name avro-logger-agent --conf-file ./avro-memory-logger.conf -Dflume.root.logger=INFO,console ecs00ä¸Š 1flume-ng agent --conf $&#123;FLUME_HOME&#125;/conf --name netcat-avro-agent --conf-file ./netcat-memory-avro.conf -Dflume.root.logger=INFO,console ecs00ä¸Š 1telnet localhost 44444","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"http://yoursite.com/tags/Flume/"}]},{"title":"socket/file/queue_streaming","slug":"socket-file-queue-streaming","date":"2020-05-20T08:13:34.000Z","updated":"2020-05-20T08:13:35.007Z","comments":true,"path":"2020/05/20/socket-file-queue-streaming/","link":"","permalink":"http://yoursite.com/2020/05/20/socket-file-queue-streaming/","excerpt":"","text":"socket_streaming1234567891011121314151617181920212223242526package workuseimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import org.apache.spark.SparkConfobject socket_streaming &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setAppName(\"socket_streaming\") val ssc = new StreamingContext(sparkConf, Seconds(4)) val lineStream = ssc.socketTextStream(\"localhost\", 9999) val wordStreams = lineStream.flatMap(_.split(\" \")) val wordAndOneStreams = wordStreams.map((_, 1)) val wordAndCountStreams = wordAndOneStreams.reduceByKey(_+_) wordAndCountStreams.print() ssc.start() ssc.awaitTermination() &#125;&#125; file_streaming1234567891011121314151617181920212223242526package workuseimport org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object file_streaming &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setAppName(\"fileStream\") val ssc = new StreamingContext(sparkConf, Seconds(4)) val dirStreams = ssc.textFileStream(\"hdfs://localhost:9000/fileStream\") val wordStreams = dirStreams.flatMap(_.split(\",\")) val wordAndOneStreams = wordStreams.map((_, 1)) val wordAndCountStreams = wordAndOneStreams.reduceByKey(_+_) wordAndCountStreams.print() ssc.start() ssc.awaitTermination() &#125;&#125; queue_streaming123456789101112131415161718192021222324252627282930313233package workuseimport org.apache.spark.SparkConfimport org.apache.spark.rdd.RDDimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import scala.collection.mutableobject rdd_queue_streaming &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setAppName(\"RDD_queue\") val ssc = new StreamingContext(sparkConf, Seconds(4)) val rddQueue = new mutable.Queue[RDD[Int]]() val inputStream = ssc.queueStream(rddQueue, oneAtATime= false) val mappedStream = inputStream.map((_, 1)) val reduceStream = mappedStream.reduceByKey(_+_) reduceStream.print() ssc.start() for (i &lt;- 1 to 5) &#123; rddQueue += ssc.sparkContext.makeRDD(1 to 300, 10) Thread.sleep(2000) &#125; &#125;&#125;","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"sparkStreaming","slug":"sparkStreaming","permalink":"http://yoursite.com/tags/sparkStreaming/"}]},{"title":"spark_coreå›¾è°±","slug":"spark-coreå›¾è°±","date":"2020-05-18T06:34:48.000Z","updated":"2020-05-18T06:34:48.266Z","comments":true,"path":"2020/05/18/spark-coreå›¾è°±/","link":"","permalink":"http://yoursite.com/2020/05/18/spark-core%E5%9B%BE%E8%B0%B1/","excerpt":"","text":"","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark_transformation","slug":"spark-transformation","date":"2020-05-15T07:21:22.000Z","updated":"2020-05-15T07:21:22.568Z","comments":true,"path":"2020/05/15/spark-transformation/","link":"","permalink":"http://yoursite.com/2020/05/15/spark-transformation/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590package sk_demoimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object transformations &#123; val _sc_conf = new SparkConf() .setAppName(\"spark_transformation\") .setMaster(\"local\") val sc = new SparkContext(_sc_conf) def main(args: Array[String]): Unit = &#123; /** * Return a new distributed dataset formed by passing each element of the source through a function func. **/ map() /** * Similar to map, but runs separately on each partition (block) of the RDD, * so func must be of type Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt; when running on an RDD of type T. * */ mapPartitions() /** * Similar to mapPartitions, * but also provides func with an integer value representing the index of the partition, * so func must be of type (Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt; when running on an RDD of type T. * */ mapPartitionsWithIndex() /** * Similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item). * */ flatMap() /** * Return an RDD created by coalescing all elements within each partition into an array * */ glom() /** * Return an RDD of grouped items. Each group consists of a key and a sequence of elements * */ groupBy() /** * Return a new dataset formed by selecting those elements of the source on which func returns true. * */ filter() /** * Sample a fraction fraction of the data, with or without replacement, using a given random number generator seed. * */ sample() /** * Return a new dataset that contains the distinct elements of the source dataset. * */ distinct() /** * Decrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently after filtering down a large dataset. * */ coalesce() /** * Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them. This always shuffles all data over the network. * */ repartition() /** * Return this RDD sorted by the given key function. * */ sortBy() /** * Return an RDD created by piping elements to a forked external process * */ pipe() /** * Return the union of this RDD and another one. Any identical elements will appear multiple * */ union() /** * Return an RDD with the elements from `this` that are not in `other`. * */ subtract() /** * Return the intersection of this RDD and another one. The output will not contain any duplicate * */ intersection() /** * Return the Cartesian product of this RDD and another one * */ cartesian() /** * Zips this RDD with another one, returning key-value pairs with the first element in each RDD * */ zip() /** * Return a copy of the RDD partitioned using the specified partitioner. * */ partitionBy() /** * Merge the values for each key using an associative and commutative reduce function. * */ reduceByKey() /** * Group the values for each key in the RDD into a single sequence. * */ groupByKey() /** * Aggregate the values of each key, using given combine functions and a neutral \"zero value\". * */ aggregateByKey() /** * Merge the values for each key using an associative function and a neutral \"zero value\" * */ foldByKey() /** * Sort the RDD by key, so that each partition contains a sorted range of the elements. * */ sortByKey() /** * Pass each value in the key-value pair RDD through a map function without changing the keys * */ mapValues() /** * Return an RDD containing all pairs of elements with matching keys in `this` and `other`. * */ join() /** * For each key k in `this` or `other`, return a resulting RDD that contains a tuple with the * list of values for that key in `this` as well as `other`. * */ cogroup() &#125; /** * @Desc RDDä¸­çš„æ¯ä¸ªå…ƒç´ è¿›è¡Œ func æ“ä½œ, è¿”å›ä¸€ä¸ªæ–°çš„RDD * @Date 10:57 ä¸Šåˆ 2020/5/15 * @Param [] * @Return void **/ def map(): Unit = &#123; val rdd = sc.parallelize(1 to 10) val res = rdd.map(_+2) res.foreach(println) &#125; /** * @Desc å°†ä¸€ä¸ªåˆ†åŒºä¸Šçš„æ‰€æœ‰å…ƒç´ ï¼Œä½¿ç”¨ä¸€æ¬¡å‡½æ•°ï¼Œè€Œä¸æ˜¯mapçš„æ¯ä¸ªå…ƒç´ éƒ½ä½¿ç”¨ä¸€æ¬¡å…ƒç´  * @append æ•ˆç‡è¦æ¯”mapå¥½ï¼Œä½†æ˜¯è¦æ³¨æ„æ•°æ®é‡å¤§è€Œå¯¼è‡´çš„oom * @Date 11:25 ä¸Šåˆ 2020/5/15 * @Param [] * @Return void **/ def mapPartitions(): Unit = &#123; val rdd = sc.parallelize(1 to 10) val rdd2 = rdd.mapPartitions(mapPartitionsFunc) rdd2.foreach(println) &#125; /*** * @Desc * @Date 11:31 ä¸Šåˆ 2020/5/15 * @Param [iter] * @Return scala.collection.Iterator&lt;java.lang.Object&gt; **/ def mapPartitionsFunc(iter:Iterator[Int]) :Iterator[Int]= &#123; var res = List[Int]() while (iter.hasNext) &#123; val iterVal = iter.next() res :+= (iterVal + 2) &#125; res.iterator &#125; /*** * @Desc ç±»ä¼¼mapPartitions, å¤šäº†åˆ†åŒºçš„index * @Date 11:39 ä¸Šåˆ 2020/5/15 * @Param [] * @Return void **/ def mapPartitionsWithIndex(): Unit = &#123; val rdd = sc.parallelize(1 to 10, 2) val res = rdd.mapPartitionsWithIndex(mapPartitionsWithIndexFunc) res.foreach(println) &#125; /*** * @Desc * @Date 11:39 ä¸Šåˆ 2020/5/15 * @Param [idx, iter] * @Return scala.collection.Iterator&lt;scala.Tuple2&lt;java.lang.Object,java.lang.Object&gt;&gt; **/ def mapPartitionsWithIndexFunc(idx:Int, iter:Iterator[Int]): Iterator[(Int, Int)] = &#123; var res = List[(Int, Int)]() while (iter.hasNext) &#123; val iterVal = iter.next() res :+= (idx, iterVal+2) &#125; res.iterator &#125; /*** * @Desc æ‰å¹³åŒ–çš„æ“ä½œï¼Œæ‰€ä»¥è¾“å‡ºçš„æ˜¯ä¸€ä¸ªåºåˆ—ï¼Œä¸æ˜¯å•ä¸ªçš„å€¼ * @Date 11:46 ä¸Šåˆ 2020/5/15 * @Param [] * @Return void **/ def flatMap(): Unit = &#123; val rdd = sc.parallelize(Array((\"A\", 1), (\"B\", 2), (\"C\", 2))) val mapRes = rdd.map(x =&gt; x._1 + \"-&gt;\" + x._2) val flatMapRes = rdd.flatMap(x =&gt; x._1 + \"-&gt;\" + x._2) mapRes.foreach(println) flatMapRes.foreach(println) &#125; /** * @Desc æŒ‰åˆ†åŒºè½¬åŒ–ä¸ºæ•°ç»„ç±»å‹çš„RDD * @Date 1:36 ä¸‹åˆ 2020/5/15 * @Param [] * @Return void **/ def glom(): Unit = &#123; val rdd = sc.parallelize(Array(1, 2, 3, 4, 1, 2, 3, 0), 2) val res = rdd.glom().map(_.max) res.foreach(println) &#125; /** * @Desc æ ¹æ®æ¯ä¸ªå…ƒç´ çš„è®¡ç®—è¿›è¡Œåˆ†ç»„ * @Date 1:45 ä¸‹åˆ 2020/5/15 * @Param [] * @Return void **/ def groupBy(): Unit = &#123; val rdd = sc.parallelize(1 to 10) val res = rdd.groupBy(_%2) res.foreach(println) &#125; /** * @Desc è¿”å›æ¡ä»¶ä¸ºtrueçš„ * @Date 1:49 ä¸‹åˆ 2020/5/15 * @Param [] * @Return void **/ def filter(): Unit = &#123; val rdd = sc.parallelize(1 to 10) val res = rdd.filter(_%2 == 0) res.foreach(println) &#125; /** * @Desc å–æ · (æ˜¯å¦é‡å¤å–æ ·) * @param fraction expected size of the sample as a fraction of this RDD's size * without replacement: probability that each element is chosen; fraction must be [0, 1] * with replacement: expected number of times each element is chosen; fraction must be greater * than or equal to 0 * @Date 1:51 ä¸‹åˆ 2020/5/15 * @Param [] * @Return void **/ def sample(): Unit = &#123; val rdd = sc.parallelize(1 to 10) val res = rdd.sample(false, 0.5) res.foreach(println) &#125; /** * @Desc å»é‡åçš„RDD å‚æ•° numPartitions * @Date 2:02 ä¸‹åˆ 2020/5/15 * @Param [] * @Return void **/ def distinct(): Unit = &#123; val rdd = sc.parallelize(Array(1, 2, 3, 1, 2, 4, 5)) val res = rdd.distinct(2) res.foreach(println) &#125; /** * @Desc é™ä½åˆ†åŒºæ•° * @Date 2:06 ä¸‹åˆ 2020/5/15 * @Param [] * @Return void **/ def coalesce(): Unit = &#123; val rdd = sc.parallelize(1 to 16, 4) val res = rdd.coalesce(3) println(res.partitions.size) &#125; /** * @Desc é‡æ–°åˆ†åŒº,éšæœºæ’åˆ—ï¼Œåˆ†åŒºä¹‹é—´ä¿æŒå¹³è¡¡ * @Date 2:09 ä¸‹åˆ 2020/5/15 * @Param [] * @Return void **/ def repartition(): Unit = &#123; val rdd = sc.parallelize(1 to 16, 4) val res = rdd.repartition(2) println(res.partitions.size) &#125; /** * @Desc æ ¹æ®ç»™å®škeyæ’åº * @Date 2:12 ä¸‹åˆ 2020/5/15 * @Param [] * @Return void **/ def sortBy(): Unit = &#123; val rdd = sc.parallelize(Array((\"A\", 1), (\"C\", 3), (\"B\", 4), (\"D\", 2))) val res = rdd.sortBy(_._1) res.foreach(println) &#125; /** * @Desc åœ¨ç®¡é“ä¸­æ‰§è¡Œshell/perlè„šæœ¬ * @Date 2:15 ä¸‹åˆ 2020/5/15 * @Param [] * @Return void **/ def pipe(): Unit = &#123; val rdd = sc.parallelize(Array(\"A\", \"B\", \"C\")) val res = rdd.pipe(\"shell_path.sh\") res.foreach(println) &#125; /** * @Desc è¿æ¥RDD * @Date 2:18 ä¸‹åˆ 2020/5/15 * @Param [] * @Return void **/ def union(): Unit = &#123; val rdd1 = sc.parallelize(1 to 6) val rdd2 = sc.parallelize(5 to 10) val res = rdd1.union(rdd2) res.foreach(println) &#125; /** * @Desc å°†rdd1ä¸­ä¹Ÿåœ¨rdd2ä¸­çš„å…ƒç´ å»æ‰ * @Date 2:22 ä¸‹åˆ 2020/5/15 * @Param [] * @Return void **/ def subtract(): Unit = &#123; val rdd1 = sc.parallelize(1 to 6) val rdd2 = sc.parallelize(5 to 10) val res = rdd1.subtract(rdd2) res.foreach(println) &#125; /** * @Desc äº¤é›† * @Date 2:25 ä¸‹åˆ 2020/5/15 * @Param [] * @Return void **/ def intersection(): Unit = &#123; val rdd1 = sc.parallelize(1 to 6) val rdd2 = sc.parallelize(5 to 10) val res = rdd1.intersection(rdd2) res.foreach(println) &#125; /** * @Desc ç¬›å¡å°”ç§¯ * @Date 2:27 ä¸‹åˆ 2020/5/15 * @Param [] * @Return void **/ def cartesian(): Unit = &#123; val rdd1 = sc.parallelize(1 to 6) val rdd2 = sc.parallelize(5 to 10) val res = rdd1.cartesian(rdd2) res.foreach(println) &#125; /** * @Desc k vç»„åˆ * @Date 2:32 ä¸‹åˆ 2020/5/15 * @Param [] * @Return void **/ def zip(): Unit = &#123; val rdd1 = sc.parallelize(Array(\"A\", \"B\", \"C\")) val rdd2 = sc.parallelize(Array(1, 2, 3)) val res = rdd1.zip(rdd2) res.foreach(println) &#125; /** * @Desc æ ¹æ®partitioner è¿›è¡Œåˆ†åŒº * @Date 2:38 ä¸‹åˆ 2020/5/15 * @Param [] * @Return void **/ def partitionBy(): Unit = &#123; val rdd = sc.parallelize(Array((\"A\", 1), (\"B\", 1), (\"C\", 2), (\"D\",3))) val res = rdd.partitionBy(new org.apache.spark.HashPartitioner(2)) res.foreach(println) &#125; /** * @Desc * @Date 2:41 ä¸‹åˆ 2020/5/15 * @Param [] * @Return void **/ def reduceByKey(): Unit = &#123; val rdd = sc.parallelize(Array((\"A\", 1), (\"B\", 1), (\"A\", 2), (\"D\", 3))) val res = rdd.reduceByKey((x, y) =&gt; x + y) res.foreach(println) &#125; /** * @Desc * @Date 2:45 ä¸‹åˆ 2020/5/15 * @Param [] * @Return void **/ def groupByKey(): Unit = &#123; val rdd = sc.parallelize(Array((\"A\", 1), (\"B\", 1), (\"A\", 2), (\"D\", 3))) val res = rdd.groupByKey() res.foreach(println) &#125; /** * @Desc * @Date 2:56 ä¸‹åˆ 2020/5/15 * @Param [] * @Return void **/ def aggregateByKey(): Unit = &#123; val rdd = sc.parallelize(Array(1, 2, 3, 4, 5, 6, 7, 8), 2) val res1 = rdd.aggregate(0)(math.max(_, _), _+_) val res2 = rdd.aggregate(5)(math.max(_, _), _+_) println(res1, res2) &#125; /** * @Desc * @Date 3:01 ä¸‹åˆ 2020/5/15 * @Param [] * @Return void **/ def foldByKey(): Unit = &#123; val rdd = sc.parallelize(List((1, 2), (2, 3), (1, 3), (2, 5)), 2) val res = rdd.foldByKey(0)(_+_) res.foreach(println) &#125; def combineByKey(): Unit = &#123; &#125; /** * @Desc * @Date 3:09 ä¸‹åˆ 2020/5/15 * @Param [] * @Return void **/ def sortByKey(): Unit = &#123; val rdd = sc.parallelize(Array((\"A\", 1), (\"C\", 2), (\"B\", 3))) val res = rdd.sortByKey(true) res.foreach(println) &#125; /** * @Desc v map * @Date 3:11 ä¸‹åˆ 2020/5/15 * @Param [] * @Return void **/ def mapValues():Unit = &#123; val rdd = sc.parallelize(Array((\"A\", 1), (\"C\", 2), (\"B\", 3))) val res = rdd.mapValues(_+2) res.foreach(println) &#125; /** * @Desc join * @Date 3:13 ä¸‹åˆ 2020/5/15 * @Param [] * @Return void **/ def join(): Unit = &#123; val rdd1 = sc.parallelize(Array((1,\"a\"),(2,\"b\"),(3,\"c\"))) val rdd2 = sc.parallelize(Array((1,4),(2,5),(3,6))) val res = rdd1.join(rdd2) res.foreach(println) &#125; /** * @Desc * @Date 3:16 ä¸‹åˆ 2020/5/15 * @Param [] * @Return void **/ def cogroup(): Unit = &#123; val rdd = sc.parallelize(Array((1,\"a\"),(2,\"b\"),(3,\"c\"))) val rdd1 = sc.parallelize(Array((1,4),(2,5),(3,6))) val res = rdd.cogroup(rdd1) res.foreach(println) &#125;&#125;","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"},{"name":"transformation","slug":"transformation","permalink":"http://yoursite.com/tags/transformation/"}]},{"title":"dataxçš„åŸºæœ¬ä½¿ç”¨","slug":"dataxçš„åŸºæœ¬ä½¿ç”¨","date":"2020-05-12T07:39:02.000Z","updated":"2020-05-12T07:39:02.922Z","comments":true,"path":"2020/05/12/dataxçš„åŸºæœ¬ä½¿ç”¨/","link":"","permalink":"http://yoursite.com/2020/05/12/datax%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","excerpt":"","text":"dataxçš„åŸºæœ¬ä½¿ç”¨ dataxæ˜¯ä¸€ç§ä¸­å¿ƒåŒ–å¤„ç†å½¢å¼ï¼Œåˆ†ä¸ºreaderå’Œwriterè¿æ¥åˆ°dataxï¼Œç„¶åè¿›è¡Œè½¬å‚¨æ“ä½œ mysql2mysql123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&#123; \"job\": &#123; \"content\": [ &#123; \"reader\": &#123; \"name\":\"mysqlreader\", \"parameter\": &#123; \"column\":[ \"id\", \"name\" ], \"connection\":[ &#123; \"jdbcUrl\":[ \"jdbc:mysql://127.0.0.1:3306/test\" ], \"table\":[ \"d1\" ] &#125; ], \"password\":\"root\", \"username\":\"root\" &#125; &#125;, \"writer\": &#123; \"name\":\"mysqlwriter\", \"parameter\":&#123; \"column\":[ \"id\", \"name\" ], \"connection\":[ &#123; \"jdbcUrl\":\"jdbc:mysql://127.0.0.1:3306/test\", \"table\":[\"d2\"] &#125; ], \"password\":\"root\", \"username\":\"root\" &#125; &#125; &#125; ], \"setting\":&#123; \"speed\":&#123; \"channel\":\"1\" &#125; &#125; &#125;&#125; file2hive1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&#123; \"job\":&#123; \"content\": [ &#123; \"reader\": &#123; \"name\": \"txtfilereader\", \"parameter\": &#123; \"path\": [\"/home/kowhoy/tmp/10_days_orders_2.csv\"], \"encoding\": \"UTF-8\", \"column\": [ &#123; \"index\":0, \"type\": \"long\", &#125;, &#123; \"index\": 1, \"type\": \"long\" &#125;, &#123; \"index\": 2, \"type\": \"string\" &#125;, &#123; \"index\": 3, \"type\": \"long\" &#125;, &#123;\"index\": 4, \"type\": \"long\"&#125;, &#123;\"index\": 5, \"type\": \"long\"&#125;, &#123;\"index\": 6, \"type\": \"long\"&#125;, &#123;\"index\": 7, \"type\": \"long\"&#125;, &#123;\"index\": 8, \"type\": \"long\"&#125;, &#123;\"index\": 9, \"type\": \"long\"&#125;, &#123;\"index\": 10, \"type\": \"long\"&#125;, &#123;\"index\": 11, \"type\": \"long\"&#125;, &#123;\"index\": 12, \"type\": \"long\"&#125;, &#123;\"index\": 13, \"type\": \"long\"&#125;, &#123;\"index\": 14, \"type\": \"long\"&#125;, &#123;\"index\": 15, \"type\": \"long\"&#125;, &#123;\"index\": 16, \"type\": \"long\"&#125;, &#123;\"index\": 17, \"type\": \"long\"&#125;, &#123;\"index\": 18, \"type\": \"long\"&#125;, &#123;\"index\": 19, \"type\": \"string\"&#125;, &#123;\"index\": 20, \"type\": \"double\"&#125;, &#123;\"index\": 21, \"type\": \"double\"&#125;, &#123;\"index\": 22, \"type\": \"double\"&#125;, &#123;\"index\": 23, \"type\": \"double\"&#125;, &#123;\"index\": 24, \"type\": \"long\"&#125;, &#123;\"index\": 25, \"type\": \"long\"&#125;, &#123;\"index\": 26, \"type\": \"long\"&#125;, &#123;\"index\": 27, \"type\": \"double\"&#125;, &#123;\"index\": 28, \"type\": \"double\"&#125;, &#123;\"index\": 29, \"type\": \"long\"&#125;, &#123;\"index\": 30, \"type\": \"long\"&#125;, &#123;\"index\": 31, \"type\": \"long\"&#125;, &#123;\"index\": 32, \"type\": \"long\"&#125;, &#123;\"index\": 33, \"type\": \"long\"&#125;, &#123;\"index\": 34, \"type\": \"long\"&#125;, &#123;\"index\": 35, \"type\": \"long\"&#125;, &#123;\"index\": 36, \"type\": \"long\"&#125;, &#123;\"index\": 37, \"type\": \"double\"&#125;, &#123;\"index\": 38, \"type\": \"long\"&#125;, &#123;\"index\": 39, \"type\": \"long\"&#125;, &#123;\"index\": 40, \"type\": \"long\"&#125;, &#123;\"index\": 41, \"type\": \"string\"&#125;, &#123;\"index\": 42, \"type\": \"string\"&#125;, &#123;\"index\": 43, \"type\": \"long\"&#125;, &#123;\"index\": 44, \"type\": \"long\"&#125;, &#123;\"index\": 45, \"type\": \"string\"&#125; ], \"fieldDelimiter\": \",\" &#125; &#125;, \"writer\": &#123; \"name\": \"hdfswriter\", \"parameter\": &#123; \"defaultFS\": \"hdfs://ecs:9000\", \"fileType\": \"text\", \"path\": \"/user/hive/warehouse/orders.db/datax_file_tab\", \"fileName\": \"datax_file_tab\", \"column\": [ &#123;\"name\": \"order_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"channel_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"order_sn\", \"type\": \"STRING\"&#125;, &#123;\"name\": \"is_serv_order\", \"type\": \"INT\"&#125;, &#123;\"name\": \"is_personal\", \"type\": \"INT\"&#125;, &#123;\"name\": \"order_type\", \"type\": \"INT\"&#125;, &#123;\"name\": \"mode_type\", \"type\": \"INT\"&#125;, &#123;\"name\": \"order_src\", \"type\": \"INT\"&#125;, &#123;\"name\": \"ao_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"member_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"employee_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"branch_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"company_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"city_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"osp_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"oss_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"oss_ao_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"osg_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"staff_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"sku_code\", \"type\": \"STRING\"&#125;, &#123;\"name\": \"sku_base_price\", \"type\": \"FLOAT\"&#125;, &#123;\"name\": \"sku_litre_price\", \"type\": \"FLOAT\"&#125;, &#123;\"name\": \"sku_price\", \"type\": \"FLOAT\"&#125;, &#123;\"name\": \"sku_litre\", \"type\": \"FLOAT\"&#125;, &#123;\"name\": \"oil_amount\", \"type\": \"INT\"&#125;, &#123;\"name\": \"amount\", \"type\": \"INT\"&#125;, &#123;\"name\": \"status\", \"type\": \"INT\"&#125;, &#123;\"name\": \"longitude\", \"type\": \"FLOAT\"&#125;, &#123;\"name\": \"latitude\", \"type\": \"FLOAT\"&#125;, &#123;\"name\": \"up_order_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"payment_type\", \"type\": \"INT\"&#125;, &#123;\"name\": \"payment_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"payment_status\", \"type\": \"INT\"&#125;, &#123;\"name\": \"payment_amout\", \"type\": \"INT\"&#125;, &#123;\"name\": \"refund_type\", \"type\": \"INT\"&#125;, &#123;\"name\": \"refund_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"refund_status\", \"type\": \"INT\"&#125;, &#123;\"name\": \"refund_amount\", \"type\": \"FLOAT\"&#125;, &#123;\"name\": \"settlement_rate\", \"type\": \"INT\"&#125;, &#123;\"name\": \"settlement_status\", \"type\": \"INT\"&#125;, &#123;\"name\": \"truck_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"create_time\", \"type\": \"STRING\"&#125;, &#123;\"name\": \"update_time\", \"type\": \"STRING\"&#125;, &#123;\"name\": \"delete_flag\", \"type\": \"INT\"&#125;, &#123;\"name\": \"index_num\", \"type\": \"INT\"&#125;, &#123;\"name\": \"order_date\", \"type\": \"STRING\"&#125; ], \"writeMode\": \"append\", \"fieldDelimiter\": \",\" &#125; &#125; &#125; ], \"setting\": &#123; \"speed\": &#123; \"channel\": 1 &#125; &#125; &#125;&#125; sql2hive1234567891011121314151617181920212223242526272829303132333435363738394041424344&#123; \"job\":&#123; \"content\": [ &#123; \"reader\": &#123; \"name\": \"mysqlreader\", \"parameter\": &#123; \"username\": \"hive\", \"password\": \"wojiushiwo\", \"connection\": [ &#123; \"querySql\": [ \"select order_id, channel_id, order_sn, is_serv_order, is_personal, order_type from datax_sql_tab;\" ], \"jdbcUrl\": [ \"jdbc:mysql://127.0.0.1:3306/test\" ] &#125; ] &#125; &#125;, \"writer\": &#123; \"name\": \"hdfswriter\", \"parameter\": &#123; \"defaultFS\": \"hdfs://39.99.221.146:9000\", \"fileType\": \"text\", \"path\": \"/user/hive/warehouse/orders.db/datax_sql_tab\", \"fileName\": \"datax_sql_tab\", \"column\": [ &#123;\"name\": \"order_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"channel_id\", \"type\": \"INT\"&#125;, &#123;\"name\": \"order_sn\", \"type\": \"STRING\"&#125;, &#123;\"name\": \"is_serv_order\", \"type\": \"INT\"&#125;, &#123;\"name\": \"is_personal\", \"type\": \"INT\"&#125;, &#123;\"name\": \"order_type\", \"type\": \"INT\"&#125; ], \"writeMode\": \"append\", \"fieldDelimiter\": \",\" &#125; &#125; &#125; ], \"setting\": &#123; \"speed\": &#123; \"channel\": 1 &#125; &#125; &#125;&#125;","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"datax","slug":"datax","permalink":"http://yoursite.com/tags/datax/"}]},{"title":"mysql_å­˜å‚¨è¿‡ç¨‹/è§¦å‘å™¨ç¤ºä¾‹","slug":"mysql-å­˜å‚¨è¿‡ç¨‹-è§¦å‘å™¨ç¤ºä¾‹","date":"2020-05-09T10:15:44.000Z","updated":"2020-05-09T10:15:44.250Z","comments":true,"path":"2020/05/09/mysql-å­˜å‚¨è¿‡ç¨‹-è§¦å‘å™¨ç¤ºä¾‹/","link":"","permalink":"http://yoursite.com/2020/05/09/mysql-%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B-%E8%A7%A6%E5%8F%91%E5%99%A8%E7%A4%BA%E4%BE%8B/","excerpt":"","text":"å­˜å‚¨è¿‡ç¨‹1234567891011121314151617create procedure three2(in d double(5, 2),out v1 double(5,2),out v2 double(5,2),out v3 double(5,2),out v4 double(5,2))BEGINselect max(price_val) into v1 from price_log;select min(price_val) into v2 from price_log;select avg(price_val) into v3 from price_log;select d into v4;end;call three2(12, @v1, @v2, @v3, @v4);select @v1, @v2, @v3, @v4; è§¦å‘å™¨123create trigger demo before update on sort_test for each rowset new.node &#x3D; 100;","categories":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"HQLæ‰§è¡Œæµç¨‹","slug":"HQLæ‰§è¡Œæµç¨‹","date":"2020-05-09T09:45:14.000Z","updated":"2020-05-09T09:45:14.944Z","comments":true,"path":"2020/05/09/HQLæ‰§è¡Œæµç¨‹/","link":"","permalink":"http://yoursite.com/2020/05/09/HQL%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B/","excerpt":"","text":"","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"hiveå­˜å‚¨æ¨¡å‹","slug":"hiveå­˜å‚¨æ¨¡å‹","date":"2020-05-09T09:40:47.000Z","updated":"2020-05-09T09:40:47.342Z","comments":true,"path":"2020/05/09/hiveå­˜å‚¨æ¨¡å‹/","link":"","permalink":"http://yoursite.com/2020/05/09/hive%E5%AD%98%E5%82%A8%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"hiveå­˜å‚¨æ¨¡å‹ä¸€ã€å†…éƒ¨è¡¨ä¸å¤–éƒ¨è¡¨ å†…éƒ¨è¡¨çš„ä¸€åˆ‡éƒ½åœ¨hiveä¸Šç®¡ç†ï¼Œåˆ é™¤äº†å†…éƒ¨è¡¨ï¼Œå°†ä¸¢å¤±æ•°æ®å¤–éƒ¨è¡¨hiveä¸Šåªæ˜¯æä¾›æ“ä½œä»¥åŠç»“æ„ï¼Œæ•°æ®åˆ†å¼€å­˜å‚¨ï¼Œåˆ é™¤å¤–éƒ¨è¡¨ï¼Œæ•°æ®ä¸ä¼šä¸¢å¤±ï¼Œé‡æ–°å»ºè¡¨å³å¯æ­£å¸¸ä½¿ç”¨ 1. å»ºç«‹å†…éƒ¨è¡¨123456789create table atm (id int,name string,address string,money float)row format delimitedfileds terminated by ','stored as textfile; 1load data local inpath &#39;file_path&#39; into table atm; 2. å»ºç«‹å¤–éƒ¨è¡¨123456789create external table ex_atm(id int,name string,address string,money float)row format delimitedfields terminated by ','stored as textfile 1load data local inpath &#39;file_path&#39; into table ex_atm; äºŒã€åˆ†åŒºä¸åˆ†æ¡¶ åˆ†åŒºè¡¨åˆ›å»ºè¡¨æ—¶æ‰§è¡Œä¸€ä¸ªåˆ†åŒºæ ‡è¯†ï¼Œä¼ å…¥æ•°æ®çš„æ—¶å€™æŒ‡å®šåˆ†åŒºæ ‡è¯†ï¼Œæ²¡æœ‰é«˜çº§åˆ°ä½¿ç”¨æ•°æ®ä¸­çš„åˆ—ä½œä¸ºåˆ†åŒºæ ‡è¯†ï¼Œåˆ†åŒºå°±æ˜¯å°†æ•°æ®æŒ‰æ ‡è¯†æ”¾åˆ°hdfsä¸Šä¸åŒæ–‡ä»¶å¤¹ä¸‹ï¼Œåœ¨æŸ¥è¯¢çš„æ—¶å€™å¯ä»¥å‡å°‘æ•°æ®é‡ï¼Œè€Œä¸æ˜¯å…¨è¡¨æ‰«æåˆ†æ¡¶è¡¨å¯ä»¥æ–¹ä¾¿æŠ½æ · 1. åˆ†åŒºè¡¨çš„åˆ›å»º123456789create table atm (id int,address string,money float)partitoned by (name string)row format delimitedfields terminated by &#39;,&#39;stored as textfile; 1load data local inpath &#39;file_path&#39; into table atm partiton (name&#x3D;&quot;zh&quot;); 1select * from atm where name &#x3D; &#39;zh&#39;; 1show partitions atm; 2. åˆ†æ¡¶è¡¨çš„åˆ›å»º1234567create table bucket_atm (id int, name string, address string, money float)clustered by (name) into 3 bucketsrow format delimitedfields terminated by &#39;,&#39;stored as textfile; 1insert overwrite table bucket_atm select * from atm; 1234567select id, name from table tablesample(bucket x out of y on column);--- x: ä»ç¬¬å‡ ä¸ªåˆ†æ¡¶å¼€å§‹æŠ½å–--- y: æ¯éš”å‡ ä¸ªåˆ†æ¡¶æŠ½å–4åˆ†æ¡¶ x &#x3D; 1, y &#x3D; 2å°±æŠ½å– 1,3åˆ†æ¡¶","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"æ•°æ®åº“çš„ä¼˜åŒ–å™¨","slug":"æ•°æ®åº“çš„ä¼˜åŒ–å™¨","date":"2020-05-09T03:39:13.000Z","updated":"2020-05-09T03:39:13.957Z","comments":true,"path":"2020/05/09/æ•°æ®åº“çš„ä¼˜åŒ–å™¨/","link":"","permalink":"http://yoursite.com/2020/05/09/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8/","excerpt":"","text":"æ•°æ®åº“ä¸»è¦ç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼Œåˆ†åˆ«æ˜¯è§£æå™¨ã€ä¼˜åŒ–å™¨å’Œæ‰§è¡Œå¼•æ“ã€‚ RBORBO(Rule-Based Optimizer) åŸºäºè§„åˆ™çš„ä¼˜åŒ–å™¨ã€‚æ˜¯æ ¹æ®å·²ç»åˆ¶å®šå¥½çš„ä¸€äº›ä¼˜åŒ–è§„åˆ™å¯¹å…³ç³»è¡¨è¾¾å¼è¿›è¡Œè½¬æ¢ï¼Œæœ€ç»ˆç”Ÿæˆä¸€ä¸ªæœ€ä¼˜çš„æ‰§è¡Œè®¡åˆ’ã€‚å®ƒæ˜¯ä¸€ç§ç»éªŒå¼çš„ä¼˜åŒ–æ–¹æ³•ï¼Œä¼˜åŒ–è§„åˆ™éƒ½æ˜¯é¢„å…ˆå®šä¹‰å¥½çš„ï¼Œåªéœ€è¦å°†SQLæŒ‰ç…§ä¼˜åŒ–è§„åˆ™çš„é¡ºåºå¾€ä¸Šå¥—å°±è¡Œï¼Œä¸€æ—¦æ»¡è¶³æŸä¸ªè§„åˆ™åˆ™è¿›è¡Œä¼˜åŒ–ã€‚è¿™æ ·çš„ç»“æœå°±æ˜¯åŒæ ·ä¸€æ¡SQLï¼Œæ— è®ºè¯»å–çš„è¡¨ä¸­çš„æ•°æ®æ˜¯æ€æ ·çš„ï¼Œæœ€åç”Ÿæˆçš„æ‰§è¡Œè®¡åˆ’éƒ½æ˜¯ä¸€æ ·çš„ï¼ˆä¼˜åŒ–è§„åˆ™éƒ½ä¸€æ ·ï¼‰ã€‚è€Œä¸”SQLçš„å†™æ³•ä¸åŒä¹Ÿå¾ˆæœ‰å¯èƒ½å½±å“æœ€ç»ˆçš„æ‰§è¡Œè®¡åˆ’ï¼Œä»è€Œå½±å“SQLçš„æ€§èƒ½ï¼ˆåŸºäºä¼˜åŒ–è§„åˆ™é¡ºåºæ‰§è¡Œï¼‰ã€‚æ‰€ä»¥è¯´ï¼Œè™½ç„¶RBOæ˜¯ä¸€ä¸ªè€å¸æœºï¼ŒçŸ¥é“å¸¸è§çš„å¥—è·¯ï¼Œä½†æ˜¯å½“è·¯å†µä¸åŒæ—¶ï¼Œä¹Ÿæ— æ³•é’ˆå¯¹æ€§çš„è¾¾åˆ°æœ€ä½³çš„æ•ˆæœã€‚ CBOCBOï¼ˆCost-Based Optimizerï¼‰åŸºäºä»£ä»·çš„ä¼˜åŒ–å™¨ã€‚æ ¹æ®ä¼˜åŒ–è§„åˆ™å¯¹å…³ç³»è¡¨è¾¾å¼è¿›è¡Œè½¬æ¢ï¼Œç”Ÿæˆå¤šä¸ªæ‰§è¡Œè®¡åˆ’ï¼Œæœ€åæ ¹æ®ç»Ÿè®¡ä¿¡æ¯å’Œä»£ä»·æ¨¡å‹è®¡ç®—æ¯ä¸ªæ‰§è¡Œè®¡åˆ’çš„Costã€‚ä»ä¸­æŒ‘é€‰Costæœ€å°çš„æ‰§è¡Œè®¡åˆ’ä½œä¸ºæœ€ç»ˆçš„æ‰§è¡Œè®¡åˆ’ã€‚ä»æè¿°æ¥çœ‹ï¼ŒCBOæ˜¯ä¼˜äºRBOçš„ï¼ŒRBOåªè®¤è§„åˆ™ï¼Œå¯¹æ•°æ®ä¸æ•æ„Ÿï¼Œè€Œåœ¨å®é™…çš„è¿‡ç¨‹ä¸­ï¼Œæ•°æ®çš„é‡çº§ä¼šä¸¥é‡å½±å“åŒæ ·SQLçš„æ€§èƒ½ã€‚æ‰€ä»¥ä»…ä»…é€šè¿‡RBOç”Ÿæˆçš„æ‰§è¡Œè®¡åˆ’å¾ˆæœ‰å¯èƒ½ä¸æ˜¯æœ€ä¼˜çš„ã€‚è€ŒCBOä¾èµ–äºç»Ÿè®¡ä¿¡æ¯å’Œä»£ä»·æ¨¡å‹ï¼Œç»Ÿè®¡ä¿¡æ¯çš„å‡†ç¡®ä¸å¦ã€ä»£ä»·æ¨¡å‹æ˜¯å¦åˆç†éƒ½ä¼šå½±å“CBOé€‰æ‹©æœ€ä¼˜è®¡åˆ’ã€‚ç›®å‰å„å¤§æ•°æ®åº“å’Œå¤§æ•°æ®è®¡ç®—å¼•æ“éƒ½å·²ç»åœ¨ä½¿ç”¨CBOäº†ï¼Œæ¯”å¦‚Oracleã€Hiveã€Sparkã€Flinkç­‰ç­‰ã€‚ åŠ¨æ€CBOåŠ¨æ€CBOï¼Œå°±æ˜¯åœ¨æ‰§è¡Œè®¡åˆ’ç”Ÿæˆçš„è¿‡ç¨‹ä¸­åŠ¨æ€ä¼˜åŒ–çš„æ–¹å¼ã€‚éšç€å¤§æ•°æ®æŠ€æœ¯çš„é£é€Ÿå‘å±•ï¼Œé™æ€çš„CBOå·²ç»æ— æ³•æ»¡è¶³æˆ‘ä»¬SQLä¼˜åŒ–çš„éœ€è¦äº†ï¼Œé™æ€çš„ç»Ÿè®¡ä¿¡æ¯æ— æ³•æä¾›å‡†ç¡®çš„å‚è€ƒï¼Œåœ¨æ‰§è¡Œè®¡åˆ’çš„ç”Ÿæˆè¿‡ç¨‹ä¸­åŠ¨æ€ç»Ÿè®¡æ‰ä¼šå¾—åˆ°æœ€ä¼˜çš„æ‰§è¡Œè®¡åˆ’ã€‚","categories":[{"name":"æ•°æ®åº“","slug":"æ•°æ®åº“","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"ä¼˜åŒ–å™¨","slug":"ä¼˜åŒ–å™¨","permalink":"http://yoursite.com/tags/%E4%BC%98%E5%8C%96%E5%99%A8/"}]},{"title":"pythonæ“ä½œzookeeper","slug":"pythonæ“ä½œzookeeper","date":"2020-05-07T06:45:17.000Z","updated":"2020-05-07T06:45:17.729Z","comments":true,"path":"2020/05/07/pythonæ“ä½œzookeeper/","link":"","permalink":"http://yoursite.com/2020/05/07/python%E6%93%8D%E4%BD%9Czookeeper/","excerpt":"","text":"ä¸€ã€å®‰è£…1pip install kazoo äºŒã€è¿æ¥Zookeeper123from kazoo.client import KazooClientzk = KazooClient(hosts='39.99.221.106') é»˜è®¤ç«¯å£ä¸º2181ï¼Œæ³¨æ„é˜²ç«å¢™/å®‰å…¨ç»„ ä¸‰ã€åˆ›å»ºèŠ‚ç‚¹ pathï¼š èŠ‚ç‚¹è·¯å¾„ valueï¼š èŠ‚ç‚¹å¯¹åº”çš„å€¼ï¼Œæ³¨æ„å€¼çš„ç±»å‹æ˜¯ bytes ephemeralï¼š è‹¥ä¸º True åˆ™åˆ›å»ºä¸€ä¸ªä¸´æ—¶èŠ‚ç‚¹ï¼Œsession ä¸­æ–­åè‡ªåŠ¨åˆ é™¤è¯¥èŠ‚ç‚¹ã€‚é»˜è®¤ False sequence: è‹¥ä¸º True åˆ™åœ¨ä½ åˆ›å»ºèŠ‚ç‚¹ååé¢å¢åŠ 10ä½æ•°å­—ï¼ˆä¾‹å¦‚ï¼šä½ åˆ›å»ºä¸€ä¸ª testplatform/test èŠ‚ç‚¹ï¼Œå®é™…åˆ›å»ºçš„æ˜¯ testplatform/test0000000003ï¼Œè¿™ä¸²æ•°&gt;å­—æ˜¯é¡ºåºé€’å¢çš„ï¼‰ã€‚é»˜è®¤ False makepathï¼š è‹¥ä¸º False çˆ¶èŠ‚ç‚¹ä¸å­˜åœ¨æ—¶æŠ› NoNodeErrorã€‚è‹¥ä¸º True çˆ¶èŠ‚ç‚¹ä¸å­˜åœ¨åˆ™åˆ›å»ºçˆ¶èŠ‚ç‚¹ã€‚é»˜è®¤ False 12345678from kazoo.client import KazooClientzk = KazooClient(hosts=\"39.99.221.106\")zk.start()zk.create(\"/testplatform/test\", b\"this is test\", makepath=True)zk.stop() å››ã€æŸ¥çœ‹èŠ‚ç‚¹ get_children() æŸ¥çœ‹å­èŠ‚ç‚¹, get() æŸ¥çœ‹å€¼ 12345678910from kazoo.client import KazooClientzk = KazooClient(hosts=\"39.99.221.106\")zk.start()node = zk.get_children(\"/testplatform\")value = zk.get(\"/testplatform/test\")zk.stop() äº”ã€æ›´æ”¹èŠ‚ç‚¹12345678910from kazoo.client import KazooClientzk = KazooClient(hosts=\"39.99.221.106\")zk.start()zk.set(\"/testplatform/test\", b\"testabc\")value = zk.get(\"/testplatform/test\")zk.stop() å…­ã€åˆ é™¤èŠ‚ç‚¹1234567from kazoo.client import KazooClientzk = KazooClient(hosts=\"39.99.221.106\")zk.start()zk.delete(\"/testplatform/test\", recursive=False)zk.stop() å‚æ•° recursiveï¼šè‹¥ä¸º Falseï¼Œå½“éœ€è¦åˆ é™¤çš„èŠ‚ç‚¹å­˜åœ¨å­èŠ‚ç‚¹ï¼Œä¼šæŠ›å¼‚å¸¸ NotEmptyError ã€‚è‹¥ä¸ºTrueï¼Œåˆ™åˆ é™¤ æ­¤èŠ‚ç‚¹ ä»¥åŠ åˆ é™¤è¯¥èŠ‚ç‚¹çš„æ‰€æœ‰å­èŠ‚ç‚¹ ä¸ƒã€watches å®è·µ12345678910from kazoo.client import KazooClientzk = KazooClient(hosts=\"39.99.221.106\")zk.start()def test(event): print(\"è§¦å‘äº‹ä»¶\")if __name__ == \"__main__\": zk.get(\"/testplatform/test\", watch=test)","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"},{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}]},{"title":"ZOOKEEPERåŸºç¡€","slug":"ZOOKEEPERåŸºç¡€","date":"2020-05-07T06:06:44.000Z","updated":"2020-05-07T06:22:21.418Z","comments":true,"path":"2020/05/07/ZOOKEEPERåŸºç¡€/","link":"","permalink":"http://yoursite.com/2020/05/07/ZOOKEEPER%E5%9F%BA%E7%A1%80/","excerpt":"","text":"ZOOKEEPER ç›‘å¬å™¨åŸç†:ï¼ˆ1ï¼‰åœ¨Zookeeperçš„APIæ“ä½œä¸­ï¼Œåˆ›å»ºmain()ä¸»æ–¹æ³•å³ä¸»çº¿ç¨‹ï¼› ï¼ˆ2ï¼‰åœ¨mainçº¿ç¨‹ä¸­åˆ›å»ºZookeeperå®¢æˆ·ç«¯ï¼ˆzkClientï¼‰ï¼Œè¿™æ—¶ä¼šåˆ›å»ºä¸¤ä¸ªçº¿ç¨‹ï¼š çº¿ç¨‹connetè´Ÿè´£ç½‘ç»œé€šä¿¡è¿æ¥ï¼Œè¿æ¥æœåŠ¡å™¨ï¼› çº¿ç¨‹Listenerè´Ÿè´£ç›‘å¬ï¼›ï¼ˆ3ï¼‰å®¢æˆ·ç«¯é€šè¿‡connetçº¿ç¨‹è¿æ¥æœåŠ¡å™¨ï¼› å›¾ä¸­getChildren(&quot;/&quot; , true) ï¼Œ&quot; / &quot;è¡¨ç¤ºç›‘å¬çš„æ˜¯æ ¹ç›®å½•ï¼Œtrueè¡¨ç¤ºç›‘å¬ï¼Œä¸ç›‘å¬ç”¨falseï¼ˆ4ï¼‰åœ¨Zookeeperçš„æ³¨å†Œç›‘å¬åˆ—è¡¨ä¸­å°†æ³¨å†Œçš„ç›‘å¬äº‹ä»¶æ·»åŠ åˆ°åˆ—è¡¨ä¸­ï¼Œè¡¨ç¤ºè¿™ä¸ªæœåŠ¡å™¨ä¸­çš„/pathï¼Œå³æ ¹ç›®å½•è¿™ä¸ªè·¯å¾„è¢«å®¢æˆ·ç«¯ç›‘å¬äº†ï¼› ï¼ˆ5ï¼‰ä¸€æ—¦è¢«ç›‘å¬çš„æœåŠ¡å™¨æ ¹ç›®å½•ä¸‹ï¼Œæ•°æ®æˆ–è·¯å¾„å‘ç”Ÿæ”¹å˜ï¼ŒZookeeperå°±ä¼šå°†è¿™ä¸ªæ¶ˆæ¯å‘é€ç»™Listenerçº¿ç¨‹ï¼› ï¼ˆ6ï¼‰Listenerçº¿ç¨‹å†…éƒ¨è°ƒç”¨processæ–¹æ³•ï¼Œé‡‡å–ç›¸åº”çš„æªæ–½ï¼Œä¾‹å¦‚æ›´æ–°æœåŠ¡å™¨åˆ—è¡¨ç­‰ã€‚ é€‰ä¸¾æœºåˆ¶ï¼š1)åŠæ•°æœºåˆ¶:é›†ç¾¤ä¸­åŠæ•°ä»¥ä¸Šæœºå™¨å­˜æ´»ï¼Œé›†ç¾¤å¯ç”¨ã€‚æ‰€ä»¥ Zookeeper é€‚åˆå®‰è£…å¥‡æ•°å° æœåŠ¡å™¨ã€‚2)Zookeeper è™½ç„¶åœ¨é…ç½®æ–‡ä»¶ä¸­å¹¶æ²¡æœ‰æŒ‡å®š Master å’Œ Slaveã€‚ä½†æ˜¯ï¼ŒZookeeper å·¥ä½œæ—¶ï¼Œ æ˜¯æœ‰ä¸€ä¸ªèŠ‚ç‚¹ä¸º Leaderï¼Œå…¶ä»–åˆ™ä¸º Followerï¼ŒLeader æ˜¯é€šè¿‡å†…éƒ¨çš„é€‰ä¸¾æœºåˆ¶ä¸´æ—¶äº§ç”Ÿçš„ã€‚ (1)æœåŠ¡å™¨ 1 å¯åŠ¨ï¼Œå‘èµ·ä¸€æ¬¡é€‰ä¸¾ã€‚æœåŠ¡å™¨ 1 æŠ•è‡ªå·±ä¸€ç¥¨ã€‚æ­¤æ—¶æœåŠ¡å™¨ 1 ç¥¨æ•°ä¸€ç¥¨ï¼Œä¸å¤ŸåŠæ•°ä»¥ä¸Š(3 ç¥¨)ï¼Œé€‰ä¸¾æ— æ³•å®Œæˆï¼ŒæœåŠ¡å™¨ 1 çŠ¶æ€ä¿æŒä¸º LOOKING;(2)æœåŠ¡å™¨ 2 å¯åŠ¨ï¼Œå†å‘èµ·ä¸€æ¬¡é€‰ä¸¾ã€‚æœåŠ¡å™¨ 1 å’Œ 2 åˆ†åˆ«æŠ•è‡ªå·±ä¸€ç¥¨å¹¶äº¤æ¢é€‰ç¥¨ä¿¡æ¯: æ­¤æ—¶æœåŠ¡å™¨ 1 å‘ç°æœåŠ¡å™¨ 2 çš„ ID æ¯”è‡ªå·±ç›®å‰æŠ•ç¥¨æ¨ä¸¾çš„(æœåŠ¡å™¨ 1)å¤§ï¼Œæ›´æ”¹é€‰ç¥¨ä¸ºæ¨ä¸¾ æœåŠ¡å™¨ 2ã€‚æ­¤æ—¶æœåŠ¡å™¨ 1 ç¥¨æ•° 0 ç¥¨ï¼ŒæœåŠ¡å™¨ 2 ç¥¨æ•° 2 ç¥¨ï¼Œæ²¡æœ‰åŠæ•°ä»¥ä¸Šç»“æœï¼Œé€‰ä¸¾æ— æ³•å®Œæˆï¼ŒæœåŠ¡å™¨ 1ï¼Œ2 çŠ¶æ€ä¿æŒ LOOKING(3)æœåŠ¡å™¨ 3 å¯åŠ¨ï¼Œå‘èµ·ä¸€æ¬¡é€‰ä¸¾ã€‚æ­¤æ—¶æœåŠ¡å™¨ 1 å’Œ 2 éƒ½ä¼šæ›´æ”¹é€‰ç¥¨ä¸ºæœåŠ¡å™¨ 3ã€‚æ­¤æ¬¡æŠ•ç¥¨ç»“æœ:æœåŠ¡å™¨ 1 ä¸º 0 ç¥¨ï¼ŒæœåŠ¡å™¨ 2 ä¸º 0 ç¥¨ï¼ŒæœåŠ¡å™¨ 3 ä¸º 3 ç¥¨ã€‚æ­¤æ—¶æœåŠ¡å™¨ 3 çš„ç¥¨æ•°å·² ç»è¶…è¿‡åŠæ•°ï¼ŒæœåŠ¡å™¨ 3 å½“é€‰ Leaderã€‚æœåŠ¡å™¨ 1ï¼Œ2 æ›´æ”¹çŠ¶æ€ä¸º FOLLOWINGï¼ŒæœåŠ¡å™¨ 3 æ›´æ”¹ çŠ¶æ€ä¸º LEADING;(4)æœåŠ¡å™¨ 4 å¯åŠ¨ï¼Œå‘èµ·ä¸€æ¬¡é€‰ä¸¾ã€‚æ­¤æ—¶æœåŠ¡å™¨ 1ï¼Œ2ï¼Œ3 å·²ç»ä¸æ˜¯ LOOKING çŠ¶æ€ï¼Œ ä¸ä¼šæ›´æ”¹é€‰ç¥¨ä¿¡æ¯ã€‚äº¤æ¢é€‰ç¥¨ä¿¡æ¯ç»“æœ:æœåŠ¡å™¨ 3 ä¸º 3 ç¥¨ï¼ŒæœåŠ¡å™¨ 4 ä¸º 1 ç¥¨ã€‚æ­¤æ—¶æœåŠ¡å™¨ 4 æœä»å¤šæ•°ï¼Œæ›´æ”¹é€‰ç¥¨ä¿¡æ¯ä¸ºæœåŠ¡å™¨ 3ï¼Œå¹¶æ›´æ”¹çŠ¶æ€ä¸º FOLLOWING;(5)æœåŠ¡å™¨ 5 å¯åŠ¨ï¼ŒåŒ 4 ä¸€æ ·å½“å°å¼Ÿã€‚ é›†ç¾¤æœ€å°‘éœ€è¦æœºå™¨æ•°:3 ###å†™æ•°æ®æµç¨‹","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}]},{"title":"zookeeperå®‰è£…éƒ¨ç½²","slug":"zookeeperå®‰è£…éƒ¨ç½²","date":"2020-05-07T03:53:34.000Z","updated":"2020-05-07T03:53:34.490Z","comments":true,"path":"2020/05/07/zookeeperå®‰è£…éƒ¨ç½²/","link":"","permalink":"http://yoursite.com/2020/05/07/zookeeper%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/","excerpt":"","text":"zookeeperå®‰è£…éƒ¨ç½²ä¸€ã€æœ¬åœ°æ¨¡å¼å®‰è£…éƒ¨ç½²1. ä¸‹è½½å®‰è£…1wget https://mirrors.huaweicloud.com/apache/zookeeper/zookeeper-3.4.13/zookeeper-3.4.13.tar.gz 1tar -zxvf zookeeper-3.4.13.tar.gz -C ../ 12cd zookeeper-3.4.13mkdir zkData 12cd conf/cp zoo_sample.cfg zoo.cfg 12vim zoo.cfgdataDir=/home/kowhoy/software/zookeeper-3.4.13/zkData 2. ä½¿ç”¨12345bin&#x2F;zkServer.sh start #å¯åŠ¨serverbin&#x2F;zkServer.sh status #æŸ¥çœ‹çŠ¶æ€bin&#x2F;zkServer.sh stop #åœæ­¢stopbin&#x2F;zkCli.sh #å¯åŠ¨å®¢æˆ·ç«¯ äºŒã€åˆ†å¸ƒå¼å®‰è£…éƒ¨ç½²1. ä¸‹è½½è§£å‹2. myid åˆ›å»ºzkData1234cd zkDatavim myid2 ##ä¾æ¬¡ 3ï¼Œ 4 3. zoo.cfg1234567dataDir&#x3D;&#x2F;home&#x2F;kowhoy&#x2F;software&#x2F;zookeeper-3.4.13&#x2F;zkDataquorumListenOnAllIPs&#x3D;trueserver.2&#x3D;ecs01:2888:3888server.3&#x3D;ecs00:2888:3888server.4&#x3D;bcc00:2888:3888 1234567891011server.A&#x3D;B:C:DA æ˜¯ä¸€ä¸ªæ•°å­—ï¼Œè¡¨ç¤ºè¿™ä¸ªæ˜¯ç¬¬å‡ å·æœåŠ¡å™¨;é›†ç¾¤æ¨¡å¼ä¸‹é…ç½®ä¸€ä¸ªæ–‡ä»¶ myidï¼Œè¿™ä¸ªæ–‡ä»¶åœ¨ dataDir ç›®å½•ä¸‹ï¼Œè¿™ä¸ªæ–‡ä»¶é‡Œé¢æœ‰ä¸€ä¸ªæ•°æ® å°±æ˜¯ A çš„å€¼ï¼ŒZookeeper å¯åŠ¨æ—¶è¯»å–æ­¤æ–‡ä»¶ï¼Œæ‹¿åˆ°é‡Œé¢çš„æ•°æ®ä¸ zoo.cfg é‡Œé¢çš„é…ç½®ä¿¡æ¯æ¯” è¾ƒä»è€Œåˆ¤æ–­åˆ°åº•æ˜¯å“ªä¸ª serverã€‚B æ˜¯è¿™ä¸ªæœåŠ¡å™¨çš„åœ°å€; C æ˜¯è¿™ä¸ªæœåŠ¡å™¨ Follower ä¸é›†ç¾¤ä¸­çš„ Leader æœåŠ¡å™¨äº¤æ¢ä¿¡æ¯çš„ç«¯å£; D æ˜¯ä¸‡ä¸€é›†ç¾¤ä¸­çš„ Leader æœåŠ¡å™¨æŒ‚äº†ï¼Œéœ€è¦ä¸€ä¸ªç«¯å£æ¥é‡æ–°è¿›è¡Œé€‰ä¸¾ï¼Œé€‰å‡ºä¸€ä¸ªæ–°çš„Leaderï¼Œè€Œè¿™ä¸ªç«¯å£å°±æ˜¯ç”¨æ¥æ‰§è¡Œé€‰ä¸¾æ—¶æœåŠ¡å™¨ç›¸äº’é€šä¿¡çš„ç«¯å£ã€‚ 4.åŒæ­¥åˆ°å…¶ä»–æœºå™¨ä¸Š5. åˆ†åˆ«å¯åŠ¨1./zkServer.sh start 6. æŸ¥çœ‹çŠ¶æ€1./zkServer.sh status","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}]},{"title":"HIVEå®‰è£…é…ç½®","slug":"HIVEå®‰è£…é…ç½®","date":"2020-05-07T02:10:43.000Z","updated":"2020-05-07T02:10:43.794Z","comments":true,"path":"2020/05/07/HIVEå®‰è£…é…ç½®/","link":"","permalink":"http://yoursite.com/2020/05/07/HIVE%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/","excerpt":"","text":"HIVEå®‰è£…é…ç½®ä¸€ã€å‰ç½®ç¯å¢ƒ JAVA HADOOP MYSQL äºŒã€å®‰è£… ä¸‹è½½è§£å‹ 123wget https://mirrors.huaweicloud.com/apache/hive/hive-2.3.6/apache-hive-2.3.6-bin.tar.gztar -zxvf apache-hive-2.3.6-bin.tar.gz -C ../ é…ç½®ç¯å¢ƒå˜é‡ vim ~/.bash_profile 12HIVE_HOME&#x3D;&#x2F;home&#x2F;kowhoy&#x2F;software&#x2F;apache-hive-2.3.6-binexport PATH&#x3D;$HIVE_HOME&#x2F;bin:$PATH ä¸‰ã€é…ç½® æ‹·è´é…ç½®æ–‡ä»¶ 1234567cd $&#123;HIVE_HOME&#125;/confcp hive-env.sh.template hive-env.shcp hive-exec-log4j2.properties.template hive-exec-log4j2.propertiescp hive-log4j2.properties.template hive-log4j2.propertiescp hive-default.xml.template hive-site.xmlcp beeline-log4j2.properties.template beeline-log4j2.properties vim hive-env.sh 12export JAVA_HOME&#x3D;&#x2F;home&#x2F;kowhoy&#x2F;software&#x2F;jdk1.8.0_191export HADOOP_HOME&#x3D;&#x2F;home&#x2F;kowhoy&#x2F;software&#x2F;hadoop-2.8.5 vim hive-site.xml æ·»åŠ  123456789&lt;property&gt;&lt;name&gt;system:java.io.tmpdir&lt;&#x2F;name&gt;&lt;value&gt;&#x2F;tmp&#x2F;hive&#x2F;java&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt;&lt;name&gt;system:user.name&lt;&#x2F;name&gt;&lt;value&gt;$&#123;user.name&#125;&lt;&#x2F;value&gt;&lt;&#x2F;property&gt; ä¿®æ”¹12345678910111213141516171819&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionURL&lt;&#x2F;name&gt;&lt;value&gt;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;metastore?createDatabaseIfNotExist&#x3D;true&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;&#x2F;name&gt;&lt;value&gt;com.mysql.jdbc.Driver&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;&#x2F;name&gt;&lt;value&gt;hive&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;&#x2F;name&gt;&lt;value&gt;passwd&lt;&#x2F;value&gt;&lt;&#x2F;property&gt; å››ã€æ·»åŠ jaråŒ…1cp mysql-connector-java-5.1.48-bin.jar ../apache-hive-2.3.6-bin/lib/ äº”ã€hdfsåˆ›å»ºæ–‡ä»¶å¤¹å¹¶è®¾ç½®æƒé™123456hdfs dfs -mkdir &#x2F;hivehdfs dfs -mkdir &#x2F;hive&#x2F;tmphdfs dfs -mkdir &#x2F;hive&#x2F;loghdfs dfs -mkdir &#x2F;hive&#x2F;warehousehdfs dfs -chmod 777 &#x2F;hive&#x2F;tmp å…­ã€å¼‚å¸¸å¤„ç† å‡ºç°123Failed to open new session: java.lang.RuntimeException:org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User:xxx not allowed to impersonate anonymous vim ${HADOOP_HOME}/etc/hadoop/core-site.xml 123456789&lt;property&gt;&lt;name&gt;hadoop.proxyuser.xxx.hosts&lt;&#x2F;name&gt;&lt;value&gt;*&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt;&lt;name&gt;hadoop.proxyuser.xxx.groups&lt;&#x2F;name&gt;&lt;value&gt;*&lt;&#x2F;value&gt;&lt;&#x2F;property&gt; æŠ¥é”™ä¿¡æ¯ä¸­çš„xxx ä¸é…ç½®ä¸­çš„xxxä¸€è‡´ ä¸ƒã€åˆå§‹åŒ–æ•°æ®åº“1$&#123;HIVE_HOME&#125;&#x2F;bin&#x2F;schematool --dbType mysql --initSchema å…«ã€ä½¿ç”¨ hive-client 1hive beenline å¼€å¯ 1$&#123;HIVE_HOME&#125;/bin/hiveserver2 &amp; è¿æ¥ 1$&#123;HIVE_HOME&#125;/bin/beeline -u jdbc:hive2://localhost:10000 username passwd","categories":[{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HIVE","slug":"HIVE","permalink":"http://yoursite.com/tags/HIVE/"}]},{"title":"Pandasè„‘å›¾","slug":"Pandasè„‘å›¾","date":"2020-04-26T10:02:45.000Z","updated":"2020-04-26T10:02:45.508Z","comments":true,"path":"2020/04/26/Pandasè„‘å›¾/","link":"","permalink":"http://yoursite.com/2020/04/26/Pandas%E8%84%91%E5%9B%BE/","excerpt":"","text":"","categories":[{"name":"Pandas","slug":"Pandas","permalink":"http://yoursite.com/categories/Pandas/"},{"name":"Python","slug":"Pandas/Python","permalink":"http://yoursite.com/categories/Pandas/Python/"}],"tags":[{"name":"Pandas","slug":"Pandas","permalink":"http://yoursite.com/tags/Pandas/"}]},{"title":"Numpyè„‘å›¾","slug":"Numpyè„‘å›¾","date":"2020-04-26T10:01:45.000Z","updated":"2020-04-26T10:01:45.596Z","comments":true,"path":"2020/04/26/Numpyè„‘å›¾/","link":"","permalink":"http://yoursite.com/2020/04/26/Numpy%E8%84%91%E5%9B%BE/","excerpt":"","text":"","categories":[{"name":"Numpy","slug":"Numpy","permalink":"http://yoursite.com/categories/Numpy/"},{"name":"Python","slug":"Numpy/Python","permalink":"http://yoursite.com/categories/Numpy/Python/"}],"tags":[{"name":"Numpy","slug":"Numpy","permalink":"http://yoursite.com/tags/Numpy/"}]},{"title":"matplotlibåŸºç¡€å›¾è¡¨","slug":"matplotlibåŸºç¡€å›¾è¡¨","date":"2020-04-26T09:53:07.000Z","updated":"2020-04-26T10:13:45.715Z","comments":true,"path":"2020/04/26/matplotlibåŸºç¡€å›¾è¡¨/","link":"","permalink":"http://yoursite.com/2020/04/26/matplotlib%E5%9F%BA%E7%A1%80%E5%9B%BE%E8%A1%A8/","excerpt":"","text":"matplotlibåŸºç¡€å›¾è¡¨ä¸€ã€æ ¹æ®x,yç»˜åˆ¶12345678910111213import numpy as npfrom matplotlib import pyplot as pltx = np.arange(1, 11)y = x * 2 + 1plt.title(\"demo1\")plt.xlabel(\"x\")plt.ylabel(\"y\")plt.plot(x, y)plt.show() 12345678910111213import numpy as npfrom matplotlib import pyplot as pltx = np.arange(0, 3 * np.pi, 0.1)y = np.sin(x)plt.title(\"demo\")plt.xlabel(\"x\")plt.ylabel(\"y\")plt.plot(x, y, \"om\") #oæŒ‡ä»¥åœ†ç‚¹å±•ç¤º, mè¡¨ç¤ºç«çº¢è‰²plt.show() 123456789101112131415161718192021222324252627282930import numpy as npfrom matplotlib import pyplot as pltp1_x = np.arange(1, 11)p1_y = p1_x * 5 + 8p2_x = np.arange(1, 11)p2_y = p2_x ** 3 + 10p3_x = np.arange(0, 4 * np.pi, 0.1)p3_y = np.sin(p3_x)p4_x = np.arange(0, 4 * np.pi, 0.1)p4_y = np.cos(p4_x)plt.subplot(2, 2, 1) # ä¸¤è¡Œä¸¤åˆ—ç¬¬ä¸€ä¸ªplt.title(\"p1\")plt.plot(p1_x, p1_y, \"om\")plt.subplot(2, 2, 2) # ä¸¤è¡Œä¸¤åˆ—ç¬¬äºŒä¸ªplt.title(\"p2\")plt.plot(p2_x, p2_y, \"pm\")plt.subplot(2, 2, 3)plt.title(\"p3\")plt.plot(p3_x, p3_y)plt.subplot(2, 2, 4)plt.title(\"p4\")plt.plot(p4_x, p4_y) äºŒã€æŸ±çŠ¶å›¾1234567891011121314import numpy as npfrom matplotlib import pyplot as pltb1_x = np.arange(1, 10, 2)b1_y = b1_x * 2 + 10b2_x = np.arange(2, 11, 2)b2_y = b2_x * 3 + 2plt.bar(b1_x, b1_y, color=\"m\", align='center')plt.bar(b2_x, b2_y, color=\"b\", align=\"center\")plt.title(\"bar_demo\")plt.show() ä¸‰ã€ ç›´æ–¹å›¾12345678910import numpy as npfrom matplotlib import pyplot as plta = np.random.randint(50, size=100)plt.hist(a, bins=[0, 10, 20, 30, 40, 50])plt.title(\"histogram\")plt.show() å››ã€æ•£ç‚¹å›¾12345678import numpy as npfrom matplotlib import pyplot as pltx = np.random.randn(1000)y = np.random.randn(1000)plt.plot(x, y, \"om\")plt.show() 12345678import numpy as npfrom matplotlib import pyplot as pltx = np.random.randn(1000)y = np.random.randn(1000)plt.scatter(x, y)plt.show()","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"},{"name":"matplotlib","slug":"matplotlib","permalink":"http://yoursite.com/tags/matplotlib/"}]},{"title":"hadoopäº‘æœåŠ¡å™¨åˆ†å¸ƒå¼å®‰è£…é…ç½®","slug":"hadoopäº‘æœåŠ¡å™¨åˆ†å¸ƒå¼å®‰è£…é…ç½®","date":"2020-04-22T06:10:42.000Z","updated":"2020-04-22T06:10:42.489Z","comments":true,"path":"2020/04/22/hadoopäº‘æœåŠ¡å™¨åˆ†å¸ƒå¼å®‰è£…é…ç½®/","link":"","permalink":"http://yoursite.com/2020/04/22/hadoop%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/","excerpt":"","text":"hadoopäº‘æœåŠ¡å™¨åˆ†å¸ƒå¼å®‰è£…é…ç½® åœ¨ä¸‰å°äº‘æœåŠ¡å™¨ä¸Šæ­å»ºhadoopé›†ç¾¤, 1master + 2slaveä½¿ç”¨ç‰ˆæœ¬: hadoop-3.2.1ã€jdk1.8.0_191ã€ scala-2.13.1âš ï¸ jdk11ç‰ˆæœ¬ä¼šå‡ºç°Caused by: java.lang.NoClassDefFoundError: javax/activation/DataSourceé”™è¯¯ï¼Œè¿˜æ˜¯é€‰æ‹©äº†ä½ç‰ˆæœ¬ ä¸€ã€æœåŠ¡å™¨åˆ›å»ºç”¨æˆ·è®¾ç½®å¯†ç ã€é…ç½®äº’ç›¸ä¹‹é—´å…å¯†ç™»å½•** âš ï¸ æœåŠ¡å™¨çš„hostname ä»¥åŠ /etc/hostsä¸­é…ç½®çš„hostname å‡ä¸å¯å¸¦_ä¸‹åˆ’çº¿ ** ä¸€ã€ä¸‹è½½å®‰è£…åŒ…(ä½¿ç”¨å›½å†…é•œåƒ)script12345wget https://mirrors.huaweicloud.com/java/jdk/8u191-b12/jdk-8u191-linux-x64.tar.gzwget https://downloads.lightbend.com/scala/2.13.1/scala-2.13.1.tgzwget https://mirrors.huaweicloud.com/apache/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz äºŒã€è§£å‹é…ç½®ç¯å¢ƒå˜é‡script12345678910export SCALA_HOME=/home/kowhoy/software/scala-2.13.1export PATH=$SCALA_HOME/bin:$PATHexport HADOOP_HOME=/home/kowhoy/software/hadoop-3.2.1export PATH=$HADOOP_HOME/bin:$PATHJAVA_HOME=/home/kowhoy/software/jdk1.8.0_191JRE_HOME=/home/kowhoy/software/jdk1.8.0_191/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=$JAVA_HOME/bin:$PATH ä¸‰å°æœåŠ¡å™¨å‡è¿›è¡Œä»¥ä¸Šæ“ä½œ ä¸‰ã€é…ç½®master,åŒæ­¥ç»™slave é…ç½®æ–‡ä»¶çš„æ ¹ç›®å½• $HADOOP_HOME/etc/hadoopé…ç½®æ–‡ä»¶ä¸­æ¶‰åŠåˆ°çš„æ–‡ä»¶å¤¹ï¼Œéœ€è¦è‡ªè¡Œåˆ›å»º hadoop-env.sh script1export JAVA_HOME=/home/kowhoy/software/jdk1.8.0_191 core-site.xml 12345678910111213&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;description&gt;hdfsurl&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/kowhoy/software/hadoop-3.2.1/tmp&lt;/value&gt; &lt;description&gt;ä¸´æ—¶æ–‡ä»¶è·¯å¾„&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml 1234567891011121314151617181920212223242526&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;master:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/kowhoy/software/hadoop-3.2.1/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/kowhoy/software/hadoop-3.2.1/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt; &lt;value&gt;/home/kowhoy/software/hadoop-3.2.1/dfs/namesecondary&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;decription&gt;master + slaveä¸ªæ•°&lt;/decription&gt; &lt;/property&gt;&lt;/configuration&gt; mapred-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml 12345678910111213&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;master&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; workerså†™é…ç½®çš„slave 12ecs00bcc00 mastersæ–‡ä»¶æ˜¯æ²¡æœ‰çš„ï¼Œé»˜è®¤masterçš„hostsé…ç½®åº”è¯¥æ˜¯ masterï¼Œå¦‚æœä¸æ˜¯ï¼Œå¯èƒ½æ˜¯åœ¨mastersæ–‡ä»¶ä¸­é…ç½®ï¼Œæœªæµ‹è¯• ä»¥ä¸Šé…ç½®å¯scpåˆ°slaveä¸Šï¼Œæ³¨æ„å¦‚æœhostnameä¸ä¸€è‡´ï¼Œéœ€è¦ä¿®æ”¹ å››ã€å¯åŠ¨é›†ç¾¤ æ ¼å¼åŒ– script1hadoop namenode -format å¯åŠ¨ script1$&#123;HADOOP_HOME&#125;/sbin/start_all.sh jpsæŸ¥çœ‹ script12345678910#master25189 NameNode25638 ResourceManager25417 SecondaryNameNode26638 Jps#slave21406 NodeManager23646 Jps21278 DataNode äº”ã€è¡¥å……å¦‚æœ‰é”™è¯¯ï¼ŒæŸ¥çœ‹${HADOOP_HOME}/logsä¸‹çš„æ—¥å¿—ï¼Œdatanodeç›¸å…³æ—¥å¿—æ˜¯åœ¨slaveä¸Š","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"}]},{"title":"Scalaæ¨¡å¼åŒ¹é…","slug":"Scalaæ¨¡å¼åŒ¹é…","date":"2020-04-21T07:01:32.000Z","updated":"2020-04-21T07:01:32.883Z","comments":true,"path":"2020/04/21/Scalaæ¨¡å¼åŒ¹é…/","link":"","permalink":"http://yoursite.com/2020/04/21/Scala%E6%A8%A1%E5%BC%8F%E5%8C%B9%E9%85%8D/","excerpt":"","text":"æ¨¡å¼åŒ¹é…12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879package com.kowhoy.scalaimport scala.util.Random// æ¨¡å¼åŒ¹é…object MatchDemo &#123; def main(args: Array[String]): Unit = &#123; // åŸºç¡€åŒ¹é… basic_match() // æ¡ä»¶åŒ¹é… condition_match() // ç±»å‹åŒ¹é… type_match(List(1,2,4)) type_match(12) type_match(1.2) // arrayåŒ¹é… array_match(Array()) // å¼‚å¸¸å¤„ç† error_exception_match() &#125; def basic_match(): Unit = &#123; val names = Array(\"A\", \"B\", \"C\") val name = names(Random.nextInt(names.length)) name match &#123; case \"A\" =&gt; println(\"AAAA\") case \"B\" =&gt; println(\"BBBB\") case _ =&gt; println(\"CCCCC\") &#125; &#125; def condition_match(): Unit = &#123; val names = Array(\"A\", \"B\", \"C\") val name = names(Random.nextInt(names.length))// name = \"C\" val buff = \"red\" name match &#123; case \"A\" =&gt; println(\"A\") case \"B\" =&gt; println(\"B\") case _ if(buff == \"red\") =&gt; println(\"D\") case _ =&gt; println(\"C\") &#125; &#125; def type_match(data:Any): Unit = &#123; data match &#123; case x:Int =&gt; println(\"INT\") case x:String =&gt; println(\"String\") case x:List[Any] =&gt; println(\"List\") case _ =&gt; println(\"other\") &#125; &#125; def array_match(array:Array[String]): Unit = &#123; array match &#123; case Array(\"abc\") =&gt; println(\"type1_abc\") case Array(x, y) =&gt; println(\"type2:\", x, y) case Array(x, _*) =&gt; println(\"type3\", x) case _ =&gt; println(\"other\") &#125; &#125; def error_exception_match(): Unit = &#123; try &#123; val d = 10 / 0 &#125; catch &#123; case e: ArithmeticException =&gt; println(\"error0\") case e: Exception =&gt; println(e.getStackTrace) &#125; finally &#123; println(\"Aaaaaaa\") &#125; &#125;&#125;","categories":[{"name":"Scala","slug":"Scala","permalink":"http://yoursite.com/categories/Scala/"}],"tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"}]},{"title":"æŸ¯é‡ŒåŒ–","slug":"æŸ¯é‡ŒåŒ–","date":"2020-04-15T10:29:12.000Z","updated":"2020-04-15T10:29:12.110Z","comments":true,"path":"2020/04/15/æŸ¯é‡ŒåŒ–/","link":"","permalink":"http://yoursite.com/2020/04/15/%E6%9F%AF%E9%87%8C%E5%8C%96/","excerpt":"","text":"æŸ¯é‡ŒåŒ–å®šä¹‰ å°†åŸæ¥å¤šä¸ªå‚æ•°çš„å‡½æ•°ï¼Œåˆ†æˆå¤šä¸ªå•ä¸ªå‚æ•°çš„å‡½æ•° å¥½å¤„ å¤æ‚é€»è¾‘ç®€å•åŒ– ç¤ºä¾‹12345678910111213141516171819package com.kowhoy.scala//æŸ¯é‡ŒåŒ–object keli &#123; def main(args: Array[String]): Unit = &#123; var inc = (x:Int, y:Int) =&gt; x + y println(inc(1, 2)) println(inc_kl(1)(2)) var firstNum = inc_kl((1))_ var res = firstNum(2) println(res) &#125; def inc_kl(x:Int)(y:Int) = &#123; x + y &#125;&#125; æ³¨æ„ å½“åªä¼ é€’éƒ¨åˆ†å‚æ•°ç”Ÿäº§å‡½æ•°çš„æ—¶å€™ï¼Œéœ€è¦ç”¨å ä½ç¬¦_è¡¥å……åé¢çš„å‚æ•°","categories":[{"name":"Scala","slug":"Scala","permalink":"http://yoursite.com/categories/Scala/"}],"tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"}]},{"title":"åŒ¿åå‡½æ•°","slug":"åŒ¿åå‡½æ•°","date":"2020-04-15T10:28:45.000Z","updated":"2020-04-15T10:28:45.241Z","comments":true,"path":"2020/04/15/åŒ¿åå‡½æ•°/","link":"","permalink":"http://yoursite.com/2020/04/15/%E5%8C%BF%E5%90%8D%E5%87%BD%E6%95%B0/","excerpt":"","text":"åŒ¿åå‡½æ•° ç®€å†™å‡½æ•°å®šä¹‰, åŒ¿åå‡½æ•°ä¸å¯ä»¥å†™æˆä¼ åè°ƒç”¨ 12345678910111213141516171819202122232425262728293031package com.kowhoy.scala//åŒ¿åå‡½æ•°object niming &#123; def main(args: Array[String]): Unit = &#123; var initNum = 0 var counter = () =&gt; &#123; initNum += 1 initNum &#125; var sendValue = (x: Int) =&gt; &#123; for (i &lt;- 0 to 5) &#123; println(x) &#125; &#125; sendValue(counter()) sendName(counter()) &#125; def sendName(x: =&gt; Int) = &#123; for (i &lt;- 0 to 5) &#123; println(x) &#125; &#125;&#125;","categories":[{"name":"Scala","slug":"Scala","permalink":"http://yoursite.com/categories/Scala/"}],"tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"}]},{"title":"ä¼ å€¼ä¼ åè°ƒç”¨","slug":"ä¼ å€¼ä¼ åè°ƒç”¨","date":"2020-04-15T10:28:02.000Z","updated":"2020-04-15T10:28:02.982Z","comments":true,"path":"2020/04/15/ä¼ å€¼ä¼ åè°ƒç”¨/","link":"","permalink":"http://yoursite.com/2020/04/15/%E4%BC%A0%E5%80%BC%E4%BC%A0%E5%90%8D%E8%B0%83%E7%94%A8/","excerpt":"","text":"ä¼ å€¼ä¼ åè°ƒç”¨ ä¼ å€¼ name:type è®¡ç®—å¥½å€¼è¿›è¡Œè°ƒç”¨ ä¼ å name: =&gt; typeåœ¨è°ƒç”¨å¤„å†è¿›è¡Œè®¡ç®— 12345678910111213141516171819202122232425262728293031323334package com.kowhoy.scala// ä¼ å€¼è°ƒç”¨å’Œä¼ åè°ƒç”¨object sendWay &#123; var initNum = 0 def main(args: Array[String]):Unit = &#123; sendName(counter) println(\"--\" * 20) initNum = 0 sendValue(counter) &#125; def counter:Int = &#123; initNum += 1 initNum &#125; def sendName(x: =&gt; Int) :Unit = &#123; for (i &lt;- 0 to 5) &#123; println(x) &#125; &#125; def sendValue(x: Int) :Unit = &#123; for (i &lt;- 0 to 5) &#123; println(x) &#125; &#125;&#125; 12345678910111213123456----------------------------------------111111","categories":[{"name":"Scala","slug":"Scala","permalink":"http://yoursite.com/categories/Scala/"}],"tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"}]},{"title":"scalaå˜é•¿å‚æ•°","slug":"scalaå˜é•¿å‚æ•°","date":"2020-04-15T10:27:19.000Z","updated":"2020-04-15T10:27:19.716Z","comments":true,"path":"2020/04/15/scalaå˜é•¿å‚æ•°/","link":"","permalink":"http://yoursite.com/2020/04/15/scala%E5%8F%98%E9%95%BF%E5%8F%82%E6%95%B0/","excerpt":"","text":"å˜é•¿å‚æ•° å‡½æ•°çš„å‚æ•°å¯ä»¥è®¾ç½®ä¸ºå¤šä¸ªåŒç§ç±»å‹æˆ–ä¸åŒç±»å‹çš„å‚æ•° 1234567891011121314151617181920212223242526272829303132333435363738package com.kowhoy.scala//å˜é•¿å‚æ•°, å¯å˜å‚æ•°object varargs &#123; def main(args: Array[String]): Unit = &#123; var numsSum = sumargs(1, 1, 3, 4, 5, 6) println(numsSum) var minusRight = minusargs(\"right\", 5, 4, 3, 2, 1) // 5 4 3 (2 -1 ) =&gt; 5 - 4 - (3-1) =&gt; 5 - 4 - 2 =&gt; 5 - 2 =&gt; 3 var minusLeft = minusargs(\"left\", 5, 4, 3, 2, 1) println(minusRight, minusLeft) anyArgs(\"abc\", 123) &#125; def sumargs(args: Int*):Int = &#123; var res = args.reduceLeft(_+_) res &#125; def minusargs(minuDirction:String, args: Int*):Int = &#123; var res = 0 if (minuDirction == \"right\") &#123; res = args.reduceRight(_-_) &#125; else if (minuDirction == \"left\") &#123; res = args.reduceLeft(_-_) &#125; res &#125; def anyArgs(args: Any*):Unit = &#123; args.foreach(println) &#125;&#125; reduceLeft / reduceRight","categories":[{"name":"Scala","slug":"Scala","permalink":"http://yoursite.com/categories/Scala/"}],"tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"}]},{"title":"Scalaä¸­çš„lazyä½¿ç”¨","slug":"Scalaä¸­çš„lazyä½¿ç”¨","date":"2020-04-15T10:26:21.000Z","updated":"2020-04-15T10:26:21.267Z","comments":true,"path":"2020/04/15/Scalaä¸­çš„lazyä½¿ç”¨/","link":"","permalink":"http://yoursite.com/2020/04/15/Scala%E4%B8%AD%E7%9A%84lazy%E4%BD%BF%E7%94%A8/","excerpt":"","text":"Scalaä¸­çš„lazyä½¿ç”¨ä»€ä¹ˆæ˜¯lazy lazyç”¨åœ¨èµ‹å€¼çš„æ—¶å€™ï¼Œç›®çš„æ˜¯ä¸ºäº†åœ¨èµ‹å€¼å˜é‡çœŸæ­£è¢«ä½¿ç”¨çš„æ—¶å€™ï¼Œæ‰ä¼šè¿›è¡Œå˜é‡èµ‹å€¼å¤„ç† ç¤ºä¾‹12345678910111213141516171819202122232425262728293031323334353637383940414243//lazyèµ‹å€¼package com.kowhoy.scalaobject Lazy &#123; def main(args: Array[String]): Unit = &#123; withNoLazy.run() println(\"--\"*20) withLazy.run() &#125;&#125;object withNoLazy &#123; def init(): Int = &#123; println(\"éæƒ°æ€§èµ‹å€¼\") 0 &#125; def run(): Unit = &#123; val initNum = init() println(\"Start...\") println(initNum) println(\"End...\") &#125;&#125;object withLazy &#123; def init(): Int = &#123; println(\"æƒ°æ€§èµ‹å€¼\") 0 &#125; def run(): Unit = &#123; lazy val initNum = init() println(\"Start...\") println(initNum) println(\"End...\") &#125;&#125; ä¸Šæ–‡çš„è¾“å‡ºå¦‚ä¸‹123456789éæƒ°æ€§èµ‹å€¼Start...0End...----------------------------------------Start...æƒ°æ€§èµ‹å€¼0End... æ³¨æ„ ä½¿ç”¨lazyèµ‹å€¼çš„å˜é‡åªèƒ½æ˜¯valå£°æ˜çš„lazy modifier allowed only with value definitions","categories":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/categories/scala/"}],"tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"}]},{"title":"mysqlåˆ†ç»„topä¼˜åŒ–","slug":"mysqlåˆ†ç»„topä¼˜åŒ–","date":"2020-04-14T08:26:35.000Z","updated":"2020-04-14T08:26:35.915Z","comments":true,"path":"2020/04/14/mysqlåˆ†ç»„topä¼˜åŒ–/","link":"","permalink":"http://yoursite.com/2020/04/14/mysql%E5%88%86%E7%BB%84top%E4%BC%98%E5%8C%96/","excerpt":"","text":"mysqlåˆ†ç»„topä¼˜åŒ– mysqlåˆ†ç»„å–top1 é€šå¸¸æ˜¯ç”¨max() ç„¶åå†…è¿ å¯ä»¥ä½¿ç”¨å˜é‡çš„æ–¹å¼é¿å…joinæ“ä½œ é—®é¢˜åœºæ™¯åŒä¸€ç»„oss_id, osp_id, pro_sku_code, start_time ä¼šæœ‰å¤šç§ price_val,å…ˆéœ€è¦ä¾æ®update_timeå­—æ®µå–æ¯ä¸€ç»„çš„æœ€åä¸€æ¡æ›´æ–°å†…å®¹ åŸå§‹å†™æ³•123456789101112select a.osp_id, a.oss_id, a.pro_sku_code, a.price_val, a.start_time from pricing.price_log a,( select max(update_time) as max_up, oss_id, osp_id, pro_sku_code, start_time from pricing.price_log where price_type = 10 AND STATUS = 1 AND delete_flag = 0 group by oss_id, osp_id, pro_sku_code, start_time ) b where a.oss_id = b.oss_id and a.osp_id = b.osp_id and a.pro_sku_code = b.pro_sku_code and a.start_time = b.start_time and a.update_time = b.max_up and a.price_type = 10 and a.status = 1 and a.delete_flag = 0 150wæ•°æ®10minæ‰§è¡Œæ—¶é—´ * ä½¿ç”¨å˜é‡å†™æ³•1234567891011121314151617181920set @sameno := 1;set @oss_id := -1;set @osp_id := -1;set @pro_sku_code := \"\";set @start_time := \"1970-01-01\";select oss_id, osp_id, pro_sku_code, start_time, price_val from (select @sameno := case when @oss_id = oss_id and @osp_id = osp_id and @pro_sku_code = pro_sku_code and @start_time = start_time then @sameno + 1 else 1 end as sameno, @oss_id := oss_id oss_id, @osp_id := osp_id osp_id, @pro_sku_code := pro_sku_code pro_sku_code, @start_time := start_time start_time, price_valfrom(selectoss_id, osp_id, pro_sku_code, start_time, price_valfrom price_log where price_type = 10 and status = 1 and delete_flag = 0 order byoss_id, osp_id, pro_sku_code, start_time, update_time desc ) a) b where sameno = 1 30så·¦å³ * âš ï¸ éœ€è¦å…ˆè®¾å®šå¤´éƒ¨åˆå§‹å˜é‡ï¼Œå¦åˆ™é¦–æ¬¡æ‰§è¡Œç»“æœéé¢„æœŸï¼Œå¤šæ¬¡æ‰§è¡Œæ•ˆæœä¸ä¸€è‡´ï¼Œå¾ˆå¥‡æ€ª ä½¿ç”¨pythonæ“ä½œ ä¸å¯ä½¿ç”¨pd.read_sql_query() æœ‰å¼‚å¸¸ 1234567891011121314151617181920212223242526272829sql = ''' select oss_id, osp_id, pro_sku_code, start_time, price_val from ( select @sameno := case when @oss_id = oss_id and @osp_id = osp_id and @pro_sku_code = pro_sku_code and @start_time = start_time then @sameno + 1 else 1 end as sameno, @oss_id := oss_id oss_id, @osp_id := osp_id osp_id, @pro_sku_code := pro_sku_code pro_sku_code, @start_time := start_time start_time, price_val from (select oss_id, osp_id, pro_sku_code, start_time, price_val from price_log where price_type = 10 and status = 1 and delete_flag = 0 order by oss_id, osp_id, pro_sku_code, start_time, update_time desc ) a) b where sameno = 1 ''' db_conf = conf.db_conf oms_conf = db_conf['oms_db'] db = pymysql.connect(oms_conf['host'], oms_conf['user'], oms_conf['passwd'], 'pricing') heads = [\"set @sameno := 1;\", \"set @oss_id := -1;\", \"set @osp_id := -1;\", \"set @pro_sku_code := '';\", \"set @start_time := '1970-01-01'\"] cursor = db.cursor() for col in heads: cursor.execute(col) cursor.execute(sql) res = cursor.fetchall() è¿˜å¯ä»¥ä½¿ç”¨ç¨‹åºå†…è¿›è¡Œç­›é€‰ï¼Œä¸è¿‡æ•ˆç‡å¹¶ä¸é«˜ï¼Œä½†æ˜¯æ¯”maxè¦å¥½","categories":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"é…ç½®æ•°æ®ç®¡ç†æŸ¥è¯¢","slug":"é…ç½®æ•°æ®ç®¡ç†æŸ¥è¯¢","date":"2020-04-10T10:05:02.000Z","updated":"2020-04-10T10:16:55.898Z","comments":true,"path":"2020/04/10/é…ç½®æ•°æ®ç®¡ç†æŸ¥è¯¢/","link":"","permalink":"http://yoursite.com/2020/04/10/%E9%85%8D%E7%BD%AE%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86%E6%9F%A5%E8%AF%A2/","excerpt":"","text":"é…ç½®æ•°æ®ç®¡ç†æŸ¥è¯¢ èƒŒæ™¯: æ•°æ®åº“ä»¥åŠæœåŠ¡å™¨çš„é…ç½®ä¿¡æ¯æ¯”è¾ƒå¤šï¼Œä¸´æ—¶ä½¿ç”¨é¢‘ç‡ä¹Ÿé«˜,æ‰¾èµ·æ¥ä¸æ–¹ä¾¿ pylsy configparser catconfig.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import sysfrom pylsy import pylsytableimport configparsercp &#x3D; configparser.ConfigParser()cp.read(&quot;&#x2F;Users&#x2F;zhouke&#x2F;code&#x2F;catconfig&#x2F;catconfig.cfg&quot;)cp_sections &#x3D; cp.sections()config_dict &#x3D; &#123;&#125;for section in cp_sections: name &#x3D; section.split(&quot;:&quot;)[0] if name not in config_dict: config_dict[name] &#x3D; [] config_data &#x3D; cp._sections[section] config_dict[name].append(config_data)match_dict &#x3D; &#123; &quot;db_51&quot;:[&quot;51&quot;, &quot;ç”Ÿäº§&quot;], &quot;db_69&quot;:[&quot;db_69&quot;], &quot;db_oms&quot;:[&quot;oms&quot;], &quot;db_oil&quot;:[&quot;oil&quot;], &quot;db_oil_test&quot;:[&quot;oil_test&quot;], &quot;db_ygh&quot;:[&quot;ygh&quot;, &quot;petrol&quot;], &quot;db_cd&quot;:[&quot;db_cd&quot;, &quot;db_ç¦…é“&quot;, &quot;cd&quot;, &quot;ç¦…é“&quot;], &quot;db_yzg&quot;:[&quot;yzg&quot;], &quot;db_yzg_test&quot;:[&quot;yzg_test&quot;], &quot;mail_data&quot;:[&quot;mail&quot;], &quot;web_datav&quot;:[&quot;datav&quot;], &quot;web_ali&quot;:[&quot;zy_ali&quot;], &quot;tm&quot;:[&quot;teamview&quot;, &quot;teamviewer&quot;, &quot;tm&quot;], &quot;s_69&quot;:[&quot;s_69&quot;, &quot;69&quot;], &quot;s_jump&quot;:[&quot;jump&quot;, &quot;jumper&quot;, &quot;2222&quot;], &quot;s_cd&quot;:[&quot;s_cd&quot;, &quot;s_ç¦…é“&quot;], &quot;s_54&quot;:[&quot;54&quot;, &quot;s_54&quot;], &quot;s_56&quot;:[&quot;56&quot;, &quot;s_56&quot;, &quot;win&quot;, &quot;windows&quot;], &quot;s_ali&quot;:[&quot;ali&quot;], &quot;s_bcc&quot;:[&quot;bcc&quot;, &quot;baidu&quot;, &quot;s_bcc&quot;], &quot;db_bcc&quot;:[&quot;db_bcc&quot;], &quot;wx&quot;:[&quot;wx&quot;, &quot;wx_mail&quot;], &quot;bk&quot;:[&quot;bucket&quot;, &quot;bos&quot;], &quot;tab&quot;:[&quot;tableau&quot;, &quot;license&quot;, &quot;tab&quot;]&#125;find_dict &#x3D; &#123;&#125;for name, alias_list in match_dict.items(): for idx, alias in enumerate(alias_list): find_dict[alias] &#x3D; name find_dict[name.split(&quot;_&quot;)[0] + alias] &#x3D; namesearch_str &#x3D; sys.argv[1]if search_str not in find_dict: print(&quot;ğŸˆšï¸&quot;)else: config_res &#x3D; config_dict[find_dict[search_str]] table_dict &#x3D; &#123;&#125; for idx, row in enumerate(config_res): for k, v in row.items(): if k not in table_dict: table_dict[k] &#x3D; [] table_dict[k].append(v) table_head &#x3D; list(table_dict.keys()) table &#x3D; pylsytable(table_head) for k, v in table_dict.items(): table.add_data(k, v) print(table) catconfig.cfg 123456789[db_a:1]host &#x3D; 111user &#x3D; 111passwd &#x3D; 111[db_a:2]host &#x3D; 111user &#x3D; 222passwd &#x3D; 222 catconfig.sh 12#!&#x2F;bin&#x2F;bashpython &#x2F;Users&#x2F;zhouke&#x2F;code&#x2F;catconfig&#x2F;catconfig.py $1 æ·»åŠ å…¨å±€å‘½ä»¤ 1sudo ln -s &#x2F;Users&#x2F;zhouke&#x2F;code&#x2F;catconfig&#x2F;catconfig.sh &#x2F;usr&#x2F;local&#x2F;bin&#x2F;catconfig ä½¿ç”¨ 1catconfig a ## a: é…ç½®çš„æ˜ å°„å­—æ®µ æ•ˆæœ","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"å·¥å…·","slug":"å·¥å…·","permalink":"http://yoursite.com/tags/%E5%B7%A5%E5%85%B7/"}]},{"title":"uwsgi+nginxéƒ¨ç½²flask","slug":"uwsgi-nginxéƒ¨ç½²flask","date":"2020-04-10T09:54:21.000Z","updated":"2020-04-10T10:14:43.268Z","comments":true,"path":"2020/04/10/uwsgi-nginxéƒ¨ç½²flask/","link":"","permalink":"http://yoursite.com/2020/04/10/uwsgi-nginx%E9%83%A8%E7%BD%B2flask/","excerpt":"","text":"uwsgi + nginxéƒ¨ç½²flaskä¸€ã€å®‰è£…uwsgi ä½¿ç”¨pythonç‰ˆæœ¬å¯¹åº”çš„pipè¿›è¡Œå®‰è£…uwsgi 1pip install uwsgi åœ¨é¡¹ç›®ä¸‹åˆ›å»ºuwsgi.iniæ–‡ä»¶ 12345678910111213[uwsgi]http &#x3D; 0.0.0.0:5000pythonpath &#x3D; &#x2F;home&#x2F;anaconda3&#x2F;bin&#x2F;pythonchdir &#x3D; &#x2F;home&#x2F;code&#x2F;apiwsgi-file &#x3D; app.pycallable &#x3D; appprocesses &#x3D; 4threads &#x3D; 2stats &#x3D; 127.0.0.1:9191pidfile &#x3D; uwsgi.piddeamonize &#x3D; .&#x2F;log&#x2F;uwsgi.loglazy-apps &#x3D; truetouch-chain-reload &#x3D; true åå°è¿è¡Œuwsgi 1uwsgi -d --ini uwsgi.ini äºŒã€å®‰è£…é…ç½®nginx ä¸‹è½½å®‰è£…åŒ… 1wget http:&#x2F;&#x2F;nginx.org&#x2F;download&#x2F;nginx-1.17.9.tar.gz è§£å‹ï¼Œä¸è¦æ”¾åˆ°/usr/localä¸‹ï¼Œä¼šå®‰è£…åˆ°è¿™ä¸ªç›®å½• 1tar -zxvf nginx-1.17.9.tar.gz æŸ¥çœ‹ç¯å¢ƒæ˜¯å¦æ»¡è¶³ 123456789101112131415cd nginx-1.17.9&#x2F;.&#x2F;configure##ä¸å‡ºç°erroråˆ™æ»¡è¶³##å¦åˆ™æ£€æŸ¥ä¾èµ–åº“#ä¾èµ–åº“å®‰è£…sudo yum install gcc-c++sudo yum install pcresudo yum install pcre-develsudo yum install zlibsudo yum install zlib-develsudo yum install opensslsudo yum install openssl-devel æ— é”™è¯¯è¿›è¡Œç¼–è¯‘ 123makesudo make install ä¸‰ã€é…ç½®nginx è‡ªå®šä¹‰é…ç½®æ–‡ä»¶ /usr/local/nginx/conf/conf.d/api.conf 12345678910111213141516171819202122232425262728upstream project&#123; server localhost:5000;&#125;server &#123; listen 8080; server_name IP&#x2F;localhost; access_log &#x2F;home&#x2F;api&#x2F;access.log; error_log &#x2F;home&#x2F;api&#x2F;error.log; location &#x2F; &#123; proxy_pass http:&#x2F;&#x2F;project; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504; proxy_max_temp_file_size 0; proxy_connect_timeout 90; proxy_send_timeout 90; proxy_read_timeout 90; proxy_buffer_size 4k; proxy_buffers 4 32k; proxy_busy_buffers_size 64k; proxy_temp_file_write_size 64k; &#125;&#125; é…ç½®/usr/local/nginx/conf/nginx.conf 12##åœ¨httpä¸‹é¢åŠ ä¸Šinclude &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;conf&#x2F;conf.d&#x2F;*.conf; é…ç½®å¼€æœºå¯åŠ¨ 1234567891011121314151617vim &#x2F;lib&#x2F;systemd&#x2F;system&#x2F;nginx.service[Unit]Description&#x3D;nginxAfter&#x3D;network.target[Service]Type&#x3D;forkingExecStart&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;nginxExecReload&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;nginx reloadExecStop&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;nginx quitPrivateTmp&#x3D;true[Install]WantedBy&#x3D;multi-user.targetsystemctl enable nginx.service å¯åŠ¨nginx 1systemctl start nginx","categories":[{"name":"nginx","slug":"nginx","permalink":"http://yoursite.com/categories/nginx/"},{"name":"uwsgi","slug":"nginx/uwsgi","permalink":"http://yoursite.com/categories/nginx/uwsgi/"}],"tags":[{"name":"uwsgi","slug":"uwsgi","permalink":"http://yoursite.com/tags/uwsgi/"},{"name":"nginx","slug":"nginx","permalink":"http://yoursite.com/tags/nginx/"}]},{"title":"ElasticsearchåŸºç¡€ä½¿ç”¨","slug":"ElasticsearchåŸºç¡€ä½¿ç”¨","date":"2020-04-02T08:23:17.000Z","updated":"2020-04-02T10:22:04.764Z","comments":true,"path":"2020/04/02/ElasticsearchåŸºç¡€ä½¿ç”¨/","link":"","permalink":"http://yoursite.com/2020/04/02/Elasticsearch%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/","excerpt":"","text":"ElasticsearchåŸºç¡€ä½¿ç”¨ä¸€ã€å®‰è£… éœ€è¦javaç¯å¢ƒ ä¸‹è½½å®‰è£…åŒ… 1wget https:&#x2F;&#x2F;mirrors.huaweicloud.com&#x2F;elasticsearch&#x2F;7.6.1&#x2F;elasticsearch-7.6.1-linux-x86_64.tar.gz è§£å‹ï¼Œé…ç½®ç¯å¢ƒå˜é‡ 12345tar -zxvf elasticsearch-7.6.1-linux-x86_64.tar.gz#~&#x2F;.bash_profileexport ELASTICSEARCH_PATH&#x3D;&#x2F;home&#x2F;kowhoy&#x2F;software&#x2F;elasticsearch-7.6.1export PATH&#x3D;$&#123;ELASTICSEARCH_PATH&#125;&#x2F;bin:$PATH å¯åŠ¨ 1elasticsearch æ£€æŸ¥ 12#å¦å¯ç»ˆç«¯curl &#39;http:&#x2F;&#x2F;localhost:9200&#x2F;?pretty&#39; äºŒã€åŸºç¡€æ¦‚å¿µ elasticsearchä¸­çš„æ¦‚å¿µä¸å…³ç³»å‹æ•°æ®åº“çš„æ¦‚å¿µç±»æ¯”: å…³ç³»å‹æ•°æ®åº“ elasticsearch databases indices tables types rows documents cloumns fields ä¸‰ã€åŸºç¡€ä½¿ç”¨ æœ¬ç¯‡ä½¿ç”¨çš„æ˜¯insomniaå·¥å…·ç›®çš„æ˜¯åˆ›å»ºä¸€ä¸ªç±»ä¼¼ä»¥ä¸‹å…³ç³»å‹æ•°æ®åº“ç»“æ„çš„Elasticsearch ç±»å‹ åç§° database-name history-demo table-name shell column-1-name command column-2-name create-by åˆ›å»ºindex 12345678910111213141516171819202122232425262728@method: PUT@url: http://localhost:9200/history_demo@return: &#123; \"acknowledged\": true, \"shards_acknowledged\": true, \"index\": \"history_demo\"&#125;or##å†™å…¥æ•°æ®çš„åŒæ—¶ä¹Ÿåˆ›å»ºindex@method: PUT@url: http://localhost:9200/history_demo/shell/1@data: &#123;\"command\":\"df -h\"&#125;@return: &#123; \"_index\": \"history_demo\", \"_type\": \"shell\", \"_id\": \"1\", \"_version\": 1, \"result\": \"created\", \"_shards\": &#123; \"total\": 2, \"successful\": 1, \"failed\": 0 &#125;, \"_seq_no\": 0, \"_primary_term\": 1&#125; æŸ¥çœ‹indices 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748##æŸ¥çœ‹æŒ‡å®šçš„index@method: GET@url: http://localhost:9200/history_demo@return: &#123; \"history_demo\": &#123; \"aliases\": &#123;&#125;, \"mappings\": &#123; \"shell\": &#123; \"properties\": &#123; \"command\": &#123; \"type\": \"text\", \"fields\": &#123; \"keyword\": &#123; \"type\": \"keyword\", \"ignore_above\": 256 &#125; &#125; &#125; &#125; &#125; &#125;, \"settings\": &#123; \"index\": &#123; \"creation_date\": \"1585796252661\", \"number_of_shards\": \"5\", \"number_of_replicas\": \"1\", \"uuid\": \"FrQU3tV4Smu5XioGH6qUjw\", \"version\": &#123; \"created\": \"6060199\" &#125;, \"provided_name\": \"history_demo\" &#125; &#125; &#125;&#125;## æŸ¥çœ‹æ‰€æœ‰çš„indices@method: GET@url: http://localhost:9200/_cat/indices@remark: ä½¿ç”¨`_cat/indices`@return:yellow open oss JYYZYHlxQsCYpUfJGjNnfw 5 1 0 0 1.2kb 1.2kbyellow open doc 0JUEuXarQ0aBaB1YHD2KcA 5 1 1 0 4.7kb 4.7kbyellow open history_demo FrQU3tV4Smu5XioGH6qUjw 5 1 1 0 4.4kb 4.4kbyellow open posts c18QVZ00Snyp1OOomxjJuA 5 1 1 0 5.5kb 5.5kbgreen open .kibana_1 jLWNf4ysRDSUvR3qfmv5Ow 1 0 2 0 9.3kb 9.3kbyellow open blog x_vNxVVHT6qgQY3_RKiaHg 5 1 6 0 28.3kb 28.3kbyellow open person 1ctpvuoqTV-vTO4q9G9cnQ 5 1 2 0 8.1kb 8.1kb æ’å…¥documents 123456789101112131415161718192021222324252627282930313233343536373839## æ’å…¥å•æ¡æ•°æ®@method: PUT/POST@url: http://localhost:9200/history_demo/shell/3@data: &#123;\"command\":\"df -h\"&#125;@return: &#123; \"_index\": \"history_demo\", \"_type\": \"shell\", \"_id\": \"3\", \"_version\": 1, \"result\": \"created\", \"_shards\": &#123; \"total\": 2, \"successful\": 1, \"failed\": 0 &#125;, \"_seq_no\": 0, \"_primary_term\": 1&#125;## æ’å…¥æŒ‡å®šIDçš„å¤šæ¡æ•°æ®@method: PUT/POST@url: http://localhost:9200/history_demo/shell/_bulk@data: &#123;\"index\":&#123;\"_id\":6&#125;&#125;&#123;\"command\":\"mysql\"&#125;&#123;\"index\":&#123;\"_id\":7&#125;&#125;&#123;\"command\":\"logatsh\"&#125;#!!æ³¨æ„ è¿™ä¸ªjson_dataæœ«å°¾è¦å¤šç•™ä¸€è¡Œ@return: ...## æ’å…¥ä¸æŒ‡å®šIDçš„å¤šæ¡æ•°æ®@method: PUT/POST@url: http://localhost:9200/history_demo/shell/_bulk@data:&#123;\"index\":&#123;&#125;&#125;&#123;\"command\":\"mysql\"&#125;&#123;\"index\":&#123;&#125;&#125;&#123;\"command\":\"logatsh\"&#125; æŸ¥è¯¢ 1234567891011## æŸ¥è¯¢æ‰€æœ‰çš„documents@method: GET@url: http://localhost:9200/history_demo/shell/_search@data: &#123;\"query\":&#123;\"match_all\":&#123;&#125;&#125;&#125;@return: ...## æŒ‰æ¡ä»¶æŸ¥è¯¢@method: GET@url: http://localhost:9200/history_demo/shell/_search@data: &#123;\"query\":&#123;\"match\":&#123;\"command\":\"mysql\"&#125;&#125;&#125;@return: ... åˆ é™¤ 12345678910## æŒ‡å®šIDè¿›è¡Œdelete@method: DELETE@url: http://localhost:9200/history_demo/shell/1@return: ...## æ ¹æ®æ¡ä»¶åˆ é™¤@method:POST@url: http://localhost:9200/history_demo/shell/_delete_by_query@data: &#123;\"query\":&#123;\"match\":&#123;\"command\":\"mysql\"&#125;&#125;&#125;@return: ... ä¿®æ”¹ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051## æŒ‡å®šIDè¿›è¡Œä¿®æ”¹@method:POST@url: http://localhost:9200/history_demo/shell/2/_update@data: &#123; \"script\": &#123; \"source\":\"ctx._source.command=params.command\", \"lang\": \"painless\", \"params\": &#123; \"command\":\"history\" &#125; &#125;&#125;@return: ...@remark: ctxæŒ‡å½“å‰äº‹åŠ¡, _sourceæŒ‡å½“å‰çš„document, langæŒ‡ä½¿ç”¨painlessè„šæœ¬è¯­è¨€æ¥ç¼–å†™script## ä»…å¢åŠ å­—æ®µè¿›è¡Œä¿®æ”¹@method: POST@url: http://localhost:9200/history_demo/shell/2/_update@data: &#123; \"script\": &#123; \"source\": \"ctx._source.bash_time='2020-02-02'\" &#125;&#125;@return: ...## ä»…åˆ é™¤å­—æ®µè¿›è¡Œä¿®æ”¹@method: POST@url: http://localhost:9200/history_demo/shell/2/_update@data: &#123; \"script\": &#123; \"source\": \"ctx._source.remove('bash_time')\" &#125;&#125;@return: ...## æ ¹æ®æ¡ä»¶è¿›è¡Œä¿®æ”¹@method: POST@url: http://localhost:9200/history_demo/shell/_update_by_query@data: &#123; \"query\": &#123; \"match\":&#123;\"command\":\"df -h\"&#125; &#125;, \"script\": &#123; \"source\": \"ctx._source.command=params.command\", \"lang\": \"painless\", \"params\": &#123; \"command\": \"tail data.log\" &#125; &#125;&#125;@return: ..","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://yoursite.com/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://yoursite.com/tags/Elasticsearch/"}]},{"title":"flaskæ€è·¯æ•´ç†","slug":"flaskæ€è·¯æ•´ç†","date":"2020-03-27T10:13:06.000Z","updated":"2020-03-27T10:35:34.834Z","comments":true,"path":"2020/03/27/flaskæ€è·¯æ•´ç†/","link":"","permalink":"http://yoursite.com/2020/03/27/flask%E6%80%9D%E8%B7%AF%E6%95%B4%E7%90%86/","excerpt":"","text":"flaskæ€è·¯æ•´ç†ä¸€. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒpython3 -m venv venv. venv/bin/activate äºŒ. init.pyä¸»è¦å·¥ä½œï¼š åˆ›å»ºåº”ç”¨ æ·»åŠ é…ç½® åˆå§‹åŒ–åº”ç”¨ æ³¨å†Œè“å›¾ åˆ›å»ºåº”ç”¨: 123456import osfrom flask import Flaskdef create_app(test_config=None): ## instance_relative_config ä½¿ç”¨Trueçš„è¯å°±ä¼šä»instanceæ–‡ä»¶å¤¹ä½œä¸ºé…ç½®æ–‡ä»¶çš„è·¯å¾„ app = Flask(__name__, instance_relative_config=True) æ·»åŠ é…ç½® 12345678910111213##æ–¹å¼ä¸€:app.config.from_mapping( SECRET_KEY='dev' )##æ–¹å¼äºŒ:#silent æ–‡ä»¶ä¸å­˜åœ¨æ˜¯å¦é™é»˜æŠ¥é”™app.config.from_pyfile('config.py', silent=True)#éšæœºkeyçš„ç”Ÿæˆimport osimport binasciibinascii.hexlify(os.urandom(16)) åˆå§‹åŒ–åº”ç”¨ 123app.teardown_appcontext(db.close_db) ##åœ¨è¿”å›å“åº”åè¿›è¡Œæ¸…ç†è°ƒç”¨çš„ç¨‹åºapp.cli.add_command(db.init_db_command) ##æ·»åŠ å‘½ä»¤è¡ŒæŒ‡ä»¤ æ³¨å†Œè“å›¾ 12from . import authapp.register_buleprint(auth.bp) ä¸‰ã€db.py ä½¿ç”¨clickåˆ›å»ºå‘½ä»¤ 12345678import clickfrom flask.cli import with_appcontext@click.command('init-db')@with_appcontextdef init_db_command(): **** click.echo(\"è¾“å‡ºå†…å®¹\") æ•°æ®åº“è¿æ¥ä½¿ç”¨g,ä¸ä¼šæ¯æ¬¡è¯·æ±‚éƒ½é‡æ–°åˆ›å»ºè¿æ¥ 1234567from flasj import current_app, gdef get_db(): if 'db' not in g: g.db = åˆ›å»ºè¿æ¥ return g.db å…³é—­æ•°æ®åº“ 12345def close_db(e=None): db = g.pop('db', None) if db is not None: db.close() å››ã€å…·ä½“é€»è¾‘ è“å›¾ 123from flask import Blueprintbp &#x3D; Blueprint(&#39;auth&#39;, __name__, url_prefix&#x3D;&#39;&#x2F;auth&#39;) å¯†ç ç›¸å…³ 1from werkzeug.security import check_password_hash, generate_password_hash ä¿è¯è¯·æ±‚ä¹‹å‰çŸ¥é“å½“æ—¶çš„ç”¨æˆ· 12345678@bp.before app_requestdef load_logged_in_user(): user_id = session.get('user_id') if user_id is None: g.user = None else: g.user = get_db()..... è¦æ±‚æ˜¯åœ¨ç™»å½•çŠ¶æ€(ä½¿ç”¨è£…é¥°ç¬¦) 123456789import functoolsdef login_required(view): @functools.wraps(view) def wrapped_view(**kwargs): if g.user is None: return redirect(url_for(\"auth.login\")) return view(**kwargs) return wrapped_view äº”ã€é¡¹ç›®å¯å®‰è£… setup.py 123456789101112from setuptools import find_packages, setupsetup( name='flaskr', version='1.0.0', packages=find_packages(), include_package_data=True, zip_safe=False, install_requires=[ 'flask', ],) MANIIFEST.in 1234include flaskr&#x2F;schema.sqlgraft flaskr&#x2F;staticgraft flaskr&#x2F;templatesglobal-exclude *.pyc å¯ä»¥ä½¿ç”¨pip install -e .å®‰è£…é¡¹ç›® å…­ã€éƒ¨ç½²äº§å“ æ„å»º 123pip install wheelpython setup.py bdist_wheel å®‰è£…å°†ç”Ÿæˆçš„.whlæ–‡ä»¶å‘é€åˆ°æœåŠ¡æ±‚ 1pip install ***.whl æ›´æ”¹é…ç½®æ˜¯åœ¨venv/var/***-instance/ä¸‹ ä¸ƒã€è¿è¡Œäº§å“æœåŠ¡å™¨12pip iinstall waitresswaitress-serve --call &#39;appname:create_app&#39; å¯åŠ¨åº”ç”¨ 12345export FLASK_APP&#x3D;APP_NAMEexport FLASK_ENV&#x3D;developmentflask runflask init-db","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"flask","slug":"flask","permalink":"http://yoursite.com/tags/flask/"},{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"tmuxâ€”â€”åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹","slug":"tmuxâ€”â€”åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹","date":"2020-03-27T04:00:05.000Z","updated":"2020-03-27T04:09:32.564Z","comments":true,"path":"2020/03/27/tmuxâ€”â€”åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹/","link":"","permalink":"http://yoursite.com/2020/03/27/tmux%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B/","excerpt":"","text":"tmuxâ€”â€”åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹ ä½¿ç”¨tmuxåå°è¿è¡Œbash_history_notifyé¡¹ç›® ä¸€ã€å®‰è£…tmux12345#mac:brew install tmux#centos:yum install tmux äºŒã€é…ç½®tmux ä½¿ç”¨äº†githubä¸Šçš„é…ç½®ï¼Œç„¶ååšäº†ä¿®æ”¹tmuxçš„é»˜è®¤å¿«æ·é”®å‰ç¼€ä¸º ctrl + b, ç°åœ¨æ”¹ä¸ºctrl+aï¼Œä¸‹æ–‡ä¸­ä½¿ç”¨Prefixä»£æ›¿ctrl+aâš ï¸ :ä½¿ç”¨å¿«æ·é”®æ˜¯å…ˆæ‘ä¸‹å¿«æ·é”®å‰ç¼€ï¼Œç„¶åæŠ¬æ‰‹ï¼Œåœ¨è¿›è¡Œå…·ä½“çš„å¿«æ·æŒ‰é”® 1234cdgit clone https:&#x2F;&#x2F;github.com&#x2F;gpakosz&#x2F;.tmux.gitln -s -f .tmux&#x2F;.tmux.confcp .tmux&#x2F;.tmux.conf.local . ä¿®æ”¹å¿«æ·é”®å‰ç¼€ vim .tmux/.tmux.conf.localå»æ‰290è¡Œé™„è¿‘çš„æ³¨é‡Šç¬¦ 1234unbind C-aunbind C-bset -g prefix C-abind C-a send-prefix ä¸‰ã€ä½¿ç”¨tmuxå¼€å¯ä»»åŠ¡ bcc_00åˆ›å»ºkafka_serverçš„sessionn1tmux new -s kafka_server ä½¿ç”¨Prefix+%åˆ†å±ï¼Œä½¿ç”¨Prefix+æ–¹å‘é”®åˆ‡æ¢é¼ æ ‡æ‰€åœ¨çª—æ ¼ åœ¨çª—æ ¼1ä¸­å¯åŠ¨zookeeper 12cd app&#x2F;kafka_2.12-2.4.0&#x2F;.&#x2F;bin&#x2F;zookeeper-server-start.sh .&#x2F;config&#x2F;zookeeper.properties åœ¨çª—æ ¼2ä¸­å¯åŠ¨kafka 12cd app&#x2F;kafka_2.12-2.4.0&#x2F;.&#x2F;bin&#x2F;kafka-server-start.sh .&#x2F;config&#x2F;server.properties ) ä½¿ç”¨Prefix+c åˆ›å»ºæ–°çª—å£,å¹¶åˆ›å»º3ä¸ªçª—æ ¼,å¯åŠ¨bash_historyçš„æ¶ˆè´¹ç«¯ã€ç”Ÿäº§ç«¯ã€å®éªŒçª—å£ åœ¨çª—æ ¼1ä¸­å¯åŠ¨æ¶ˆè´¹ç«¯ 12cd ~&#x2F;code&#x2F;python&#x2F;bash_history&#x2F;python bash_history_consumer.py åœ¨çª—æ ¼2ä¸­å¯åŠ¨ç”Ÿäº§ç«¯ 12cd ~&#x2F;code&#x2F;python&#x2F;bash_history&#x2F;python bash_history_producer.py ~&#x2F;.bash_history åœ¨çª—æ ¼3ä¸­éšä¾¿è¾“å…¥å‘½ä»¤ï¼Œæ£€æŸ¥å¦å¤–ä¸¤ä¸ªçª—æ ¼æ˜¯å¦æœ‰è¾“å‡º bcc_01ä¸Šå¼€å¯ç”Ÿäº§ç«¯åŒä¸Šæ­¥éª¤ï¼Œbcc_01ä¸Šæ²¡æœ‰æ¶ˆè´¹ç«¯ ä½¿ç”¨Prefix + d é€€å‡ºsession ä½¿ç”¨tmux ls æŸ¥çœ‹sessionï¼Œ ä½¿ç”¨tmux at -t &lt;session_name&gt;è¿æ¥session","categories":[{"name":"å·¥å…·","slug":"å·¥å…·","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[]},{"title":"kafakç¤ºä¾‹-è¿œç¨‹ç›‘æ§ç»ˆç«¯è¾“å…¥","slug":"kafakç¤ºä¾‹-è¿œç¨‹ç›‘æ§ç»ˆç«¯è¾“å…¥","date":"2020-03-26T03:52:54.000Z","updated":"2020-03-26T03:52:54.112Z","comments":true,"path":"2020/03/26/kafakç¤ºä¾‹-è¿œç¨‹ç›‘æ§ç»ˆç«¯è¾“å…¥/","link":"","permalink":"http://yoursite.com/2020/03/26/kafak%E7%A4%BA%E4%BE%8B-%E8%BF%9C%E7%A8%8B%E7%9B%91%E6%8E%A7%E7%BB%88%E7%AB%AF%E8%BE%93%E5%85%A5/","excerpt":"","text":"kafakç¤ºä¾‹-è¿œç¨‹ç›‘æ§ç»ˆç«¯è¾“å…¥ ä½¿ç”¨pyinotifyç›‘å¬.bash_historyæ–‡ä»¶ï¼Œä½¿ç”¨kafkaç”Ÿäº§æ¶ˆè´¹ ä¸€ã€kafkaå®‰è£…é…ç½® å®‰è£…javaç¯å¢ƒ å®˜ç½‘ä¸‹è½½kafkaå®‰è£…åŒ… 1wget http:&#x2F;&#x2F;archive.apache.org&#x2F;dist&#x2F;kafka&#x2F;1.0.0&#x2F;kafka_2.12-1.0.0.tgz è§£å‹å‹ç¼©åŒ… é…ç½®/config/server.properties 1advertised.listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;&#123;&#123;IP&#125;&#125;:9092 å¯åŠ¨kafka 12345##zookeeperå¯åŠ¨.&#x2F;bin&#x2F;zookeeper-server-start.sh .&#x2F;config&#x2F;zookeeper.properties ##serverå¯åŠ¨.&#x2F;bin&#x2F;kafka-server-start.sh .&#x2F;config&#x2F;server.properties åˆ›å»ºtopic 1.&#x2F;bin&#x2F;kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic &#123;&#123;topic_name&#125;&#125; äºŒã€å®‰è£…pythonä¾èµ–12345#kafka-pythonpip install kafka-python#pyinotifypip install pyinotify ä¸‰ã€æœåŠ¡å™¨ç«¯ä»£ç 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495# -*- coding: utf-8 -*-# @Description: ç›‘å¬æ–‡ä»¶å˜æ›´å‘kafkaå‘é€# @Author: kowhoy# @Last Modified by: kowhoy# @filename: bash_history_producer.pyimport pyinotifyimport timeimport osimport sysimport jsonfrom kafka import KafkaProducerimport datetimeimport socketbootstrap_servers = \"ip:9092\"topic = \"bash_history\"host_name = \"bcc_00\"host_ip = socket.gethostbyname(socket.getfqdn(socket.gethostname()))'''@name: [class]Monitor_file@date: 2020-03-26 09:41:13@desc: ç›‘å¬æ–‡ä»¶@param: [str]filename @return: '''class Monitor_file: def notify_bash_history(self, filename): wm = pyinotify.WatchManager() notifier = pyinotify.Notifier(wm) wm.watch_transient_file(filename, pyinotify.IN_MODIFY, Process_transient_file) notifier.loop()'''@name: [class]Process_transient_file@date: 2020-03-26 09:44:29@desc: æ–‡ä»¶å‘ç”Ÿå˜åŒ–è§¦å‘@param: @return: '''class Process_transient_file(pyinotify.ProcessEvent): def process_IN_MODIFY(self, event): line = file.readlines() if line: if len(line) == 2: order_time = int(line[0][1:-1]) order_time_array = time.localtime(order_time) order_time_str = time.strftime(\"%Y-%m-%d %H:%M:%S\", order_time_array) order = line[1][:-1] msg = &#123; \"host_name\": host_name, \"host_ip\": host_ip, \"order_time\": order_time_str, \"order\": order &#125; Send_to_kafka().send_msg(msg)'''@name: [class]Send_to_kafka@date: 2020-03-26 09:47:50@desc: å‘kafkaå‘é€msg@param:@return: '''class Send_to_kafka(): def __init__(self): self.producer = KafkaProducer(bootstrap_servers=bootstrap_servers) def send_msg(self, msg): msg = json.dumps(msg, ensure_ascii=False).encode(\"utf-8\") self.producer.send(topic, msg) print(\"-\"*10, \"æ•°æ®å·²å‘é€\", \"-\"*10) print(msg) self.producer.flush()if __name__ == \"__main__\": filename = sys.argv[1] if not os.path.exists(filename): raise FileExistsError file_stat = os.stat(filename) file_size = file_stat[6] file = open(filename, \"r\") file.seek(file_size) Monitor_file().notify_bash_history(filename) å››ã€å®¢æˆ·ç«¯ä»£ç 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697# -*- coding: utf-8 -*-# @Description: # @Author: kowhoy# @Date: 2020-03-26 10:15:24# @Last Modified time: 2020-03-26 11:25:15from kafka import KafkaConsumerfrom kafka.structs import TopicPartitionimport pandas as pdfrom sqlalchemy import create_engineimport jsonimport sysimport osdbh_config = &#123; \"host\": \"localhost\", \"user\": \"root\", \"passwd\": \"passwd\", \"database\": \"bash_history\", \"table\": \"bash_history_log\"&#125;save_to_tb = True ## æ˜¯å¦å­˜æ•°æ®åº“topic = \"bash_history\"bootstrap_servers = [\"ip:9092\"]single_file_size = 1 #Mclass Bash_history_consumer: def __init__(self): self.topic = \"bash_history\" self.consumer = KafkaConsumer(bootstrap_servers=bootstrap_servers) self.log_file_path = \"./bash_history_log/\" log_files = os.listdir(self.log_file_path) max_log_count = max([int(filename.split(\"_\")[-1]) for filename in log_files]) if len(log_files) &gt; 0 else -1 self.count = max_log_count + 1 self.save_list = [] if save_to_tb: e = \"mysql+pymysql://%s:%s@%s/%s\"%(dbh_config[\"user\"], dbh_config[\"passwd\"], dbh_config[\"host\"], dbh_config[\"database\"]) self.dbh = create_engine(e) empty_sql = 'select * from information_schema.TABLES where TABLE_SCHEMA = \"&#123;db&#125;\" and TABLE_NAME = \"&#123;tb&#125;\"'.\\ format(db=dbh_config[\"database\"], tb=dbh_config[\"table\"]) empty_df = pd.read_sql_query(empty_sql, self.dbh) self.tb_empty = empty_df.empty def bash_consumer(self): self.consumer.assign([TopicPartition(topic=topic, partition=0)]) for msg in self.consumer: msg_offset = msg.offset msg_value = (msg.value).decode(\"utf-8\") msg_value = json.loads(msg_value) self.save_list.append(msg_value) save_data_size = sys.getsizeof(self.save_list) if save_data_size &gt;= single_file_size * 1024 * 1024: save_file = self.log_file_path + \"bash_history_log_\" + str(self.count) self.count += 1 with open(save_file, \"w+\") as f: for line in self.save_list: f.write(line.decode(\"utf-8\")+\"\\n\") self.save_list = [] print(\"*\"*10, \"å†™å…¥æ–‡ä»¶\", \"*\"*10) if save_to_tb: if self.tb_empty: add_way = \"replace\" self.tb_empty = False else: add_way = \"append\" for k, v in msg_value.items(): msg_value[k] = [v] insert_df = pd.DataFrame.from_dict(msg_value) insert_df.to_sql(dbh_config[\"table\"], self.dbh, if_exists=add_way, index=False) print(\"å½“å‰size\", save_data_size, \"\\t\", msg_value)if __name__ == \"__main__\": Bash_history_consumer().bash_consumer() äº”ã€å¼€å¯ä»»åŠ¡ æˆ‘æ˜¯ä½¿ç”¨bcc_00ä½œä¸ºå”¯ä¸€çš„æ¶ˆè´¹è€…ï¼Œbcc_00å’Œbcc_01ä½œä¸ºä¸¤ä¸ªç”Ÿäº§è€…ï¼Œä¿®æ”¹å¥½ä»£ç ä¸­çš„é…ç½®æ•°æ® bcc_00å¼€å¯æ¶ˆè´¹è€… 1python bash_history_consumer.py bcc_00 å’Œ bcc_01åˆ†åˆ«å¼€å¯ç”Ÿäº§è€… 1python bash_history_producer.py ~&#x2F;.bash_history æ–°èµ·çª—å£è¿›è¡Œå‘½ä»¤è¡Œæ“ä½œï¼Œå°±å¯ä»¥åœ¨æ§åˆ¶å°å’Œæ•°æ®åº“çœ‹åˆ°ç›¸å…³bash 00_consumer: 00_producer: 01_producer: sql_table(ä¸¤å°bccå†…ç½‘ipä¸€æ ·):","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/tags/kafka/"},{"name":"pyinotify","slug":"pyinotify","permalink":"http://yoursite.com/tags/pyinotify/"}]},{"title":"macä¸‹chromedriverå‡çº§","slug":"macä¸‹chromedriverå‡çº§","date":"2020-03-13T02:50:14.000Z","updated":"2020-03-13T02:50:14.554Z","comments":true,"path":"2020/03/13/macä¸‹chromedriverå‡çº§/","link":"","permalink":"http://yoursite.com/2020/03/13/mac%E4%B8%8Bchromedriver%E5%8D%87%E7%BA%A7/","excerpt":"","text":"ä¸‹è½½ä¸æœ¬åœ°chromeç‰ˆæœ¬ä¸€è‡´çš„chromedriver,ä¸‹è½½åœ°å€[http://chromedriver.storage.googleapis.com/index.html] è§£å‹åˆ°æœ¬åœ°çš„ /usr/local/bin æ£€æŸ¥ç‰ˆæœ¬ chromedirver -v","categories":[{"name":"å·¥å…·","slug":"å·¥å…·","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"å·¥å…·","slug":"å·¥å…·","permalink":"http://yoursite.com/tags/%E5%B7%A5%E5%85%B7/"}]},{"title":"scalaä¼´ç”Ÿç±»å’Œä¼´ç”Ÿå¯¹è±¡","slug":"scalaä¼´ç”Ÿç±»å’Œä¼´ç”Ÿå¯¹è±¡","date":"2020-02-23T11:36:55.000Z","updated":"2020-02-23T11:36:55.191Z","comments":true,"path":"2020/02/23/scalaä¼´ç”Ÿç±»å’Œä¼´ç”Ÿå¯¹è±¡/","link":"","permalink":"http://yoursite.com/2020/02/23/scala%E4%BC%B4%E7%94%9F%E7%B1%BB%E5%92%8C%E4%BC%B4%E7%94%9F%E5%AF%B9%E8%B1%A1/","excerpt":"","text":"Scalaä¼´ç”Ÿç±»å’Œä¼´ç”Ÿå¯¹è±¡ å®šä¹‰ï¼š åœ¨Scalaä¸­ï¼Œç±»åå’Œå¯¹è±¡åä¸€è‡´æ—¶ï¼Œäº’ä¸ºä¼´ç”Ÿç±»å’Œä¼´ç”Ÿå¯¹è±¡ã€‚ ä¾‹å­:123456789// ä¼´ç”Ÿç±»class ApplyTest &#123;&#125;// ä¼´ç”Ÿå¯¹è±¡object ApplyTest &#123;&#125; ä½¿ç”¨: é€šå¸¸åœ¨ä¼´ç”Ÿå¯¹è±¡ä¸­å†™apply()æ–¹æ³•ï¼Œç„¶ååœ¨applyæ–¹æ³•ä¸­å®ä¾‹ä¼´ç”Ÿç±»ï¼Œç„¶åæ¯æ¬¡ä½¿ç”¨ç±»åç§°()çš„æ—¶å€™ï¼Œå°±å·²ç»å®ä¾‹åŒ–äº†ç±» ä¾‹å­:12345678910111213141516object ApplyApp &#123; def main(args:Array[String]):Unit = &#123; val c = ApplyTest() println(c) &#125;&#125;class ApplyTest &#123;&#125;object ApplyTest &#123; def apply() = &#123; new ApplyTest() &#125;&#125;","categories":[{"name":"Scala","slug":"Scala","permalink":"http://yoursite.com/categories/Scala/"}],"tags":[{"name":"ä¼´ç”Ÿ","slug":"ä¼´ç”Ÿ","permalink":"http://yoursite.com/tags/%E4%BC%B4%E7%94%9F/"}]}]}